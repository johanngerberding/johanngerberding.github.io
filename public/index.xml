<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Johanns Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Johanns Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 10:00:00 +0200</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Retrieval-Augmented Text Generation (RAG)</title>
      <link>http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/</link>
      <pubDate>Tue, 23 Apr 2024 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/</guid>
      <description>A short intro to Retrieval-Augmented Text Generation, what is it, why is it useful and how does it work.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
In 2020, Lewis et al. introduced a novel approach to enhance language model outputs through the use of Retrieval-Augmented Generation (RAG). Since its inception, RAG has evolved significantly, providing a robust solution to some of the persistent challenges faced by Large Language Models (LLMs).
</p>
<p align="justify">
LLMs, while transformative, are not without their issues. They often produce "hallucinations" or convincing yet inaccurate answers, due to their sole reliance on the fixed dataset on which they were trained on. This limitation is compounded by their inability to incorporate information that emerges after their last training update, rendering them outdated as the world evolves. Additionally, their performance can be suboptimal in specialized areas because their training prioritizes breadth over depth to ensure broad accessibility and applicability.
</p>
<p align="justify">
RAG addresses these challenges by dynamically supplementing model responses with external data fetched in real time in response to queries. This approach not only keeps the information current but also enhances the accuracy and relevancy of the responses.
</p>
<p align="justify">
For a RAG system to deliver high-quality outputs, two core components are crucial. First, it must effectively retrieve the most relevant information from external sources. This involves a sophisticated understanding of the query context and the ability to sift through vast amounts of data quickly and efficiently. Second, it needs to generate accurate responses that integrate the retrieved information, ensuring that the final output is both coherent and contextually appropriate.
</p>
<p align="justify">
Building on these technologies, startups <a href="https://www.trychroma.com/">like Chroma</a>, <a href="https://weaviate.io/">Weaviate</a>, and <a href="https://www.pinecone.io/">Pinecone</a> have expanded upon RAG's foundational concepts by incorporating robust text storage solutions and advanced tooling into their platforms. These enhancements facilitate more efficient data handling and retrieval processes, which are critical for delivering precise and contextually relevant responses.
</p>
<p align="justify">
In the upcoming sections, we will dig deeper into the world of RAG by providing a comprehensive overview of the framework and highlighting the key components essential for an effective RAG system. Additionally, we will explore practical strategies that can enhance the quality and performance of your RAG system. While RAG offers significant benefits, it is not without its challenges. Therefore, we will also examine some of the common obstacles encountered in building RAG systems and discuss the current limitations that engineers need to be aware of as they build these kind of systems.
</p>
<h2 id="rag-framework">RAG Framework</h2>
<p align="justify">
The basic workflow of a RAG system consists of three fundamental steps: indexing, retrieval, and generation. This sequence begins with indexing, where data is organized to facilitate quick and efficient access. The retrieval step can be further divided into three subparts: pre-retrieval, retrieval, and post-retrieval. Pre-retrieval involves preparing and setting up the necessary parameters for the search. Retrieval is the actual process of fetching the most relevant information based on the query. Post-retrieval includes the refinement and selection of the fetched data to ensure its relevance and accuracy. Finally, in the generation phase, the system synthesizes this information into coherent and contextually appropriate responses. These foundational steps remain at the core of every advanced RAG workflow, serving as the building blocks for more complex systems.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_framework.png#center"
         alt="Abstract Concept Graphic Retrieval-Augmented Generation Elements" width="100%"/> <figcaption>
            <p>Figure 1. RAG Framework Paradigm [1]</p>
        </figcaption>
</figure>

<p align="justify">
RAG presents a cost-effective alternative to the traditional, resource-intensive training and fine-tuning processes required for LLMs. Unlike standard LLMs that need model updates to incorporate new information, RAG dynamically integrates fresh data by retrieving relevant external content and concatenating it to the search query. This ensures flexibility and scalability to make it particularly valuable across a variety of use cases, like self-service applications where solutions get retrieved from an internal knowledge base and integrated into a helpful answer. Down below you can see the basic workflow that most RAG systems follow. 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_workflow.png#center"
         alt="Basic Retrieval-Augmented Generation Workflow" width="100%"/> <figcaption>
            <p>Figure 2. Basic RAG Workflow []</p>
        </figcaption>
</figure>

<p align="justify"> 
You start by generating document vectors using an embedding model for every document or text you want to include in your knowledge database. Embedding models (e.g. BERT, OpenAI text-embedding-ada-002) turn text into a vector representation, which is essential for the retrieval process by enabling similarity search. The documents you would want to retrieve can range from specific company guidelines or documents, a website to solution descriptions to known problems. Popular choices for such vector stores are <a href="https://github.com/facebookresearch/faiss">FAISS</a>, <a href="https://www.trychroma.com/">Chroma</a> or <a href="https://lancedb.com/">Lance</a>.  
</p>
<p align="justify"> 
After you have generated the vector store you are good to go. First your query gets embedded into a vector, like we have described above, which is then used in a similarity search over your vector store to determine the most relevant documents. These documents get retrieved and concatenated to your query before its fed into the LLM. The LLM then produces based on the query and context an answer. This describes the very basic and naive workflow. In practice there are a lot of other things involved, like prompt engineering, a limited context window, limited input tokens of the embedding model, preprocessing of the query and documents, postprocessing of the answer etc. We will touch most of these things in more detail in the next sections. 
</p>
<ul>
<li>
<p>Open Source Platforms: LangChain, LLamaIndex, DSPy</p>
</li>
<li>
<p>old school algorithms like BM25 focus on term frequency and presence for document ranking and they don&rsquo;t use semantic information of the queries</p>
</li>
<li>
<p>this has changed quite a bit with deep learning</p>
</li>
<li>
<p>modern retrieval strategies leverage pretrained LMs like BERT (Devlin et al. 2019), which capture the semantic meaning of queries more effectively</p>
</li>
<li>
<p>improved search accuracy by considering synonyms and the structure of phrases</p>
</li>
<li>
<p>how: measure vector distances between entries in knowledge database (e.g. some vector database) and queries</p>
</li>
</ul>
<h3 id="pre-retrieval">Pre-Retrieval</h3>
<p>The pre-retrieval phase is all about the data or the knowledge base. The goal is to ensure efficient and correct information retrieval.</p>
<h4 id="indexing">Indexing</h4>
<p>It starts with indexing which is dependent on the task and data type. In the classical workflow, documents are split into chunks and then each chunk is embedded (into a feature vector) and indexed. The embeddings are created through the use of an embedding model.</p>
<ul>
<li>chunk size is kind of a hyperparameter</li>
<li>different chunking strategies</li>
</ul>
<p>Let&rsquo;s start by looking at the indexing process. The basic rag workflow starts with the creation of an index comprising external sources (e.g. Websites, company relevant pdfs) which is the basis for retrieval of relevant information.</p>
<ul>
<li>
<p>data prep is key here!</p>
</li>
<li>
<p>involves:
text normalization (tokenization, stemming, removal of stop words)
organize text segments into sentences or paragraphs to facilitate more focused searches</p>
</li>
<li>
<p>generation of semantic vector representations of texts which are stored in a vector database for rapid and precise retrieval</p>
</li>
<li>
<p>to achieve this you use a so called embedding model</p>
</li>
</ul>
<p>MEMWALKER</p>
<h4 id="query-manipulation">Query Manipulation</h4>
<ul>
<li>data and query preparation</li>
<li>you can imagine, based on the use case and data type, different strategies could be helpful here</li>
<li>e.g. sentence-level indexing is beneficial for question-answering systems to precisely locate answers, while document-level indexing is more appropriate for summarizing documents to understand their main concepts and ideas</li>
<li>the next thing you have to think about is query manipulation, which is performed to adjust user queries for a better match with the indexed data</li>
<li>query reformulation: rewrite the query to align more closely with the user&rsquo;s intention</li>
<li>query expansion: extend the query to capture more relevant results through synonyms or related terms</li>
<li>query normalization: resolve differences in spelling or terminology for consistent query matching</li>
<li>another way to enhance retrieval efficiency is data modification</li>
<li>this includes preprocessing techniques like removing irrelevant or redundant information or enriching the data with additional information such as metadata to boost the relevance and diversity of the retrieved content</li>
</ul>
<h4 id="data-modification">Data Modification</h4>
<h3 id="retrieval">Retrieval</h3>
<p align="justify">
- combination of search and ranking 
- goal: select and prioritize documents from a knowledge base to enhance the quality of the generation models outputs 
- distance based search in a vector database (cosine similarity)
</p>
<h3 id="post-retrieval">Post-Retrieval</h3>
<p align="justify">
</p>
<h3 id="generation">Generation</h3>
<p align="justify">
- task: generate text that is relevant to the query and reflective of the information found in the retrieved documents
- how: concat the query with the found information and input that into a LLM
- challenge: ensuring the outputs alignment and accuracy with the retrieved contents isn't straightforward 
- the generated output should accurately convey the information from the retrieved documents and align with the query's intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data 
- another challenge: what if there is no relevant information in your knowledge base for a certain query, how to detect such instances and how to behave? 
- retrieval is kinda naive, how would you filter irrelevant documents? based on some distance metric, but how to define that distance? 
</p>
<h2 id="advanced-rag-pipelines">Advanced RAG Pipelines</h2>
<p align="justify">
The naive RAG implementation described before is rarely enough to satisfy production grade requirements. This has multiple reasons:  
<ul>
<li><b>Question ambiguity</b>: user questions are not well defined and may lead to irrelevant retrieval results </li>
<li><b>Low retrieval accuracy</b>: retrieved documents may not be equally relevant to the question </li>
<li><b>Limited knowledge</b>: the knowledge base may not include the information the user is looking for </li>
<li><b>Context window performance limitations</b>: trying to "over-retrieve" may hit on the capacity of the context window or otherwise produce a context window that is too big to return a result in a reasonable amount of time </li>
</ul>
Many new RAG patterns have emerged to address these limitations. In the following I will go over some of those techniques, but it is important to note that there is no silver bullet. Each one of these methods may still produce poor results in certain situations or isn't well fitted for your specific use case.
</p>
<h3 id="self-rag">Self RAG</h3>
<p align="justify">
The Self-Reflective RAG paper describes fine-tuned models that incorporate mechanisms for adaptive information retrieval and self critique. These models can dynamically determine when external information is needed, and can critically evaluate its generated responses for relevance and factual accuracy. Figure X down below shows the retrieval process using these new tokens.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/self_rag.png#center"
         alt="Flowchart of the Self RAG method" width="100%"/> <figcaption>
            <p>Figure X. Self RAG Process [3]</p>
        </figcaption>
</figure>

<p align="justify">
First the Language Model decides about the necessity of additional information to answer the prompt. In the second step multiple segments are generated concurrently and each of them is rated in terms of relevance to the prompt and usefulness of the retrieved information. Thereafter each segments is critiqued and the best one is chosen. Then the cycle can repeat, if the model decides to retrieve more information. The table down below shows the four new tokens that make this whole process work.
</p>
<table>
    <tr>
        <th>Name</th>
        <th>Description</th>
        <th>Output</th>
</tr>
    <tr>
        <td>Retrieval Token</td>
        <td>Retrieve additional information from knowledge base</td>
        <td>yes, no</td>
    </tr>
    <tr>
        <td>Relevance Token</td>
        <td>Retrieved information is relevant to the prompt</td>
        <td>relevant, irrelevant</td>
    </tr>
    <tr>
        <td>Support Token</td>
        <td>Generated information is supported by the provided context</td>
        <td>fully supported, partially supported, no support</td>
    </tr>
    <tr>
        <td>Critique Token</td>
        <td>Useful response to the prompt</td>
        <td>5, 4, 3, 2, 1</td>
    </tr>
</table> 
<p align="justify">
- Why is all this a good idea? (Benchmarks) 
- What are challenges?  
For more detailed information regarding training, how they inserted these special tokens and data collection I'll recommend checking out the <a href="https://arxiv.org/pdf/2310.11511">paper</a>.
</p>
<h3 id="corrective-rag">Corrective RAG</h3>
<p align="justify">
The Corrective Retrieval Augmented Generation method to improve the accuracy and robustness of LM by re-incorporating information from retrieved documents. This is accomplished by the introduction of three additional components. First a lightweight evaluator model that assesses the quality of retrieved documents and determines which to use, ignore or request more documents based on a score. The second new component is an incorporated web search to extend the information base and get more up-to-date information. The last new addition is a decompose-then-recompose algorithm for selectively focusing on key information and filtering out irrelevant information that also makes use of the evaluator model. The Figure X down below gives a nice overview of the CRAG method at inference.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/crag.png#center"
         alt="An overview of Corrective Retrieval Augmented Generation" width="100%"/> <figcaption>
            <p>Figure X. An overview of CRAG at inference [4]</p>
        </figcaption>
</figure>

<p align="justify">
The Knowledge Refinement process is quite straightforward and starts by segmenting each retrieved document into fine-grained knowledge strips through heuristic rules. Then the retrieval evaluator is used to calculate a relevance score of each knowledge strip. Based on these scores the strips are filtered and recomposed via concatenation in order.
</p>
<h3 id="rag-fusion">RAG Fusion</h3>
<p align="justify">
</p>
<h2 id="evaluation">Evaluation</h2>
<ul>
<li>How are RAG pipelines evaluated??</li>
</ul>
<p>since RAG systems are comprised of multiple components, so you can evaluate single components or just the performance of the whole pipeline without looking into the components</p>
<p>Huggingface RAG Evaluation:</p>
<ul>
<li>
<p>build a synthetic evaluation dataset and use a LLM as a judge to compute the accuracy of the system</p>
</li>
<li>
<p>RAG Finetuning??</p>
</li>
<li>
<p>Long Context LLMs</p>
</li>
<li>
<p>Multimodal RAG</p>
</li>
<li>
<p>Advanced RAG Pipelines</p>
</li>
</ul>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/pdf/2404.10981">[1]</a> Y. Huang &amp; J.X. Huang &ldquo;A Survey on Retrieval-Augmented Text Generation for Large Language Models&rdquo; (2024).</p>
<p><a href="https://www.pinecone.io/learn/advanced-rag-techniques/">[2]</a> Roie Schwaber-Cohen &ldquo;Advanced RAG Techniques&rdquo; (2024).</p>
<p><a href="https://arxiv.org/pdf/2310.11511">[3]</a> Asai et al. &ldquo;Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2401.15884">[4]</a> Yan et al. &ldquo;Corrective Retrieval Augmented Generation&rdquo; (2024)</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Parameter-Efficient LLM Finetuning</title>
      <link>http://localhost:1313/posts/2023-09-08-parameter-efficient-llm-finetuning/</link>
      <pubDate>Fri, 08 Sep 2023 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2023-09-08-parameter-efficient-llm-finetuning/</guid>
      <description>Finetuning of Large Language Models in a parameter efficient way.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
I am already quite late to the Large Language Models (LLM) party but better starting late than never. In this post I am going over a couple of popular techniques for fine-tuning these models without the need of training the whole thing which would be quite expensive. But before I dive into the techniques we should talk about what LLMs and fine-tuning are, why we need it and what the current problems are.
</p>
<p align="justify">
LLMs are based on the <a href="http://jalammar.github.io/illustrated-transformer/">Transformer</a> architecture, like the GPTs or BERT, which have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks, like Translation, Question Answering, Summarization etc. The paradigm these days is to train such a model on generic web-scale data (basically the whole internet) and fine-tune it on a downstream task. Fine-tuning in this case just means that you train the model further with a small dataset you collected for a specific task. This fine-tuning results in most cases in huge performance gains when compared to using just the LLM as is (e.g. zero-shot inference).  
</p>
<p align="justify">
However, based on the current model sizes a full fine-tuning becomes infeasible to train on consumer hardware (which makes me a bit sad). In addition when you want to store and deploy multiple fine-tuning model instances for different tasks, this becomes very expensive because they are the same size as the base LLM. Because of these two main problems, people came up with more efficient methods for doing this which are referred to as Parameter-efficient fine-tuning (PEFT) procedures. These approaches basically enable you to get performance comparable to full fine-tuning while only having a small number of trainable parameters.  
</p>
<p align="justify">
PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. They have also been shown to be better than fine-tuning in cases when you don't have that much task data and in out-of-domain scenarios. By saving just the extra model parameters you also solve the portability and cost problem because the PEFT checkpoints are much smaller, just a few MB, in contrast to the LLM checkpoints that need multiple GB. The small trained weights from PEFT approaches are added on top of the pretrained LLM. So the same LLM can be used for multiple tasks by adding small weights without having to replace the entire model. huggingface provides a nice <a href="https://github.com/huggingface/peft">library</a> for a lot of PEFT methods. 
</p>
<p align="justify">
In the following I will going to dive a bit deeper in how some of these methods work. We will cover Prefix Tuning, Adapters and Low Rank Adaptation (LoRA).
</p>
<h2 id="prefix-tuning">Prefix Tuning</h2>
<p align="justify">
There exists different forms of prompt tuning, one of them is hard prompt tuning, also called prompt engineering or in context learning, where you change the input to the model, e.g. provide examples for how the model should answer. This form of tuning is non-differentiable, so the weights don't change and you don't add any weights. Prefix tuning is a form of soft prompt tuning where you concatenate a trainable tensor to the embeddings of the input tokens. This tensor is trainable, so you add a small amount of weights. The model parameters are kept frozen and you just optimize a small continuous task-specific vector which is called prefix.  
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/parameter_efficient_llm_finetuning/finetuning_vs_prefix_tuning.png#center"
         alt="Finetuning vs. Prefix Tuning" width="70%"/> <figcaption>
            <p>Figure 1. Finetuning vs. Prefix Tuning</p>
        </figcaption>
</figure>

<p align="justify">
Prefix tuning shines especially in low-data settings and the extrapolation to new tasks, e.g. summarization of texts with different topics, is better. It has up to 1000x fewer parameters to learn than in a fine-tuning setting. Another very cool thing is, that it enables personalization by having a different prefix per user trained only on the user data (no cross-contamination) and you could do batch processing of multiple users/tasks and one LLM. 
</p>
<h2 id="adapters">Adapters</h2>
<p align="justify">
Adapter methods are somewhat related to prefix tuning as they also add additional trainable parameters to the original LLM. Instead of prepending prefixes to the input embeddings, you add adapter layers into the transformer blocks. As you can see in the figure down below, the fully connected network in the adapter module has a bottleneck structure similar to an autoencoder which keeps the number of added parameters low and makes the method quite efficient.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/parameter_efficient_llm_finetuning/adapter_module_layout.png#center"
         alt="Adapter Module Layout" width="70%"/> <figcaption>
            <p>Figure 2. Adapter Module Layout</p>
        </figcaption>
</figure>

<p align="justify">
In the orginal adapter paper, the authors trained a BERT model with this method and reached a modeling performance comparable to a fully finetuned BERT model while only requiring the training of 3.6% of the parameters. Based on the original prefix tuning paper, the performance of adapters matches the performance of prefix tuning of 0.1% of the parameters at 3%, which makes it less efficient.
</p>
<p align="justify">
Llama-Adapter combines the two ideas of prefix tuning and adapters for LLaMA models from Meta. Each transformer block in the model has its own distinct learned prefix, allowing for more tailored adaptation across different model layers. On top of that, LLaMA-Adapter introduces a zero-initialized attention mechanism coupled with gating.
</p>
<h2 id="low-rank-adaptation-lora">Low Rank Adaptation (LoRA)</h2>
<p align="justify">
LoRA and QLoRA 
</p>
<h2 id="conclusion">Conclusion</h2>
<p align="justify">
</p>
<h2 id="notes">Notes</h2>
<p align="justify">
- in-context learning -> models are able to perform tasks by providing them the right context (examples of your task) -> this is cool when you only have access to the model via an API or UI (this only works with generative models like the GPTs) 
- this works fine in some cases but for more specific things you still have to gather a dataset for the task and the specific domain and then finetune that model, this would result in superior results than just in-context learning (I don't know why they call this learning)
- what does finetuning even mean here? -> it basically means that you want to train the model on your task specific dataset 
- there are two ways to do this: 
1. the feature based approach, where you keep the transformer weights frozen (no training here) and just train a classifier which you feed the output embeddings of the frozen LLM (e.g. logistic regression model, random forest or XGBoost)
2. finetuning, where you train the whole model or a bigger portion of it (e.g. your just train the output layers after the Transformer blocks) 
- in general training the whole model will give you the best results but it is also the most expensive one    
- the problem is, that the currently best performing models are so big that there is no way that you can finetune this thing on your personal computer anymore
- so the question of how to utilize them more efficiently and effectively has become a very active area 
- people want to run those models on their laptops or desktop pcs without buying multiple graphics cards 
- so what do you do when in-context learning doesnt cut it for your use case and you don't have the compute to finetune those bigger models?
- researchers developed several techniques to make the finetuning of LLMs much more efficient by training just a small number of parameters
- I try to explain some of the most popular parameter-efficient finetuning techniques (PEFT) in this blogpost: Prefix Tuning, Adapters, Low Rank Adaptation (LoRA)
<p>and Baogao et. al wrote a paper comparing a lot of these methods and introduced HiWi</p>
</p>
<h2 id="references">References</h2>
<p>Sebastian Raschka - Understanding Parameter Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters (2023)
<a href="https://lightning.ai/pages/community/article/understanding-llama-adapters/">https://lightning.ai/pages/community/article/understanding-llama-adapters/</a></p>
<p>Sebastian Raschka - Parameter-Efficient LLM Finetuning With Low Rank Adaptation (2023)
<a href="https://lightning.ai/pages/community/tutorial/lora-llm/">https://lightning.ai/pages/community/tutorial/lora-llm/</a></p>
<p>Edward Hu et. al - LoRA: Low-Rank Adaptation of Large Language Models (2021)
<a href="https://arxiv.org/pdf/2106.09685.pdf">https://arxiv.org/pdf/2106.09685.pdf</a></p>
<p>Renrui Zhang et. al - LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (2023)
<a href="https://arxiv.org/pdf/2303.16199.pdf">https://arxiv.org/pdf/2303.16199.pdf</a></p>
<p>Xiang Lisa Li - Prefix-Tuning: Optimizing Continuous Prompts for Generation (2021)
<a href="https://arxiv.org/pdf/2101.00190.pdf">https://arxiv.org/pdf/2101.00190.pdf</a></p>
<p>Tim Dettmers et. al - QLORA: Efficient Finetuning of Quantized LLMs (2023)
<a href="https://arxiv.org/pdf/2305.14314.pdf">https://arxiv.org/pdf/2305.14314.pdf</a></p>
<p>Parameter-Efficient Fine-Tuning without Introducing New Latency (2023)
<a href="https://arxiv.org/pdf/2305.16742.pdf">https://arxiv.org/pdf/2305.16742.pdf</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Self-Supervised Learning for Computer Vision</title>
      <link>http://localhost:1313/posts/2022-03-15-self-supervised-learning-for-computer-vision/</link>
      <pubDate>Tue, 15 Mar 2022 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2022-03-15-self-supervised-learning-for-computer-vision/</guid>
      <description>Introduction to Self-Supervised Learning in the Computer Vision space.</description>
      <content:encoded><![CDATA[<p align="justify">
</p>
<br>
<h2 id="self-supervised-learning">Self-Supervised Learning</h2>
<ul>
<li>What is it?</li>
<li>Why is it important?</li>
</ul>
<h3 id="contrastive-learning">Contrastive Learning</h3>
<h2 id="popular-methods">Popular Methods</h2>
<h3 id="heading"></h3>
]]></content:encoded>
    </item>
    
    <item>
      <title>A Peek into Deep Reinforcement Learning - Part II</title>
      <link>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/</link>
      <pubDate>Mon, 14 Mar 2022 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/</guid>
      <description>Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space.</description>
      <content:encoded><![CDATA[<p align="justify">
In this second part on Deep Reinforcement Learning we are going to explore some of the combined methods (look into <a href="https://johanngerberding.github.io/posts/A-Peek-into-Deep-Reinforcement-Learning-Part-1">Part I</a> if you don't know what I mean with combined methods). We will start with the so called <b>Advantage Actor-Critic</b> algorithm which combines some of the concepts we have learned about in the first part. Then we dive into the very popular <b>Proximal Policy Optimization</b> algorithm. Moreover I will give you an overview of the <b>Alpha(Go)Zero</b> algorithm from <a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">DeepMind</a>. In the third part of this series we are going to talk more about <a href="https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c">Imitation Learning</a> and <b>Model-based RL</b>.
</p>
<br>
<h2 id="advantage-actor-critic-a2c">Advantage Actor-Critic (A2C)</h2>
<p align="justify">
As mentioned before, Advantage Actor-Critic (A2C) algorithms combine the ideas from policy gradient methods (e.g. <i>REINFORCE</i>) and a learned value function (e.g. <i>DQN</i>). Here we reinforce a policy with a learned reinforcing signal generated by a learned value function. A2C algorithms therefore consist of two jointly learned components:
</p>
<ul>
<li>an <b>actor</b> which learns a parameterized policy and</li>
<li>a <b>critic</b> which learns a value function to evaluate state-action pairs (it provides a reinforcing signal to the actor)</li>
</ul>
<p align="justify">
The motivation for this is that a learned reinforcing signal can be much more informative for a policy than the rewards available from an environment. Instead of learning $Q^{\pi}(s,a)$ or $V^{\pi}$, it is common to learn the so called <b>advantage function</b> $A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$ as the reinforcing signal. The key idea behind this is that it is better to select an action based on how it performs relative to the other actions available in a particular state, instead of using the absolute value (hence the name <i>advantage</i> actor-critic). The actors learn a parameterized policy $\pi_{\theta}$ using the policy gradient. This is similar to the REINFORCE algorithm, but instead of the Monte-Carlo estimate $R_{t}(\tau)$, the advantage is used:
</p>
<p>$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{t} \Big[ A_{t}^{\pi} \nabla_{\theta} \log \pi_{\theta} (a_{t} | s_{t})\Big]
$$</p>
<p align="justify">
The critic is responsible for learning how to evaluate state-action-pairs and using this to generate $A^{\pi}$. To estimate the advante function, we will go over two possible methods: <b>n-step returns</b> and <b>Generalized Advantage Estimation</b> (GAE). In general the advantage function measures the extent to which an action is better or worse than the policy's average action in a particular state:
</p>
<p>$$
A^{\pi}(s_{t},a_{t}) = Q^{\pi}(s_{t},a_{t}) - V^{\pi}(s_{t})
$$</p>
<p align="justify">
One benefit of using the advantage instead of $Q^{\pi}$ or $V^{\pi}$ is that it avoids penalizing an action for the policy currently being in a particularly bad state, like in the following example:
</p>
<p>$$
Q^{\pi}(s,a) = 110, \quad V^{\pi}(s) = 100, \quad A^{\pi}(s,a) = 10
$$</p>
<p>$$
Q^{\pi}(s,a) = -90, \quad V^{\pi}(s) = -100, \quad A^{\pi}(s,a) = 10
$$</p>
<p align="justify">
The advantage function is better able to capture the long-term effects of an action because it considers all future time steps while ignoring the effects of all the actions to date.
</p>
<br>
<h3 id="n-step-returns">n-step Returns</h3>
<p align="justify">
As seen before, to calculate $A^{\pi}$ we need estimates for both $Q^{\pi}(s,a)$ and $V^{\pi}(s)$. In the n-step Returns method we achieve this by learning $V^{\pi}(s)$ and estimating $Q^{\pi}(s,a)$ from it:
</p>
<p>$$
Q^{\pi}(s,a) = \mathbb{E}_{\tau \sim \pi} \Big[ r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} &hellip; + \gamma^{n} r_{t+n} \Big] + \gamma^{n+1} \hat{V}^{\pi}(s_{t+n+1})
$$</p>
<p align="justify">
The expectation part of the Q-value estimate show above is calculated based on a 3-step return, which means that we use our collected trajectory data to look three steps in the future and sum up the rewards multiplied by a discounting factor $\gamma^{t}$. This part of the equation is unbiased but has a high variance because it comes from only one trajectory. $n$ is a hyperparameter that needs to be tuned. The bigger the value of $n$, the higher the variance of the estimate. The return after the n-th step is calculated by the critic network. It has lower variance since it reflects an expectation over all of the trajectories seen so far, but it is biased because it is calculated using a function approximator. From this we now get a formula for estimating the advantage:
</p>
<p>$$
A_{NSTEP}^{\pi}(s_{t}, a_{t}) = Q^{\pi}(s_{t}, a_{t}) - V^{\pi}(s_{t})
$$</p>
<p>$$
A_{NSTEP}^{\pi}(s_{t}, a_{t}) \approx r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + &hellip; + \gamma^{n} r_{t+n} + \gamma^{n+1} \hat{V}^{\pi}(s_{t+n+1}) - \hat{V}^{\pi}(s_{t})
$$</p>
<br>
<h3 id="generalized-advantage-estimation-gae">Generalized Advantage Estimation (GAE)</h3>
<p align="justify">
Generalized Advantage Estimation was proposed as an improvement over the n-step returns estimate for the advantage function. It addresses the problem of having to explicitly choose the number of steps of returns $n$. The main idea is, instead of picking one value of $n$, we mix multiple values by calculating the advantage using a weighted average of individual advantages calculated with $n = 1, 2, 3, ..., k$. This significantly reduces the variance of the estimator while keeping the bias introduced as low as possible.
</p>
<p>$$
A_{GAE}^{\pi}(s_{t}, a_{t}) = \sum^{\infty}_{l=0}(\gamma \lambda) \delta_{t+l}
$$</p>
<p>$$
\text{where }  \delta_{t} = r_{t} + \gamma V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})
$$</p>
<p align="justify">
GAE is taking a weighted average over a number of advantage estimators with different bias and variance. It weights the high-bias, low-variance 1-step advantage the most but also includes contributions from lower-bias, higher-variance estimators using $2,3,...,n$ steps. The contributions decay at an exponential rate as the number of steps increases and the decay rate gets controlled by the coefficient $\lambda$ (the larger $\lambda$, the higher the variance). In contrast to $n$, $\lambda$ represents a softer choice than $n$, which means smaller values of $\lambda$ will more heavily weight the V-function estimate, whilst larger values will weight the actual rewards more.
</p>
<br>
<h3 id="algorithm--network-architecture">Algorithm &amp; Network Architecture</h3>
<p align="justify">
But how do we learn the value function $V^{\pi}$? We have different possibilities here, one of them is <i>Temporal Difference Learning</i>, similar to DQN. First we parameterize $V^{\pi}$ with $\theta$, then we generate $V_{tar}^{\pi}$ for each of the experiences an agent gathers. Finally we minimize the distance between $\hat{V}^{\pi}(s; \theta)$ and $V^{\pi}_{tar}$ using a simple regression loss such as MSE. You can use different methods to generate $V_{tar}^{\pi}$:
</p>
<ul>
<li>n-step estimate:</li>
</ul>
<p>$$
V_{tar}^{\pi}(s) = r + \hat{V}^{\pi}(s&rsquo;; \theta)
$$</p>
<p>$$
V_{tar}^{\pi}(s_{t}) = r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + &hellip; + \gamma^{n} r_{t+n} + \gamma^{n+1} \hat{V}^{\pi}(s_{t+n+1})
$$</p>
<ul>
<li>Monte-Carlo estimate:</li>
</ul>
<p>$$
V_{tar}^{\pi}(s_{t}) = \sum_{t&rsquo;=t}^{T}\gamma^{t&rsquo;-t} r_{t&rsquo;}
$$</p>
<ul>
<li>GAE:</li>
</ul>
<p>$$
V_{tar}^{\pi}(s_{t}) = A_{GAE}^{\pi}(s_{t}, a_{t}) + \hat{V}^{\pi}(s_{t})
$$</p>
<p align="justify">
The choice is often related to the method used to estimate the advantage. It is also possible to use a different, more complicated optimization procedure (trust region method) when learning the value function $\hat{V}^{\pi}$ which you can find in the original <a href="https://arxiv.org/abs/1506.02438">GAE paper</a>.
</p>
<p align="justify">
You can run the actor-critic algorithm online as well as batched, down below you see the simplified steps of an online version:
</p>
<ol>
<li>take action $\mathbf{a} \sim \pi_{\theta}(\mathbf{a}|\mathbf{s})$, get $(\mathbf{s}, \mathbf{a}, \mathbf{s&rsquo;}, r)$</li>
<li>update $\hat{V}_{\phi}^{\pi}$ using target $r + \gamma \hat{V}_{\phi}^{\pi}(\mathbf{s&rsquo;})$</li>
<li>evaluate $\hat{A}^{\pi}(\mathbf{s}, \mathbf{a}) = r(\mathbf{s}, \mathbf{a}) + \gamma \hat{V}_{\phi}^{\pi}(\mathbf{s&rsquo;}) - \hat{V}_{\phi}^{\pi}(\mathbf{s})$</li>
<li>$\nabla_{\theta} J(\theta) \approx \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}|\mathbf{s})\hat{A}^{\pi}(\mathbf{s}, \mathbf{a})$</li>
<li>$\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)$</li>
<li><strong>repeat</strong></li>
</ol>
<p align="justify">
To learn the parameterized functions for the actor and the critic, you can use two separate neural networks. But it is also possible and conceptually appealing to use a network structure with shared parameters, because learning $\pi$ and $V^{\pi}(s)$ for the same task are related and they share the same input. Moreover your total number of learnable parameters is reduced which makes it overall more sample efficient. The sharing of the lower layers could be interpreted as learning a common representation of the state space. Upper layers are separate because the networks have different tasks, so their structure and size can be different. But sharing parameters has also some downsides:
</p>
<ul>
<li>learning becomes more unstable because the two gradients can have different scales</li>
<li>we need to balance the two which is typically accomplished by adding a scalar weight to one of the losses for scaling</li>
<li>this adds another hyperparameter to tune</li>
</ul>
<br>
<h2 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h2>
<p align="justify">
Two of the main problems with policy gradient algorithms like REINFORCE are:
</p>
<ul>
<li>susceptibility to performance collapse (agent suddenly performs bad)</li>
<li>sample-inefficiency (on policy - no data reuse)</li>
</ul>
<p align="justify">
The PPO paper addresses both of these issues by introducing a <b>surrugate objective</b> which avoids performance collapse by guarantueeing monotonic policy improvement and enables off-policy data reuse. This leads often to more stable and sample efficient training. PPO methods have some benefits of <a href="https://arxiv.org/abs/1502.05477">trust region policy optimization</a> (TRPO) but are much simpler to implement, more general and have better sample efficiency. PPO can be used to extend f.e. REINFORCE of Actor-Critic methods by replacing their original objective $J(\pi_{\theta})$ with the surrogate objective. It is currently one of the most popular policy gradient algorithms. The paper presents two different variants of the objective function which we are diving into both.
</p>
<h3 id="adaptive-kl-penalty">Adaptive KL Penalty</h3>
<p align="justify">
This PPO version incorporates the <i>Kullback-Leiber Divergence</i> (KL) as a penalty into the surrogate objective. The KL is a measure for information loss between two distributions, you can find a nice explanation <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">here</a>. The objective function looks like the following:
</p>
<p>$$
J^{KPLEN}(\theta) = \max_{\substack{\theta}} \mathbb{E}_{t}\Big[r_{t}(\theta)A_{t}-\beta KL \big(\pi_{\theta}(a_{t}|s_{t}) \parallel \pi_{\theta_{old}}(a_{t}|s_{t})\big)\Big]
$$</p>
<p>$$
\text{with } r(\theta) = \frac{\pi(a_{t}|s_{t})}{\pi_{old}(a_{t}|s_{t})}
$$</p>
<p align="justify">
$\beta$ is an adaptive coefficient which controls the size of the KL penalty (the larger $\beta$, the larger the difference between $\pi$ and $\pi_{old}$). Because $\beta$ is a new hyperparameter that has to work for different problems in different scenarios, it is not a constant value. Instead $\beta$ gets recalculated after every policy update based on a heuristic which allows it to adapt over time:
</p>
<ul>
<li>set target value $\delta_{tar}$ for the expectation of KL, init $\beta$ randomly</li>
<li>use multiple epochs of minibatch SGD, optimize the KL-penalized surrogate objective</li>
<li>compute $\delta = \mathbb{E}_{t}\Big[ KL \big(\pi_{\theta}(a_{t}|s_{t}) \parallel \pi_{\theta_{old}}(a_{t}|s_{t})\big) \Big]$
<ul>
<li>if $\delta &lt; \delta_{tar}/1.5$ then $\beta \leftarrow \beta/2$</li>
<li>else if $\delta &gt; \delta_{tar} \times 1.5$ then $\beta \leftarrow \beta \times 2$</li>
<li>else pass</li>
</ul>
</li>
</ul>
<p align="justify">
This approach is simple to implement but you still have to choose a target value for $\delta_{tar}$. In general it is computationally expensiv e to calculate the KL, which is one reason why the second form of the PPO is often preferred.
</p>
<h3 id="clipped-surrogate-objective">Clipped Surrogate Objective</h3>
<p align="justify">
The clipped surrogate objective is much simpler and omits the calculation of the KL entirely:
</p>
<p>$$
J^{CLIP}(\theta) = \mathbb{E}_{t} \Big[ \min \big( r_{t}(\theta) A_{t}, clip(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon)A_{t} \big) \Big]
$$</p>
<p align="justify">
$\epsilon$ is a hyperparameter which defines the clipping neighbourhood and can be decayed during training. This objective function prevents parameter updates which could cause large and risky changes to the policy $\pi_{\theta}$. As mentioned before, you can use both of the described objectives with e.g. Actor-Critic methods or REINFORCE. For details take a look at the <a href="https://arxiv.org/abs/1707.06347">paper</a> itself.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/PPO_ac_algorithm.png#center"
         alt="PPO algorithm Actor-Critic style" width="90%"/> <figcaption>
            <p>Figure 1. Algorithm - PPO Actor-Critic [2]</p>
        </figcaption>
</figure>

<p align="justify">
Down below you can find a few results from the paper on several MuJoCo environments after one million steps of training. At that times it outperformed the previous methods on almost all the continuous control environments.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/PPO_results.png#center"
         alt="MuJoCo environment results" width="100%"/> <figcaption>
            <p>Figure 2. MuJoCo environment results [2]</p>
        </figcaption>
</figure>

<br>
<h2 id="alphagozero">Alpha(Go)Zero</h2>
<p align="justify">
<i>AlphaGo Zero</i> is the successor of DeepMinds <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> which was the first program to defeat a world champion in the game of Go. It differs from AlphaGo in several important aspects. It was trained solely by self-play without any use of expert data or human supervision, but knowing the rules of the game (model-based). It uses only a single neural network and defeated AlphaGo by 100 games to 0. With <i>AlphaZero</i>, DeepMind generalized and extended the method to the games Chess and Shogi.
</p>
<p align="justify">
<i>AlphaGo Zero</i> combines a neural network and <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a> (MCTS) in an elegant policy iteration framework to achieve stable learning and rapid improvement. Lets start by taking a closer look at the neural network.
</p>
<h3 id="neural-network">Neural Network</h3>
<p align="justify">
The authors used a deep Convolutional Neural Network $f_{\theta}$ which is parameterized by $\theta$ and takes as input the state $\mathbf{s}$ of the board and outputs both, move probabilities and a value $(\mathbf{p}, v) = f_{\theta}(s)$. It combines the roles of the policy network and value network into a single architecture, similar to the shared network which can be used in Actor-Critic methods. The vector of the move probabilities $\mathbf{p}$ represents the probability of selecting each move $a$, $\mathbf{p}_{a} = Pr(a|s)$. The value $v$ is a scalar evaluation, estimating the probability of the current player winning from position $\mathbf{s}$. The network is trained at the end of each game of self-play based on the data of each time step $t$ which is stored in the form $(s_{t}, \pi_{t}, z_{t})$. $\pi_{t}$ represents the search probabilities generated by MCTS and $z_{t} \in {-1, 1}$ is the game winner from the perspective of the current player (gets added at the end of the game to each timestep). The loss function $l$ sumsof the mean-squared error and cross-entropy loss:
</p>
<p>$$
l = (z - v)^{2} - \pi^{T} \log \mathbf{p} + c || \theta ||^{2}
$$</p>
<p align="justify">
where $c$ is a hyperparameter controlling the level of L2 weight regularization. Figure 3. down below illustrates this self-play RL procedure in <i>AlphaGo Zero</i>.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/AlphaGoZero_self_play.png#center"
         alt="AlphaGo Zero self-play and neural network training" width="70%"/> <figcaption>
            <p>Figure 3. AlphaGo Zero - Self-play and neural network training [7]</p>
        </figcaption>
</figure>

<h3 id="monte-carlo-tree-search">Monte Carlo Tree Search</h3>
<p align="justify">
The neural network we just talked about is trained from games of self-play by a novel reinforcement learning algorithm. In each position $s$, an MCTS is executed, guided by the neural network $f_{\theta}$ and it outputs search probabilities $\pi$ of playing each move. Based on these you can usually select a much stronger move than based on the raw move probabilities $\mathbf{p}$ output by the neural network $f_{\theta}(s)$ which makes MCTS to kind of a policy improvement operator. It uses the neural network $f_{\theta}$ for its simulations. Each edge $(s,a)$ in the search tree stores a prior probability $P(s,a)$, a visit count $N(s,a)$ and an action value $Q(s,a)$. Each simulation run starts from the root state and iteratively selects moves that maximize an upper confidence bound
</p>
<p>$$
U(s,a) = Q(s,a) + c * P(s,a) * \frac{\sum_{b}N(s,b)}{1 + N(s,a)}
$$</p>
<p align="justify">
where c is a hyperparameter which controls the degree of exploration. We initialize our empty search tree with $\mathbf{s}$ as the root. A single simulation run proceeds as follows. First we compute the action $a$ that maximizes the upper confidence bound $U(s,a)$. We play this action and if the resulting state $\mathbf{s'}$ exists in our tree, we recursively search on $\mathbf{s'}$. If it doesn't exist, it gets added to the tree and $Q(s',a)$ and $N(s',a)$ are initialized to 0 for all $a$. The neural network is used to initialize $P(s', \cdot) = \mathbf{p}$ and $v(s') = v_{\theta}(s')$. Instead of performing a rollout, $v(s')$ gets propagated up along the path seen in the current simulation and update all $Q(s,a)$ values. If on the other hand a terminal state gets encountered, the reward gets propagated.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/AlphaGoZero_mcts.png#center"
         alt="AlphaGo Zero Monte Carlo Tree Search Process" width="90%"/> <figcaption>
            <p>Figure 4. AlphaGo Zero - MCTS Search Process [7]</p>
        </figcaption>
</figure>

<p align="justify">
DeepMind trained the AlphaGo Zero system for approximately three days, generating 4.9 million games of self-play, using 1,600 simulations for each MCTS. The neural net they used consisted of 20 residual blocks (they also trained a bigger network for longer).
</p>
<br>
<h2 id="summary">Summary</h2>
<p align="justify">
In this post we looked at three combined RL methods, A2C, PPO and AlphaGoZero. As Alpha(Go)Zero is a more specialized method for games, PPO and A2C are more generally applicable. I hope I was able to give you an understandable insight into the three algorithms and their most important components. In the next post of this series I am going to dive a bit deeper into model-based RL and imitation learning. Until then, stay tuned and healthy!
</p>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/abs/1602.01783">[1]</a> Mnih et al. &ldquo;Asynchronous Methods for Deep Reinforcement Learning&rdquo; (2016).</p>
<p><a href="https://arxiv.org/abs/1707.06347">[2]</a> Schulman et al. &ldquo;Proximal Policy Optimization Algorithms&rdquo; (2017).</p>
<p><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">[3]</a> Kullback-Leiber Divergence Explained (2017).</p>
<p><a href="https://arxiv.org/abs/1502.05477">[4]</a> Schulman et al. &ldquo;Trust Region Policy Optimization&rdquo; (2015).</p>
<p><a href="https://arxiv.org/abs/1712.01815">[5]</a> Silver et al. &ldquo;Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm&rdquo; (2017).</p>
<p><a href="https://arxiv.org/abs/1506.02438">[6]</a> Schulman et al. &ldquo;High-Dimensional Continuous Control Using Generalized Advantage Estimation&rdquo; (2015).</p>
<p><a href="https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf">[7]</a> Silver et al. &ldquo;Mastering the game of Go without human knowledge&rdquo; (2017).</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>A Peek into Deep Reinforcement Learning - Part I</title>
      <link>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-1/</link>
      <pubDate>Wed, 26 Jan 2022 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-1/</guid>
      <description>Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms.</description>
      <content:encoded><![CDATA[<p align="justify">
I guess many of you interested in the field of Machine Learning have heard about <a href="https://deepmind.com/">DeepMind</a> creating a system defeating the best professional human player in the Game of Go, called <i>AlphaGo</i>. I personally have never played Go before, so at first I wasn't aware of its complexity. Two years later they presented AlphaGo's successor called <i>AlphaZero</i>, which learned from scratch to master not only Go but also Chess and Shogi and defeated <i>AlphaGo</i> 100-0 without the use of domain knowledge or any human data. In December 2020 they presented the next evolution of this algorithm, called MuZero, which was able to master Go, Chess, Shogi and nearly all Atari games without knowing the rules of the game or the dynamics of the environment. After reading all of this it got my hooked and I wanted to know more about the algorithms and theory behind this magic - <b>Reinforcement Learning (RL)</b>. In fact RL is around for quite some time now but in the last couple of years it really took off. Despite the truly impressive results of <a href="https://deepmind.com/">DeepMind</a>, however, after a short research I also quickly realized that there are still relatively few viable real-world applications of reinforcement learning. However, I hope that this will change in the near future. To be honest, I am most excited about the applications in the field of robotics (e.g. <a href="https://openai.com/blog/solving-rubiks-cube/">Solving Rubik's Cube with Robotic Hand</a>, <a href="https://ai.facebook.com/blog/ai-now-enables-robots-to-adapt-rapidly-to-changing-real-world-conditions/">Robotic Motor Adaptation</a>) and autonomous driving. This prompted me to learn more about the field of Deep Reinforcement Learning and share my learnings with you in this blogpost.
</p>
<h2 id="introduction-into-reinfocement-learning">Introduction into Reinfocement Learning</h2>
<p align="justify">
Before we dive into the algorithms and all that cool stuff I want to start with a short introduction into the most essential concepts and the terminology of RL. After that I give an overview of the framework for modeling those kinds of problems, called <b>Markov Decision Processes (MDP)</b> and I will present you a way to categorize deep RL algorithms.
</p>
<h3 id="concepts-and-terminology">Concepts and Terminology</h3>
<p align="justify">
RL in general is concerned with solving sequential decision-making problems (e.g. playing video games, driving, robotic control, optimizing inventory) and such problems can be expressed as a system consisting of an <b>agent</b> which acts in an <b>environment</b>. These two are the core components of RL. The environment produces information which describes the <b>state</b> of the system and it can be considered to be anything that is not the agent. So, what is an agent then? An agent "lives" in and interacts with an environment by observing the state (or at least a part of it) and uses this information to select between actions to take. The environment accepts these actions and transitions into the next state and after that returns the next state and a <b>reward</b> to the agent. A reward signal is a single scalar the environment sents to the agent which defines the goal of the RL problem we want to solve. This whole cycle I have described so far is called one time-step and it repeats until/if the environment terminates (Figure 1.).
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/reinforcement_learning_loop.png#center"
         alt="Reinforcement Learning Control-Loop" width="70%"/> <figcaption>
            <p>Figure 1. Reinforcement Learning Control-Loop [1]</p>
        </figcaption>
</figure>

<p align="justify">
The action selection function the agent uses is called a <b>policy</b>, which maps states to actions. Every action will change the environment and affect what an agent observes and does next. To determine which actions to take in different situations every RL problems needs to have an objective or goal which is described by the sum of rewards received over time. The goal is to maximize the objective by selecting good actions which the agent learns by interacting with the environment in a process of trial-and-error combined with using the reward signals it receives to reinforce good actions. The exchange signal is often called <b>experience</b> and described as tuple of $(s_{t}, a_{t}, r_{t})$. Moreover we have to differentiate between <i>finite</i> and <i>infinite</i> environments. In finite environments $t=0,1,...,T$ is called one <b>episode</b> and a sequence of experiences over an episode $\tau = (s_{0}, a_{0}, r_{0}), (s_{1}, a_{1}, r_{1}), ...$ is called a <b>trajectory</b>. An agent typically needs many episodes to learn a good policy, ranging from hundreds to millions depending on the complexity of the problem. Now lets describe the states, actions and rewards a bit more formally:
</p>
<ul>
<li>$s_{t} \in \mathcal{S}$ is the state, $\mathcal{S}$ is the state space</li>
<li>$a_{t} \in \mathcal{A}$ is the action, $\mathcal{A}$ is the action space</li>
<li>$r_{t} = \mathcal{R}(s_{t}, a_{t}, s_{t+1})$ is the reward, $\mathcal{R}$ is the reward function</li>
</ul>
<p align="justify">
Next we are diving into the framework for modeling those interactions between the agent and the environment called Markov Decision Processes.
</p>
<h3 id="rl-as-an-markov-decision-process">RL as an Markov Decision Process</h3>
<p align="justify">
MDP in general is a mathematical framework for modeling sequential decision making and in RL the transitions of an environment between states is described as an MDP. The <b>transition function</b> has to meet the <b>Markov property</b> which assumes that the next state $s_{t+1}$ only depends on the previous state $s_{t}$ and action $a_{t}$ instead of the whole history of states and actions. When we talk about a state here it is also important to distinguish between the <b>observed state</b> $s_{t}$ from the agent and the environments <b>internal state</b> $s_{t}^{int}$ used by the transition function. In an MDP $s_{t} = s_{t}^{int}$ but in many interesting real-world problems the agent has only limited information and $s_{t} \neq s_{t}^{int}$. In those cases the environment is described as a <b>partially oberservable</b> MDP, in short <b>POMDP</b>.
</p>
<p>All we need for the formal MDP description of a RL problem is a 4-tuple $\mathcal{S}$, $\mathcal{A}$, $P(.)$ and $\mathcal{R}(.)$:</p>
<ul>
<li>$\mathcal{S}$ is the set of states</li>
<li>$\mathcal{A}$ is the set of actions</li>
<li>$P(s_{t+1} \mid s_{t}, a_{t})$ is the state transition function of the environment</li>
<li>$\mathcal{R}(s_{t}, a_{t}, s_{t+1})$ is the reward function of the environment</li>
</ul>
<p align="justify">
It is important to note that agents to have access to the transition function of the reward function, they only get information about these functions through the $(s_{t}, a_{t}, r_{t})$ tuples. The objective of an agent can be formalized by the <b>return</b> $R(\tau)$ using a trajectory from an episode
</p>
<p>$$
R(\tau) = r_{0} + \gamma r_{1} + \gamma^{2} r_{2} + &hellip; + \gamma^{T} r_{T} = \sum_{t=0}^{T} \gamma^{t} r_{t}
$$</p>
<p align="justify">
$\gamma$ describes a <i>discount factor</i> which changes the way future rewards are valued. The objective $J(\tau)$ is simply the expectation of the returns over many trajectories 
</p>
<p>$$
J(\tau) = \mathbb{E}_{\tau \sim \pi} \big[R(\tau)\big] = \mathbb{E}_{\tau} \Big[\sum_{t=0}^{T} \gamma^{t} r_{t} \Big]
$$</p>
<p align="justify">
For problems with infinite time horizons it is important to set $\gamma < 1$ to prevent the objective from becoming unbounded.
</p>
<h3 id="learnable-functions-in-rl">Learnable Functions in RL</h3>
<p align="justify">
In RL there exist three primary functions which can be learned. One of them is the <b>policy</b> $\pi$ which maps states to actions: $a \sim \pi(s)$. This policy can be either deterministic or stochastic.
</p>
<p align="justify">
The second one is called a <b>value function</b>, $V^{\pi}$ or $Q^{\pi}(s,a)$, which estimates the expected return $\mathbb{E}_{\tau}[R(\tau)]$. Value functions provide information about the objective and thereby help an agent to understand how good the states and available actions are in terms of future rewards. As mentioned before, there exist two different versions of value functions:
</p>
<p>$$
V^{\pi}(s) = \mathbb{E}_{s_{0} = s, \tau \sim \pi} \Big[\sum_{t=0}^{T} \gamma^{t} r_{t}\Big]
$$</p>
<p>$$
Q^{\pi}(s,a) = \mathbb{E}_{s_{0} = s, a_{0} = a, \tau \sim \pi} \Big[\sum_{t=0}^{T} \gamma^{t} r_{t}\Big]
$$</p>
<p align="justify">
$V^{\pi}$ evaluates how good or bad a state is, assuming we continue with the current policy. $Q^{\pi}$ instead evaluates how good an action-state pair is.
</p>
<p align="justify">
The last of the three functions is the <b>environment model</b> or the <b>transition function</b> $P(s' \mid s,a)$ which provides information about the environment itself. If an agent learns this function, it is able to predict the next state $s'$ that the environment will transition into after taking action $a$ in state $s$. This gives the agent some kind of "imagination" about the consequences of its actions without interacting with the environment (planning).
</p>
<p align="justify">
All of the three functions discussed above can be learned so we can use deep neural networks as the function approximation method. Based on this you are also able to categorize deep RL algorithms.
</p>
<h2 id="algorithms-overview">Algorithms Overview</h2>
<p align="justify">
We can group RL algorithms based on the functions they learn into four different categories:
</p>
<ul>
<li><em>Policy-based algorithms</em></li>
<li><em>Value-based algorithms</em></li>
<li><em>Model-based algorithms</em></li>
<li><em>Combined Methods</em></li>
</ul>
<p><strong>Policy-based</strong> or <strong>policy optimization</strong> algorithms are a very general class of optimization methods which can be applied to problems with any type of actions, discrete, continuous or a mixture (multiaction). Moreover they are guaranteed to converge. The main drawbacks of these algorithms are that they have a high variance and are sample inefficient.</p>
<p align="justify">
Most of the <b>value-based</b> algorithms learn $Q^{\pi}$ instead of $V^{\pi}$ because it is easier to convert into a policy. Moreover they are typically more sample efficient than policy-based algorithms because they have lower variance and make better use of data gathered from the environment. But they don't have a convergence guarantee and are only applicable to discrete action spaces (*QT-OPT* can also be applied to continuous action spaces).
</p>
<p align="justify">
As mentioned before <b>model-based</b> algorithms learn a model of the environments transition dynamics or make use of a known dynamics model. The agent can use this model to "imagine" what will happen in the future by predicting the trajectory for a few time steps. Purely model-based approaches are commonly applied to games with a target state or navigation tasks with a goal state. These kinds of algorithms are very appealing because they equip an agent with foresight and need a lot fewer samples of data to learn good policies. But for most problems, models are hard to come by because many environments are stochastic, their transition dynamics are not known and the model therefore must be learned (which is pretty hard in large state spaces).
</p>
<p align="justify">
These days many <b>combined methods</b> try to get the best of each, e.g. <i>Actor-Critic</i> algorithms learn a policy and a value function where the policy acts and the value function critiques those actions. Another popular example would be <i>AlphaZero</i> which combined <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a> with learning $V^{\pi}$ and a policy $\pi$ to master the game of Go.
</p>
<p align="justify">
In Figure 2 you can see a slightly different way of categorizing these algorithms, by first differentiating between model-based and model-free methods.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/rl_algorithms_taxonomy.svg#center"
         alt="Reinforcement Learning Algorithms Taxonomy" width="90%"/> <figcaption>
            <p>Figure 2. Non-exhaustive RL algorithms taxonomy [2]</p>
        </figcaption>
</figure>

<p>Another possibility to distinguish between these algorithms would be:</p>
<ul>
<li><strong>on-policy</strong>: Training can only utilize data generated from the current policy $\pi$ which tends to be sample-inefficient and needs more training data.</li>
<li><strong>off-policy</strong>: Any data collected can be reused in training which is more sample-efficient but requires much more memory.</li>
</ul>
<p align="justify">
In the following we are going to describe three different algorithms. One of them is a policy-based algorithm called <b>REINFORCE</b> (Policy Gradient). The other two are value-based algorithms called <b>SARSA</b> and <b>Deep Q-Networks</b>. In the second part of this post I will also go into some more advanced combined methods.
</p>
<h2 id="policy-gradient---reinforce">Policy Gradient - REINFORCE</h2>
<p align="justify">
The REINFORCE algorithm was invented in 1992 by Ronald J. Williams. It learns a parameterized policy which produces action probabilities from states and an agent can use this policy directly to act in an environment. The action probabilities are changed by following the <i>policy gradient</i>. The algorithm has three core components:
</p>
<ul>
<li>parameterized policy</li>
<li>objective to be maximized</li>
<li>method for updating the policy parameters</li>
</ul>
<p align="justify">
A neural network is used to learn a good policy by function approximation. This is often called a <i>policy network</i> $\pi_{\theta}$ (parameterized by $\theta$). The objective function to maximize is the expected return over all complete trajectories generated by an agent:
</p>
<p>$$
J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}} \big[ R(\tau) \big] = \mathbb{E}_{\tau \sim \pi_{\theta}} \Big[ \sum_{t=0}^{T} \gamma^{t} r_{t} \Big]
$$</p>
<p>To maximize this objective, gradient ascent is performed on the policy parameters $\theta$. The parameters are then updated based on:</p>
<p>$$
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\pi_{\theta})
$$</p>
<p>The term $\nabla_{\theta} J(\pi_{\theta})$ is known as the <strong>policy gradient</strong> and is defined as:</p>
<p>$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}} \Big[ \sum_{t=0}^{T} R_{t}(\tau) \nabla_{\theta} \log \pi_{\theta} (a_{t} | s_{t})\Big]
$$</p>
<p align="justify">
The policy gradient is numerically estimated using <i>Monte Carlo Sampling</i> which refers to any method that uses random sampling to generate data used to approximate a function. Now lets take a look at the pseudocode for the algorithm:
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/reinforce.png#center"
         alt="pseudocode REINFORCE algorithm with baseline" width="80%"/> <figcaption>
            <p>Figure 3. Pseudocode REINFORCE with baseline [1]</p>
        </figcaption>
</figure>

<p align="justify">
One problem of REINFORCE is the policy gradient estimate can have high variance. One way to reduce this is by introducing a baseline (see Figure 3). Next we are going to learn about two popular value-based algorithms, SARSA and Q-Networks. If you are interested in how the actual code would look like, check out my <a href="https://github.com/johanngerberding/reinforcement-learning-pytorch">RL repository</a>.
</p>
<h2 id="sarsa">SARSA</h2>
<p align="justify">
SARSA (State-action-reward-state-action) is a value-based on-policy method which aims to learn the Q-function $Q^{\pi}(s,a)$. It is based on two core ideas:
</p>
<ol>
<li><strong>Temporal Difference Learning</strong> for learning the Q-function</li>
<li>Generate actions using the Q-function</li>
</ol>
<p align="justify">
In Temporal Difference Learning (TD-Learning) a neural network is used to produce Q-value estimates given $(s,a)$ pairs as input. This is called <b>value network</b>. The general learning workflow is pretty similar to a classical supervised learning workflow:
</p>
<ol>
<li>Generate trajectories $\tau$s and predict a $\hat{Q}$-value for each $(s,a)$-pair</li>
<li>Generate target Q-values $Q_{tar}$ based on the trajectories.</li>
<li>Minimize the distance between $\hat{Q}$ and $Q_{tar}$ using a standard regression loss (like MSE)</li>
<li>Repeat 1-3</li>
</ol>
<p align="justify">
If we would want to use Monto Carlo Sampling here, an agent would have to wait for episodes to end before any data from that episode can be used to learn from, which delays training. We can use TD-Learning to circumvent this problem. The key insight here is that we can define Q-values for the current time step in terms of Q-values of the next time step. This recursive definition is known as the <b>Bellman Equation</b>:
</p>
<p>$$
Q^{\pi}(s,a) \approx r + \gamma Q^{\pi}(s&rsquo;,a&rsquo;) = Q^{\pi}_{tar}(s,a)
$$</p>
<p align="justify">
But if we use the same policy to generate $\hat{Q}^{\pi}(s,a)$ and $Q_{tar}^{\pi}(s,a)$ but how does this work or learn at all? This is possible because $Q_{tar}^{\pi}(s,a)$ uses information one time step into the future when compared with $\hat{Q}^{\pi}(s,a)$. Thus it has access to the reward $r$ from the next state $s'$ ($Q_{tar}^{\pi}(s,a)$ is slightly more informative about how the trajectory will turn out). TD Learning gives us a method for learning how to evaluate state action pairs, but what about choosing the actions?
</p>
<p align="justify">
If we already learned the optimal Q-function, the value of each state-action pair will represent the best possible expected value from taking that action, so we can act greedily with respect to those Q-values. The problem is that this optimal Q-function isn't typically known in advance. But we can use an iterative approach to improve the Q-value by improving the Q-function:
</p>
<ol>
<li>Initialize a neural network randomly with the parameters $\theta$ to represent the Q-function $Q^{\pi}(s,a;\theta)$</li>
<li>Repeat the following until the agent stops improving:
<ol>
<li>Use $Q^{\pi}(s,a;\theta)$ to act in the environment, by action greedily (or $\varepsilon$-greedy) with respect to the Q-values and store all of the experiences $(s,a,r,s&rsquo;)$.</li>
<li>Use the stored experiences to update $Q^{\pi}(s,a;\theta)$ using the Bellman equation to improve the Q-function estimate, which, in turn, improves the policy.</li>
</ol>
</li>
</ol>
<p align="justify">
A greedy action selection policy is deterministic and might lead to an agent not exploring the whole state-action space. To mitigate this issue a so called $\varepsilon$-greedy policy is often used in practice, where you act greedy with probability 1-$\varepsilon$ and random with probability $\varepsilon$. A common strategy here is to start training with a high $\varepsilon$ (e.g. 1.0) so that the agent almost acts randomly and rapidly explores the state-action space. Decay $\varepsilon$ gradually over time so that after many steps the policy, hopefully, approaches the optimal policy. The figure down below shows the pseudocode for the whole SARSA algorithm.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/sarsa.png#center"
         alt="pseudocode SARSA algorithm" width="80%"/> <figcaption>
            <p>Figure 4. Pseudocode SARSA from [1]</p>
        </figcaption>
</figure>

<p align="justify">
In the next section we are going to learn more about another popular value-based method called <b>Deep Q-Networks</b>.
</p>
<h2 id="deep-q-networks">Deep Q-Networks</h2>
<h3 id="general-concept">General Concept</h3>
<p align="justify">
Deep Q-Networks (DQN) were proposed by Mnih et al. in 2013 and are like SARSA a value-based temporal difference learning algorithm that approximates the Q-function. It is also only applicable to environments with discrete action spaces. Instead of learning the Q-function for the current policy DQN learns the optimal Q-function which improves its stability and learning speed over SARSA. This makes it an off-policy algorithm because the optimal Q-function does not depend on the data gathering policy. This also makes it more sample efficient than SARSA. The main difference between the two is the $Q^{\pi}_{tar}(s,a)$ construction:
</p>
<p>$$
Q^{\pi}_{tar}(s,a) = r + \gamma \max_{a&rsquo;} Q^{\pi}(s&rsquo;,a&rsquo;)
$$</p>
<p align="justify">
Instead of using the action $a'$ actually taken in the next state $s'$ to estimate $Q^{\pi}_{tar}(s,a)$, DQN uses the maximum Q-value over all of the potential actions available in that state, which makes it independent from the policy. For action selection you can use e.g. $\epsilon$-greedy or <b>Boltzmann policy</b>. The $\epsilon$-greedy exploration strategy is somewhat naive because the exploration is random and do not use any previously learned knowledge about the environment. In contrast the Boltzmann policy tries to improve on this by selecting actions based on their relative Q-values which has the effect of focusing exploration on more promising actions. It is a parameterized softmax function, where a temperature parameter $\tau \in (0, \infty)$ controls how uniform or concentrated the resulting probability distribution is:
</p>
<p>$$
p(a|s) = \frac{e^{Q^{\pi}(s,a)/\tau}}{\sum_{a&rsquo;}e^{Q^{\pi}(s,a&rsquo;)/\tau}}
$$</p>
<p align="justify">
The role of the temperature parameter $\tau$ in the Boltzmann policy is analoguous to that of $\epsilon$ in the $\epsilon$-greedy policy. It encourages exploration of the state-action space, a high value for $\tau$ means more exploration. To balance exploration and exploitation during training, $\tau$ is adjusted properly (decreased over time). The Boltzmann policy is often more stable than the $\epsilon$-greedy policy but it also can cause an agent to get stuck in a local minimum if the Q-function estimate is inaccurate for some parts of the state space. This can be tackled by using a very large value for $\tau$ at the beginning of training. As mentioned at the beginning of this section, DQN is an off-policy algorithm that doesn't have to discard experiences once they have been used, so we need a so called <b>experience-replay memory</b> to store these experiences for the training. It stores the $k$ most recent experiences an agent has gathered and if the memory is full, older experiences will be discarded. The size of the memory should be large enough to contain many episodes of experiences, so that each batch will contain experiences from different episodes and policies. This will decorrelate the experiences used for training and reduce the variance of the parameter updates, helping to stabilize training. Down below you can the full algorithm from the paper.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/deep_q_learning_with_experience.png#center"
         alt="Deep Q-Learning Algorithm with Experience Replay" width="90%"/> <figcaption>
            <p>Figure 5. Deep Q-Learning Algorithm with Experience Replay [3]</p>
        </figcaption>
</figure>

<h3 id="dqn-improvements">DQN Improvements</h3>
<p align="justify">
Over time people have explored multiple ways to improve the DQN algorithm which we will talk about in the last part of this post. The three modifications are the following:
</p>
<ol>
<li><em>Target networks</em></li>
<li><em>Double DQN</em></li>
<li><em>Prioritized Experience Replay</em></li>
</ol>
<h4 id="target-networks">Target Networks</h4>
<p align="justify">
In the original DQN algorithm $Q_{tar}^{\pi}$ is constantly changing because it depends on $\hat{Q}^{\pi}(s,a)$. This makes it kind of a "moving target" which can destabilize training because it makes it unclear what the network should learn. To reduce the changes in $Q_{tar}^{\pi}(s,a)$ between training steps, you can use a target network. Second network with parameters $\varphi$ which is a lagged copy of the Q-network $Q^{\pi_{\theta}}(s,a)$. It gets periodically updated to the current values for $\theta$, which is called a replacement update. The update frequency is problem dependent (1000 - 10000, for complex environments and 100 - 1000 for simpler ones). Down below you can see the modified Bellman equation:
</p>
<p>$$
Q_{tar}^{\pi_{\varphi}}(s,a) = r + \gamma \max_{a&rsquo;}Q^{\pi_{\varphi}}(s&rsquo;,a&rsquo;)
$$</p>
<p align="justify">
Introducing this network stops the target from moving and transforms the problem into a standard supervised regression. An alternative to the periodic replacement is the so called <b>Polyak update</b>. At each time step, set $\varphi$ to be a weighted average of $\varphi$ and $\theta$, which makes $\varphi$ change more slowly than $\theta$. The hyperparameter $\beta$ controls the speed at which $\varphi$ changes:
</p>
<p>$$
\varphi \leftarrow \beta \varphi + (1 - \beta) \theta
$$</p>
<p align="justify">
It is important to note that each approach has its advantages and disadvantages and no one is clearly better than the other.
</p>
<h4 id="double-dqn">Double DQN</h4>
<p align="justify">
The Double DQN addresses the problem of overestimating Q-values. If you want to know in detail about why this actually happens, take a look at the following <a href="https://arxiv.org/pdf/1509.06461.pdf">paper</a>. The Q-value overestimation can hurt exploration and the error it causes will be backpropagated in time to earlier (s,a)-pairs which adds error to those as well. Double DQN reduces this by learning two Q-function estimates using different experiences. The Q-maximizing action $a'$ is selected using the first estimate and the Q-value that is used to calculate $Q_{tar}^{\pi}(s,a)$ is generated by the second estimate using the before selected action. This removes the bias and leads to the following:
</p>
<p>$$
Q_{tar: DDQN}^{\pi}(s,a) = r + \gamma Q^{\pi_{\varphi}} \big(s&rsquo;, \max_{a&rsquo;}Q^{\pi_{\theta}}(s&rsquo;,a&rsquo;) \big)
$$</p>
<p align="justify">
If the number of time steps between the target network and the training network is large enough, we could use this one for the Double DQN.
</p>
<h4 id="prioritized-experience-replay">Prioritized Experience Replay</h4>
<p align="justify">
The main idea behind this is that some experiences in the replay memory are more informative than others. So if we can train an agent by sampling informative experiences more often then the agent may learn faster. To achieve this, we have to answer the following two questions:
</p>
<ol>
<li><em>How can we automatically assign a priority to each experience?</em></li>
<li><em>How to sample efficiently from the replay memory using these priorities?</em></li>
</ol>
<p align="justify">
As the priority we can simply use the TD error without much computational overhead. At the start of training the priorities of all values are set to a large constant value to encourage each experience to be sampled at least once. The sampling could be done rank-based or based on proportional prioritization. For details on the rank based prioritization, take a look at this <a href="https://arxiv.org/pdf/1511.05952.pdf">paper</a>. The priority for the proportional method is calculated as follows:
</p>
<p>$$
P(i) = \frac{(|\omega_{i}| + \epsilon)^{\eta}}{\sum_{j}(|\omega_{i}| + \epsilon)^{\eta}}
$$</p>
<p align="justify">
where $\omega_{i}$ is the TD error of experience $i$, $\epsilon$ is a small positive number and $\eta$. $\eta$ determines how much to prioritize, so that the larger the $\eta$ the greater the prioritization. Prioritizing certain examples changes the expectation of the entire data distribution, which introduces bias into the training process. This can be corrected by multiplying the TD error for each example by a set of weights, which is called <b>importance sampling</b>.
</p>
<h2 id="summary">Summary</h2>
<p align="justify">
In this post I have tried to give a very short intro to the basic terminology of Reinforcement Learning. If you are interested in this field I encourage you to take a look a Barto and Suttons <a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a>) and the great resource <a href="https://spinningup.openai.com/en/latest/index.html">OpenAI Spinning Up</a> created by Josh Achiam.
</p>
<p align="justify">
Moreover we've seen a way to categorize the different algorithm families of RL. I summarized one policy gradient algorithm called REINFORCE (there exist way more out there) for you to give you a better understanding of the concept of policy learning. After that we explored two value-based algorithms in SARSA and DQN, and we looked at a few tricks to further improve the performce of DQN. In the next part of this series of posts, I'm going to dive deeper in a few more modern deep RL algorithms and combined methods we talked about.
</p>
<h2 id="references">References</h2>
<p><a href="http://incompleteideas.net/book/the-book.html">[1]</a> Barto and Sutton, Reinforcement Learning: An Introduction (2018).</p>
<p><a href="https://spinningup.openai.com/en/latest/index.html">[2]</a> Josh Achiam, OpenAI Spinning Up (2018).</p>
<p><a href="https://arxiv.org/pdf/1312.5602.pdf">[3]</a> Mnih et al., Playing Atari with Deep Reinforcement Learning (2013).</p>
<p><a href="https://arxiv.org/pdf/1509.06461.pdf">[4]</a> van Hasselt et al., Deep Reinforcement Learning with Double Q-Learning (2015).</p>
<p><a href="https://www.amazon.de/Deep-Reinforcement-Learning-Python-Hands/dp/0135172381">[5]</a> Graesser and Keng, Foundations of Deep Reinforcement Learning (2019).</p>
<p><a href="https://arxiv.org/pdf/1511.05952.pdf">[6]</a> Schaul et al., Prioritized Experience Replay (2016).</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Attention and the Transformer</title>
      <link>http://localhost:1313/posts/2021-09-11-attention-and-the-transformer/</link>
      <pubDate>Fri, 15 Oct 2021 10:45:16 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2021-09-11-attention-and-the-transformer/</guid>
      <description>Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms.</description>
      <content:encoded><![CDATA[<h2 id="whats-wrong-with-rnns">Whats wrong with RNN&rsquo;s?</h2>
<p align="justify">
Before we dive into the details of the (Vanilla) Transformer model architecture I want to give you a short intro about how the self-attention mechanism, which is one of the key elements of a transformer block, evolved and why it is part of so many state-of-the art approaches, especially in Natural Language Processing (In the meantime, it can be said that they are also gradually taking over the computer vision field; the state of the art in the field of image classification is a combination of a convolutional neural net and a transformer, called CoAtNet[9]). Much of this information comes from the 2019 lecture by [Justin Johnson](https://www.youtube.com/watch?v=YAgjfMR9R_M&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=14&ab_channel=MichiganOnline) (Michigan State University) and the blogpost [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) from Lilian Weng, which I think are two of the best resources for getting started on the topic.
</p>
<p align="justify">
Let's start by looking at pre-Transformer sequence-to-sequence architectures. Classic seq2seq models have an encoder-decoder architecture and aim to transform an input sequence (source, e.g. german sentence) into a different output sequence (target, e.g. english translation). Both sequences can be of arbitrary length and the Encoder as well as the Decoder are different Recurrent Neural Network architectures (e.g. LSTM, GRU).
</p>
<p>![Encoder Decoder Architecture]({{ &lsquo;/assets/imgs/transformer/encoder_decoder.png&rsquo; | relative_url}}){: style=&ldquo;width: 100%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 1.</strong> Encoder Decoder Architecture [5]</p>
<p align="justify">
Typical transformation tasks of those kinds of models are f.e. machine translation, question-answer dialog generation, image/video captioning [7], speech recognition [6] or parsing sentences into grammar trees [8]. The encoder and the decoder network are typically connected with a fixed length context vector which transfers information between the encoded and the decoded sequence but becomes a bottleneck for longer sequences because of its fixed size. Often the model has "forgotten" the first part of a long sequence once it completes processing the whole input.
</p>
<p align="justify">
To solve this problem, the **Attention mechanism** was born. Instead of only relying on the last hidden state, the idea was to create shortcut connections between the context vector and the entire source input which should negate the "forgetting". The alignment between the source and the target sequence is learned and controlled by the context vector. The illustration down below shows this mechanism.
</p>
<p>![Additive Attention Mechanism]({{ &lsquo;/assets/imgs/transformer/additive_attention.png&rsquo; | relative_url}}){: style=&ldquo;width: 100%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 2.</strong> Additive Attention Mechanism used in [5] from [10]</p>
<p align="justify">
To better understand how this works I have re-implemented this [here](). Feel free to clone the repo and train the model yourself. The Attention mechanism used is called **Additive Attention** (there exist different forms of Attention mechanisms). The encoder consists of a bidirectional RNN with a forward and a backward hidden state, $\overrightarrow{\boldsymbol{h_{i}}}$ and $\overleftarrow{\boldsymbol{h_{i}}}$, which are concatenated to form the encoder state $\boldsymbol{h_{i}}$. The context vector $\boldsymbol{c_{t}}$ for the output $y_{t}$ is a sum of hidden states of the input sequence weighted by alignment scores where $n$ is the length of the input sequence:
</p>
<p align="center">
$$
\boldsymbol{c}_t = \sum_{i=1}^{n}\alpha_{t,i} \boldsymbol{h}_{i} \\
\alpha_{t,i} = softmax(score(\boldsymbol{s}_{t}, \boldsymbol{h}_{i})) \\
score(\boldsymbol{s}_{t}, \boldsymbol{h}_{i}) = \boldsymbol{v}_{a}^{T} tanh(\boldsymbol{W}_{a}[\boldsymbol{s}_{t}; \boldsymbol{h}_{i}])
$$
</p>
<p align="justify">
The alignment model in [5] is a feed-forward network with a single hidden layer. It assigns a score to the pair of input at position $i$ and output at position $t$ based on how well the two words match. Both $\boldsymbol{v_{a}}$ and $\boldsymbol{W_{a}}$ are learned by the alignment model. Based on this, the decoder network calculates the hidden state:
</p>
<p align="center">
$$
\boldsymbol{s}_{t} = f(\boldsymbol{s}_{t-1}, y_{t-1}, \boldsymbol{c}_{t}) \\
$$
</p>
<p align="justify">
With the alignment scores you can create pretty cool matrices which show the correlation between the source and the target words. Down below I have created such a plot with a model I trained for a few epochs on the Multi30k torchtext dataset.
</p>
<p>![Matrix of alignment scores]({{ &lsquo;/assets/imgs/transformer/attention_matrix.png&rsquo; | relative_url}}){: style=&ldquo;width: 80%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 3.</strong> Attention score matrix</p>
<p align="justify">
If you want to learn more about different forms of Attention and popular alignment score functions you can take a look at [10] which provides a table summarizing this.
</p>
<h2 id="self-attention">(Self)-Attention</h2>
<ul>
<li>at each timestep of decoder, the context vector &ldquo;looks at&rdquo; different parts of the input sequence</li>
<li>how: calculate attention scalars based on the hidden state and the decoder state for every hidden state, then multiply the hidden states by the attention scalars and sum them up to create a context vector</li>
</ul>
<p>this mechanism doesn&rsquo;t use the fact that hidden state i forms an ordered sequence, it just treats the hidden states as an unordered set ${h_{i}}$</p>
<p>this means that you can use a similar architecture given any set of input hidden vectors (e.g in image captioning)</p>
<p>in CS, if we discover something useful which is generally applicable we try to abstract</p>
<p>![Self-Attention Layer]({{ &lsquo;/assets/imgs/transformer/self_attention_layer.png&rsquo; | relative_url}}){: style=&ldquo;width: 100%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 2.</strong> Self-Attention Layer (<a href="https://www.youtube.com/watch?v=YAgjfMR9R_M&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&amp;index=14&amp;ab_channel=MichiganOnline">Michigan Online - Justin Johnson</a>)</p>
<p>&ldquo;Memory is attention through time&rdquo; - Alex Graves (2020)</p>
<p>Attention is about ignoring things to focus on specific parts of the data.</p>
<h2 id="transformer">Transformer</h2>
<p align="justify">
I think it is fair to say that the **Transformer** is by far the most popular model architecture choice in the research community at the moment. Vaswani et al. presented the architecture in their paper titeled "Attention is All you Need" which already gives an idea of what it is all about. The Transformer is entirely built on the self-attention mechanism presented before without using any sequence aligned recurrent architecture.
</p>
<h3 id="architecture">Architecture</h3>
<p><strong>Encoder</strong></p>
<p><strong>Decoder</strong></p>
<h3 id="positional-encoding">Positional Encoding</h3>
<h3 id="multi-head-attention">Multi-Head Attention</h3>
<p><strong>Scaled Dot-Product Attention</strong></p>
<p>$$
Attention(Q,K,V) = softmax(QK^{T}/)V
$$</p>
<p><strong>Multi-Head Attention</strong></p>
<h2 id="applications">Applications</h2>
<h3 id="nlp">NLP</h3>
<p>a few popular NLP architectures (GPT, Bert and stuff)</p>
<h3 id="computer--vision">Computer  Vision</h3>
<p>Vision Transformer</p>
<h2 id="summary">Summary</h2>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/pdf/1706.03762.pdf">[1]</a> Vaswani et al. &ldquo;Attention is all you need&rdquo;, 2017.</p>
<p><a href="http://peterbloem.nl/blog/transformers">[2]</a> Transformers from Scratch.</p>
<p><a href="https://theaisummer.com/attention/">[3]</a> How Attention works in Deep Learning: Understanding the attention mechanism in sequence models.</p>
<p><a href="https://www.youtube.com/watch?v=YAgjfMR9R_M&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&amp;index=14&amp;ab_channel=MichiganOnline">[4]</a> Lecture 13: Attention, Justin Johnson.</p>
<p><a href="https://arxiv.org/pdf/1409.0473.pdf">[5]</a> Bahdanau et al. &ldquo;Neural Machine Translation by jointly learning to align and translate&rdquo;, 2016.</p>
<p><a href="https://arxiv.org/pdf/1610.03022.pdf">[6]</a> Zhang et al. &ldquo;Very Deep Convolutional Networks for End-to-End Speech Recognition&rdquo;, 2016.</p>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7505636">[7]</a> Vinyals et al. &ldquo;Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge&rdquo;, 2017.</p>
<p><a href="https://proceedings.neurips.cc/paper/2015/file/277281aada22045c03945dcb2ca6f2ec-Paper.pdf">[8]</a> Vinyals et al. &ldquo;Grammar as a Foreign Language&rdquo;, 2015.</p>
<p><a href="https://arxiv.org/pdf/2106.04803v2.pdf">[9]</a> Dai et al. &ldquo;CoAtNet: Marrying Convolution and Attention for All Data Sizes&rdquo;, 2021.</p>
<p><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">[10]</a> Lilian Weng &ldquo;Attention? Attention!&rdquo;, 2018.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Multi-Object Tracking - Overview</title>
      <link>http://localhost:1313/posts/2021-09-11-multi-object-tracking-overview/</link>
      <pubDate>Sat, 11 Sep 2021 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2021-09-11-multi-object-tracking-overview/</guid>
      <description>Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms.</description>
      <content:encoded><![CDATA[<h2 id="what-is-mot">What is MOT?</h2>
<h2 id="approaches">Approaches</h2>
<h2 id="evaluation">Evaluation</h2>
<h2 id="datasets">Datasets</h2>
<h2 id="popular-architectures">Popular Architectures</h2>
<h2 id="summary">Summary</h2>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/pdf/1706.03762.pdf">[1]</a> Vaswani et al. Attention is all you need.</p>
<p><a href="http://peterbloem.nl/blog/transformers">[2]</a> Transformers from Scratch.</p>
<p><a href="https://theaisummer.com/attention/">[3]</a> How Attention works in Deep Learning: Understanding the attention mechanism in sequence models.
<a href="https://www.youtube.com/watch?v=YAgjfMR9R_M&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&amp;index=14&amp;ab_channel=MichiganOnline">[4]</a> Lecture 13: Attention, Justin Johnson.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Object Detection - Faster Models</title>
      <link>http://localhost:1313/posts/2021-05-02-object-detection-faster-models/</link>
      <pubDate>Sun, 02 May 2021 10:45:16 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2021-05-02-object-detection-faster-models/</guid>
      <description>One-Stage Object Detection Models.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
In the previous <a href="https://johanngerberding.github.io/johannsblog/Object-Detection-From-R-CNN-to-Mask-RCNN">post</a> we have reviewed region-based object detection algorithms (R-CNN models). In the following post I will dive a bit deeper into fast one-stage detection models like YOLO and RetinaNet which are more suited for certain applications with real-time requirements. The models I'm going to talk about here are a bit outdated and don't necessarily correspond to the state-of-the-art in this area anymore. Nevertheless, I find the general development in this area very interesting and the algorithms presented here form the basis for the current state-of-the-art. At that time, two-stage detectors were usually ahead of single-stage detectors in terms of accuracy, which is no longer the case today. In my next post I will go into more detail about state-of-the-art models such as <a href="https://arxiv.org/pdf/1911.09070.pdf">EfficientDet</a> and <a href="https://arxiv.org/pdf/2004.10934.pdf">YOLOv4</a>.
</p>
<h2 id="yolo">YOLO</h2>
<p align="justify">
As mentioned before, two stage detection models like Faster R-CNN are region based and considered to slow for certain applications that require real-time capabilities, e.g. in the robotics area or autonomous driving. So let's start with YOLO ("You Only Look Once") which was one of the first approaches to building a fast real-time object detector. Instead of relying on region proposals the authors reframed the object detection as a single regression problem, predicting bounding boxes and class probabilities directly from the images (therefore the name).
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/yolo-network-architecture.png#center"
         alt="YOLO model architecture" width="100%"/> <figcaption>
            <p>Figure 1. YOLO network architecture</p>
        </figcaption>
</figure>

<p align="justify">
This makes the whole system (Figure 1) fairly simple (single ConvNet) and very fast (45 fps). Since the model uses features from the entire image to predict the boxes it reasons globally.
</p>
<h3 id="how-it-works">How it works</h3>
<p align="justify">
The input image gets divided into an $S \times S$ grid, where each grid cell predicts $B$ bounding boxes and confidence scores ($S=7$,  $B=2$). If the center of an object falls into a grid cell than this grid cell is "responsible" for the detection. Each bounding box consists of 5 predictions: <i>x_center</i>, <i>y_center</i>, <i>width</i>, <i>height</i> and <i>confidence</i>. The x and y coordinates are relative to the bounds of a grid cell. The width and height are relative to the image. So all predicted values are between 0 and 1. In addition each grid cell also predicts $C$ class probabilities which are conditional on the grid cell containing an object (for PascalVOC: $C=20$). These values encode the probabilities of that class appearing in the box and how well the predicted box fits the object. One of the main limitations of this approach is the fact that each grid cell can only contain one object (max: 49 objects per image).
</p>
<h3 id="training">Training</h3>
<p align="justify">
Now let's talk about training the YOLO model. First the convolutional layers get pretrained on ImageNet for a week with an image input size of 224x224. Thereafter to finetune the network on the detection task, four convolutional layers and two fully connected layers get added and the image size is increased to 448x448. As activation LeakyReLU is applied. The loss function is Sum-Squared Error (SSE) consisting of two parts: localization and classification loss:
</p>
<p>$$
L_{loc} = \lambda_{coord} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} [(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^{2} + (\sqrt{w_{i}} - \sqrt{\hat{w}_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}]
$$</p>
<p>$$
L_{cls} = \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} (\mathbb{1}_{ij}^{obj} \lambda_{noobj}(1 - \mathbb{1}_{ij}^{obj})) (C_{i} - \hat{C}_{i})^{2} + \sum_{i=0}^{S^{2}} \sum_{C=C} \mathbb{1}_{i}^{obj} (p_{i}(c) - \hat{p}_{i}(c))^{2}
$$</p>
<p>$$
L = L_{cls} + L_{loc}
$$</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathbb{1}_{i}^{obj}$</td>
<td>indicator of whether the cell $i$ contains an object</td>
</tr>
<tr>
<td>$\mathbb{1}_{ij}^{obj}$</td>
<td>ground-truth class label, $u = 1, &hellip;, K$ (background $u = 0$)</td>
</tr>
<tr>
<td>$C_{i}$</td>
<td>confidence score of cell $i$, $Pr(contains object) * IoU (pred, truth)$</td>
</tr>
<tr>
<td>$\hat{C}_{i}$</td>
<td>predicted confidence score (box with higher IoU of the two predicted boxes)</td>
</tr>
<tr>
<td>$\mathcal{C}$</td>
<td>set of all classes (Pascal VOC: 20)</td>
</tr>
<tr>
<td>$p_{i}$</td>
<td>conditional probability of whether cell $i$ contains an object of class $c \in \mathcal{C}$</td>
</tr>
<tr>
<td>$\hat{p}_{i}$</td>
<td>predicted conditional class probabilities</td>
</tr>
<tr>
<td>$S^{2}$</td>
<td>grid size, here $S=7$</td>
</tr>
<tr>
<td>$B$</td>
<td>number of predicted bounding boxes per grid cell</td>
</tr>
</tbody>
</table>
<p align="justify">
Because of model instability due to the inbalance between cells containing or not containing objects the authors use two scale parameters to increase the loss from bounding box predictions ($\lambda_{coord} = 0.5$) and decrease the loss from confidence predictions for boxes that don't contain objects ($\lambda_{noobj} = 0.5$). The loss function only penalizes classification error if an object is present in that grid cell and it only penalizes bbox error if the cell is "responsible" for the ground truth box.
</p>
<p align="justify">
Some more training details: The authors trained YOLO on VOC 2007 and VOC 2012 with a batch size of 64, momentum of 0.9 and a weight decay of 0.0005. For regularization they rely on dropout and data augmentation.
</p>
<h3 id="shortcomings">Shortcomings</h3>
<ul>
<li>strong spatial constraints since we have only one prediction per grid cell (7x7 -&gt; max. 49 object predictions); this is one of the reasons why the model struggles with crowds of small objects</li>
<li>struggles to generalize to objects in new or unusual aspect ratios or configurations (maybe this could be reduced with clever data augmentation or training on different image scales)</li>
<li>many incorrect localizations due to an inappropriate loss function and coarse features for bounding box prediction (multiple downsampling layers)</li>
</ul>
<h2 id="yolov2">YOLOv2</h2>
<p align="justify">
YOLOv2 is basically an improved version of YOLO, adding some tricks to overcome its shortcomings described before. Moreover the paper covers YOLO9000 which is built on top of YOLOv2 and trained with a joint dataset combining COCO and the top 9000 classes of ImageNet (combination of detection and classification). I will only cover YOLOv2 here, for those of you who are interested in YOLO9000 and the joint training procedure, should take a look a the paper.
</p>
<p align="justify">
As mentioned before, the central goal of YOLOv2 was to fix the problems of YOLO, primarily recall and localization shortcomings. The authors did this based on a variety of new ideas in the field (at that time) and they try to avoid increasing the model size at the same time to preserve the high speed:
</p>
<p><strong>Batch Normalization:</strong> This leads to significant improvements in convergence while eliminating the need for other forms of regularization like dropout (+2% mAP).</p>
<p><strong>High Resolution Classifier:</strong> Finetune the classification network at higher resolution (448x448) for 10 epochs on ImageNet before detection finetuning.</p>
<p><strong>Convolutional Anchor Box Detection:</strong> The fully connected layers from YOLO are removed and instead YOLOv2 incorporates anchor boxes (like Faster R-CNN) to predict the bounding boxes; this also decouples the class prediction from the spatial location by predicting class and objectness for every anchor box which leads to a slight decrease in accuracy (-0.3% mAP) but increases recall significantly (+7%) which gives the model more room to improve.</p>
<p><strong>Box Dimension Clustering:</strong> Instead of using hand picked anchor box sizes, YOLOv2 runs k-means clustering on the training data to determine good priors for anchor box dimensions; to maximize IoU scores, it relies on the following distance metric:</p>
<p>$$
d(x, c_{i}) = 1 - IoU(x, c_{i}), \quad i=i,&hellip;k
$$</p>
<p>where $x$ is a ground truth box candidate and $c_{i}$ is one of the centroids / the closest centroid.</p>
<p><strong>Direct Location Prediction:</strong> In Region Proposal Networks the box location prediction is unconstrained which means any anchor box can end up at any point in the image which can lead to an unstable training. YOLOv2 follows the approach of the original YOLO model by predicting location coordinates relative to the location of the grid cell (using a logistic activation). Given the anchor box width $p_{w}$ and height $p_{h}$ in the grid cell with the top left corner ($c_{x}, c_{y}$) the model predicts 5 values ($t_{x}, t_{y}, t_{w}, t_{h}, t_{o}$) which correspond to the following box values:</p>
<p>$$
b_{x} = \sigma (t_{x}) + c_{x} \
b_{y} = \sigma (t_{y}) + c_{y} \
b_{w} = p_{w}e^{t_{w}} \
b_{h} = p_{h}e^{t_{h}} \
Pr(obj) =IoU(b, obj) = \sigma (t_{o})
$$</p>
<p>This in combination with clustering priors improves mAP by up to 5%.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/yolov2-loc.png#center"
         alt="YOLOv2 bounding box prediction format" width="60%"/> <figcaption>
            <p>Figure 2. YOLOv2 bounding box prediction</p>
        </figcaption>
</figure>

<p><strong>Fine-grained Features:</strong> The grid size of the final feature map of YOLOv2 is increased from 7x7 in YOLO to 13x13. Moreover YOLOv2 incorporates a so called passthrough layer that brings features from an earlier layer at 26x26 resolution to the output layer. This process can be compared with identity mappings from ResNets to incorporate higher dimensional features (+1% mAP).</p>
<p><strong>Multi-scale Training:</strong> To increase the robustness of the model the authors trained it on images of different sizes. Every 10 batches the input size gets randomly sampled (between 320x320 and 608x608).</p>
<p align="justify">
To maintain the high inference speed, YOLOv2 is based on the <b>Darknet-19</b> model, consisting of 19 convolutional and 5 max-pooling layers. For detailed information on the architecture check out Table 6 in the <a href="https://arxiv.org/pdf/1612.08242.pdf">paper</a>.
</p>
<h2 id="retinanet">RetinaNet</h2>
<p align="justify">
Next up in our list of fast detection models is RetinaNet. The creators had the goal of closing the accuracy gap between one and two-stage detection approaches. To achieve this, RetinaNet relies on two crucial building blocks, <b>Feature Pyramid Networks</b> (FPN) as a backbone and a new loss function called <b>Focal Loss</b>.
</p>
<h3 id="focal-loss">Focal Loss</h3>
<p align="justify">
The central cause for the accuracy gap between the two approaches lies in the extreme foreground-background class imbalance during training. In two-stage detectors this problem is addressed by narrowing down the number of candidate object locations (filtering out many background samples) and by using sampling heuristics like a fixed foreground-to-background ratio or online hard example mining. The proposed Focal Loss is designed to address this issue for one-stage detectors by focusing on hard negatives and down-weighting the easier predictions (obvious empty background). It is based on the normal cross entropy loss (for simplicity we use binary loss down here)
</p>
<p>$$CE(p,y) = -y \log p - (1-y) \log (1 - p) $$</p>
<p align="justify">
where $y={0,1}$ is a ground truth binary label, indicating whether a bounding box contains an object and $p \in [0,1]$ is the predicted probability that there is an object (also called objectness or confidence score). For notational convenience, let
</p>
<p>$$
p_{t} =
\begin{cases}
p       &amp; \quad \text{if } y=1\
1-p     &amp; \quad \text{otherwise}
\end{cases}
$$</p>
<p>which leads to</p>
<p>$$
CE(p_{t}) = - \log p_{t}
$$</p>
<p align="justify">
Easily classified negatives ($p_{t} \gg 0.5 ,y=0$) comprise the majority of the loss. You can balance the importance of the positive/negative examples by adding a balancing factor $\alpha$
</p>
<p>$$
CE(p_{t}) = - \alpha_{t} \log p_{t}
$$</p>
<p align="justify">
but this does not differentiate between easy or hard examples. To overcome this the Focal Loss adds a modulating factor $(1-p_{t})^{\gamma}$ with a tunable focusing parameter $\gamma \geq 0$:
</p>
<p>$$
FL(p_{t}) = - (1 - p_{t})^{\gamma} \log (p_{t})
$$</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/focal-loss.png#center"
         alt="Focal Loss with different gamma values" width="70%"/> <figcaption>
            <p>Figure 3. Focal Loss with different gamma values</p>
        </figcaption>
</figure>

<p align="justify">
For better control of the shape of the weighting function the authors used an $\alpha$-balanced version in practice, where $\alpha = 0.25$ and $\gamma = 2$ worked best in their experiments:
</p>
<p>$$
FL(p_{t}) = - \alpha (1 - p_{t})^{\gamma} \log (p_{t})
$$</p>
<h3 id="feature-pyramid-network">Feature Pyramid Network</h3>
<p align="justify">
The FPN backbone for RetinaNet was constructed on top of ResNet. To really understand what that means you should take a look at the <a href="https://arxiv.org/pdf/1612.03144.pdf">paper</a>. Figure 4 down below shows the fundamental idea of FPN which is to leverage a ConvNets pyramidal feature hierarchy to build a feature pyramid with high level semantics throughout. It is general purpose and can be applied to many convolutional backbone architectures.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/featurized-image-pyramid.png#center"
         alt="Featurized Pyramid Network architecture" width="100%"/> <figcaption>
            <p>Figure 4. Featurized Pyramid Network architecture</p>
        </figcaption>
</figure>

<p align="justify">
The basic structure contains a sequence of pyramid levels each corresponding to one network stage. Often these stages contain multiple conv layers of the same size and stage sizes are scaled down by a factor of two. $C_{i}$ represents the different layers of those stages (for ResNet e.g. {$C_{2}, C_{3}, C_{4}, C_{5}$}). As you can see there are two different pathways which connect the conv layers:
</p>
<ol>
<li><strong>Bottom-up</strong> pathway: regular feedback path</li>
<li><strong>Top-down</strong> pathway: goes in the opposite direction, adding coarse but semantically stronger feature maps back into the previous levels of layer size by lateral connections (1x1 conv to match dimensions) and nearest neighbor upsampling; the combination of the two maps is done by element-wise addition</li>
</ol>
<p align="justify">
The final predictions ({$P_{i}$} where $i$ indicates the pyramid level and has resolution $2^{i}$ lower than the input) are generated out of every merged map by a 3x3 conv layer. RetinaNet utilizes feature pyramid levels $P_{3}$ to $P_{7}$ computed from the corresponding ResNet residual stage from $C_{3}$ to $C_{5}$. All pyramid levels have 256 channels (most of RetinaNet is similar to FPN with a few minor differences). The authors used translation-invariant anchor boxes as priors, similar to those used in RPN variant of FPN. To improve Average Precision the number of anchors was increased to $A=9$ (three aspect ratios {1:2, 1:1, 2:1} with three different sizes {$2^{0}, 2^{1/3}, 2^{2/3}$}). As seen before, for each anchor box the model predicts a class probability for each of $K$ classes with a classification subnet trained with Focal Loss. A box regression subnet outputs the offsets for the boxes to the nearest ground truth object. Both networks are independent Fully Convolutional Networks that don't share any parameters.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/retina-net.png#center"
         alt="RetinaNet architecture" width="100%"/> <figcaption>
            <p>Figure 5. RetinaNet architecture</p>
        </figcaption>
</figure>

<h2 id="yolov3">YOLOv3</h2>
<p align="justify">
YOLOv3 was created by applying changes to YOLOv2 inspired by, at that time, recent advances in the object detection world. It's a pretty short and rather unscientifically (I like it :D ) written. The following list summarizes the most important improvements:
</p>
<ul>
<li><strong>Logistic Regression for objectness scores</strong> instead of sum of squared errors</li>
<li><strong>Independent Logistic Classifiers</strong> for class prediction instead of softmax which increases the performance on non mutually exclusive multilabel datasets like Open Images</li>
<li><strong>Multi-scale predictions</strong> inspired by FPN (3 scales per stage)</li>
<li><strong>Darknet-53 as Feature Extractor</strong> which performs similar to ResNet-152 but is 2x faster</li>
</ul>
<p align="justify">
Overall YOLOv3 performs better and faster than SSD, worse then RetinaNet but is 3.8x faster and comparable to state-of-the-art methods on the $AP_{50}$ metric at that time. In the appendix Joseph (the author) adds a cool comment on his opinion about the COCO evaluation metrics. It's refreshing to see someone questioning stuff like this.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/yolov3-res.png#center"
         alt="YOLOv3 performance" width="80%"/> <figcaption>
            <p>Figure 6. YOLOv3 performance</p>
        </figcaption>
</figure>

<h2 id="summary">Summary</h2>
<p align="justify">
In this blog post, we went over four popular but now somewhat aging fast object recognition systems and you got a first introduction to the world of real-time object recognition. In the next post I would like to talk about some more recent models like EfficientDet and YOLOv4.
</p>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/pdf/1506.02640.pdf">[1]</a> Joseph Redmon, et al. You only look once: Unified, real-time object detection. CVPR 2016.</p>
<p><a href="https://arxiv.org/pdf/1612.08242.pdf">[2]</a> Joseph Redmon and Ali Farhadi. YOLO9000: Better, Faster, Stronger. CVPR 2017.</p>
<p><a href="https://arxiv.org/pdf/1804.02767.pdf">[3]</a> Joseph Redmon, Ali Farhadi. YOLOv3: An incremental improvement..</p>
<p><a href="https://arxiv.org/pdf/1612.03144.pdf">[4]</a> Tsung-Yi Lin, et al. Feature Pyramid Networks for Object Detection. CVPR 2017.</p>
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">[5]</a> Tsung-Yi Lin, et al. Focal Loss for Dense Object Detection. IEEE transactions on pattern analysis and machine intelligence, 2018.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Object Detection - From R-CNN to Mask R-CNN</title>
      <link>http://localhost:1313/posts/2021-04-03-object-detection-from-r-cnn-to-mask-rcnn/</link>
      <pubDate>Sat, 03 Apr 2021 10:23:16 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2021-04-03-object-detection-from-r-cnn-to-mask-rcnn/</guid>
      <description>Two-Stage Object Detection Models.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
In the following weeks (or months) I am going to take a deep dive into Deep Learning based Object Detection models. My goal is to create a series of posts regarding different approaches and popular architectures for this task. In this post I'll start with the description of the R-CNN ("Region-based Convolutional Neural Networks") model family, their emergence and central ideas. Nowadays there exist much more accurate and efficient architectures but I think it's a good starting point for such a series. Since the R-CNN based models are so called two-stage approaches, I'll go over a few popular one-stage architectures like YOLO in the next post as well. I'll describe in a minute what exactly this means and what the key differences are. Furthermore, following these two posts, I would like to discuss more current architectures that represent the state-of-the-art in the most recognized benchmarks like <i>EfficientDet</i> or new Transformer-based approaches like <i>DETR</i>.
</p>
<p align="justify">
As mentioned before, we can broadly distinguish between one and two-stage detection frameworks. A typical two-stage pipeline consists of an initial category-independent region-proposal stage followed by the feature extraction and classification. This allows for a high localization and recognition accuracy. In contrast single-stage object detectors do not require prior proposals which makes them faster but less accurate. But more on that in the next post. Now let's start with the R-CNN models.
</p>
<h2 id="r-cnn">R-CNN</h2>
<p align="justify">
Ross Girshick et al. presented their approach called "Region-based Convolutional Neural Networks" (R-CNN) in 2014. It was one of the first methods based on deep convolutional networks (CNNs) besides e.g. <i>Overfeat</i>. The main idea is to tackle the problem of Object Detection in several successive steps, as shown in Figure 1.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_1/RCNN.png#center"
         alt="R-CNN model workflow" width="100%"/> <figcaption>
            <p>Figure 1. R-CNN model workflow [1]</p>
        </figcaption>
</figure>

<p align="justify">
It was an important contribution to the Computer Vision community because of the significant improvement of the state-of-the-art at that time (mAP improvement of more than 30% on PASCAL VOC). Moreover the structure as well as the workflow of the framework are pretty straightforward:
</p>
<ol>
<li>Extract around 2000 of category-independent bounding box object region candidates per image using <a href="https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&amp;bib=all.bib">selective search</a></li>
<li>Each region candidate gets warped to have a fixed size of 227x227 pixels which is required by the CNN (you can detailed information on how this is done in the Appendix of the paper)</li>
<li>Extract the features of each candidate with a CNN (AlexNet)</li>
<li>Classification of each region with class-specific Support Vector Machines (SVM)</li>
<li>Bounding box regression based on the predicted class, predict bounding box offsets (boosts up mAP by 3-4 points)</li>
</ol>
<p align="justify">
The overall training procedure is a stepwise process and requires a lot of work. First you have to pretrain your CNN classification on ImageNet. In the next step you have to get rid of the last classification layer and insert a new one with $K+1$ classes ($K$ = number of classes ;+1 for background). Start finetuning this network using warped proposal windows. It is very important for training that you reduce your learning rate when finetuning (0.01 for ImageNet and 0.001 for finetuning). In the training process all proposals with an Intersection over Union (IoU) >= 0.5 are considered positive samples. The mini-batch size here was 128, consisting of 32 positive and 96 negative boxes, so its biased towards the positives (selective search produces much more negatives than positives). After finetuning your CNN we start building the class-specific binary SVMs. Here a the authors used grid search to choose the IoU threshold of 0.3. In addition to speed up the process they used hard negativ mining. If you want the details on why it is done this way you can look it up in the Appendix of the paper. The last step of the training procedure is the creation of class-specific bounding box regressors, which output bbox offsets. To train these only proposals with an IoU >= 0.6 are used.
</p>
<p align="justify">
Now let's dive a bit deeper into <b>Bounding Box Regression</b>. The offsets get calculated based on the features after $pool_{5}$ layer of each proposal $\mathbf{p}$. The regressor is build to learn scale-invariant transformations between the centers and a log-scale transformation between widths and heights. This is illustrated down below.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_1/RCNN-bbox-regression.png#center"
         alt="Bounding Box Regression" width="80%"/> <figcaption>
            <p>Figure 2. R-CNN - Bounding Box Regression</p>
        </figcaption>
</figure>

<p>$$
\hat{g}_{x} = p_{w}d_{x}(\mathbf{p}) + p_{x} \
\hat{g}_{y} = p_{h}d_{y}(\mathbf{p}) + p_{y} \
\hat{g}_{w} = p_{w}e^{d_{w}(\mathbf{p})} \
\hat{g}_{h} = p_{h}e^{d_{h}(\mathbf{p})}
$$</p>
<p>$\mathbf{p}$ represents the predicted bbox information $(p_{x}, p_{y}, p_{w}, p_{h})$ whereas $\mathbf{g}$ contains the ground truth values $(g_{x}, g_{y}, g_{w}, g_{h})$. The targets to learn are the following:</p>
<p>$$
t_{x} = (g_{x} - p_{x}) / p_{w} \
t_{y} = (g_{y} - p_{y}) / p_{h} \
t_{w} = \log (g_{w} / p_{w}) \
t_{h} = \log (g_{h} / p_{h})
$$</p>
<p align="justify">
A regression model can solve the problem by minimizing the Sum of Squared Error Loss with regularization (ridge regression):
</p>
<p>$$
L_{reg} = \sum_{i \in { x, y, w, h }} (t_{i} - d_{i}(\mathbf{p}))^{2} + \lambda | \mathbf{w} |^{2}
$$</p>
<p align="justify">
The regularization term ($\lambda$ = 1000) is critical and the authors picked it by using cross validation. One benefit of the application of these transformations is that alle the box correction functions $d_{i}(p)$ where $i \in \{ x, y, w, h \}$ can take any value $[- \infty, + \infty]$.
</p>
<p><strong>Shortcomings:</strong></p>
<ul>
<li>training is a multi-stage pipeline</li>
<li>training is expensive in space and time</li>
<li>speed bottleneck due to selective search and feature vector generation for every region proposal ($N$ images $*$ 2000)</li>
</ul>
<h2 id="fast-r-cnn">Fast R-CNN</h2>
<p align="justify">
To overcome these shortcomings described above, Girshick improved the R-CNN training procedure by unifying the three models into one jointly trainable framework called <b>Fast R-CNN</b>. Instead of extracting feature vectors for every object proposal separately, here the entire image gets forward passed through a deep CNN (<a href="https://arxiv.org/pdf/1409.1556.pdf">VGG16</a>)) to produce a convolutional feature map. For each object proposal a <b>Region of Interest (RoI) pooling layer</b> extracts a fixed length feature vector from this feature map. Those feature vectors are then fed into multiple fully-connected layers which finally branch into the object classifier and a bounding box regressor. This intergration leads to a lot of shared computation which speeds up the whole prediction process.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_1/fast-RCNN.png#center"
         alt="Fast R-CNN model workflow" width="100%"/> <figcaption>
            <p>Figure 3. Fast R-CNN architecture</p>
        </figcaption>
</figure>

<p align="justify">
One new key component of the proposed framework is the RoI pooling layer which is a type of max pooling that allows us to convert the features inside any valid region proposals into a feature map with a fixed window of size H $\times$ W. This is done by dividing the input region into H $\times$ W small grids where every subwindow size is approximately of size [h/H $\times$ w/W]. In each of those grid cells apply regular max pooling.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_1/roi_pooling.png#center"
         alt="Region of Interest Pooling" width="100%"/> <figcaption>
            <p>Figure 4. Region of Interest Pooling</p>
        </figcaption>
</figure>

<p align="justify">
The training procedure of this framework is somewhat similar to the one of R-CNN. Here they also pre-train a deep CNN on ImageNet and use selective search for proposal generation. After pre-training the last max pooling layer is replaced by a RoI pooling layer which is set to be compatible with the nets first fully connected layer, for VGG16 H = W = 7. The last fully connected layer and softmax are replaced by two sibling layers, one for classification consisting of one fully connected layer followed by a softmax over $K+1$ categories and one for category-specific bounding box regressors. Another major advantage of this approach is the possibility of end-to-end training with a multi-task loss. This loss function sums up the cost of classification and bbox prediction:
</p>
<p>$$
L(p,u,t^{u},v) = L_{cls}(p,u) + \lambda [u \geq 1] L_{loc}(t^{u}, v)
$$</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>$p$</td>
<td>discrete probability distribution (per RoI) over K+1 classes, $p=(p_{0}, &hellip;, p_{K})$ computed by a softmax</td>
</tr>
<tr>
<td>$u$</td>
<td>ground-truth class label, $u = 1, &hellip;, K$ (background $u = 0$)</td>
</tr>
<tr>
<td>$t^{u}$</td>
<td>predicted bounding box offsets, $t^{u} = (t_{x}^{u},t_{y}^{u},t_{w}^{u},t_{h}^{u})$</td>
</tr>
<tr>
<td>$v$</td>
<td>true bouding box regression targets $v = (v_{x},v_{y},v_{w},v_{h})$</td>
</tr>
<tr>
<td>$\lambda$</td>
<td>hyperparameter to control the balance between the two losses</td>
</tr>
</tbody>
</table>
<p>The indicator function $[u \geq 1]$ is defined as</p>
<p>$$
[u \geq 1] =
\begin{cases}
1       &amp; \quad \text{if } u \geq 1\
0       &amp; \quad \text{otherwise}
\end{cases}
$$</p>
<p align="justify">
to ignore background classifications. $L_{cls}(p,u) = - \log p_{u}$ is a log loss for the true class $u$. The bounding box loss is defined as followed:
</p>
<p>$$
L_{box}(t^{u}, v) = \sum_{i \in {x,y,w,h}} smooth_{L_{1}} (t_{i}^{u} - v_{i})
$$</p>
<p align="justify">
It measures the difference between $t_{i}^{u}$ and $v_{i}$ using a robust smooth $L_{1}$ loss function which is claimed to be less sensitive to outliers than the $L_{2}$ loss.
</p>
<p>$$
L_{1}^{smooth}(x) =
\begin{cases}
0.5x^2       &amp; \quad \text{if } |x| &lt; 1\
|x| - 0.5      &amp; \quad \text{otherwise}
\end{cases}
$$</p>
<p align="justify">
As mentioned before Fast R-CNN is much faster in training and testing but one speed bottleneck in form of selective search still remains which leads us to the next evolutionary stage of the architecture.
</p>
<h2 id="faster-r-cnn">Faster R-CNN</h2>
<p align="justify">
Here the region proposal step gets integrated into the CNN in form of a so called <b>Region Proposal Network</b> (RPN) that shares conv features with the detection network ("it tells the classification network where to look").
</p>
<p align="justify">
The RPN takes an image of arbitrary size as input and outputs a set of rectangular object proposals and objectness scores (= object vs. background). It is important to keep in mind that the goal here is to share computation, so a set of conv layers is chosen to be also part of the object detection pipeline to extract features.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_1/faster-RCNN.png#center"
         alt="Faster R-CNN model" width="100%"/> <figcaption>
            <p>Figure 5. Faster R-CNN architecture (left) and RPN workflow (right)</p>
        </figcaption>
</figure>

<p align="justify">
To generate region proposals, a small $n \times n$ spatial window gets slided over the last shareable conv feature map to create lower dimensional feature vectors (256 dims). These reduced feature vectors get fed into two sibling fully connected layers, a bbox regression  layer and a box classification layer. It is implemented using one $n \times n$ convolution and two $1 \times 1$ conv layers. The classification part is implemented as a two class softmax layer. At each sliding window location $k$ regions of various scales and ratios get proposed simultaneously (k = 9). Those proposals are parameterized relative to $k$ reference boxes called <b>anchors</b>. Each anchor is centered at the sliding window position and is associated with a combination of different aspect ratios (1:1, 1:2, 2:1) and scales (128x128, 256x256, 512x512). This multi-scale anchor-based design is a key component for sharing features without extra cost for addressing scales and aspect ratios.
</p>
<p align="justify">
The loss function used to train the RPN is also a multi-task loss which is similar to the one of Fast R-CNN we discussed before:
</p>
<p>$$
L({p_{i}},{t_{i}}) = \frac{1}{N_{cls}} \sum L_{cls} (p_{i}, p_{i}^{\ast}) + \lambda \frac{1}{N_{reg}} \sum p_{i}^{\ast} L_{reg} (t_{i}, t_{i}^{\ast})
$$</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>$p_{i}$</td>
<td>predicted probability that anchor $i$ is an object</td>
</tr>
<tr>
<td>$p_{i}^{\ast}$</td>
<td>ground-truth label (1 or 0)</td>
</tr>
<tr>
<td>$t_{i}$</td>
<td>predicted parameterized coordinates $(x,y,w,h)$</td>
</tr>
<tr>
<td>$t_{i}^{\ast}$</td>
<td>ground-truth parameterized coordinates $(x,y,w,h)$</td>
</tr>
<tr>
<td>$\lambda$</td>
<td>balancing parameter, set to be 10, to balance out $L_{reg}$ and $L_{cls}$</td>
</tr>
<tr>
<td>$N_{cls}$</td>
<td>normalization term, set to be the same as the mini batch size (256)</td>
</tr>
<tr>
<td>$N_{reg}$</td>
<td>normalization term, set to be approx. the number of anchor boxes ($\sim$ 2400)</td>
</tr>
</tbody>
</table>
<p>The regression loss is also smooth $L_{1}$ like in Fast R-CNN. The classification loss can be calculated as followed:</p>
<p>$$
L_{cls} (p_{i}, p_{i}^{\ast}) = -p_{i}^{\ast} \log p_{i} - (1 - p_{i}^{\ast}) \log (1 - p_{i})
$$</p>
<p align="justify">
For the calculations of $t\_{i}$ and $t\_{i}^{\ast}$ take a look at section 3.1.2 of the <a href="https://arxiv.org/abs/1506.01497">paper</a>. To train the network the authors use a mini batch size of 256 randomly sampled anchors consisting of 128 positives and negatives (padded with negative ones if necessary). All anchors which are beyond the image boundaries are ignored in the training process. For the Fast R-CNN framework integration the authors experimented with different training strategies and chose an alternating one. First they train the RPN and then the Fast R-CNN using the RPN proposals and the trainede backbone. Thereafter they initialize the RPN with the tuned Fast R-CNN network and iterate a couple of times. For detailed evaluation and benchmarks take a look at the paper.
</p>
<h2 id="mask-r-cnn">Mask R-CNN</h2>
<p align="justify">
In the last step of this "evolution", Faster R-CNN was further extended to the task of pixel level instance segmentation, called <b>Mask R-CNN</b>. A third branch is added in parallel to the existing classification and localization branches for predicting binary object masks for every proposed RoI of size $m \times m$.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_1/mask-rcnn.png#center"
         alt="Mask R-CNN model" width="100%"/> <figcaption>
            <p>Figure 6. Mask R-CNN architecture</p>
        </figcaption>
</figure>

<p align="justify">
The mask branch consists of a fully convolutional network (FCN) which allows each layer to maintain the explicit $m \times m$ object spatial layout without collapsing it into a vector representation that lacks those spatial dimensions. To ensure high quality and precision mask predictions the RoI features have to be well aligned to preserve the explicit per-pixel spatial correspondence. The RoI pooling layer lacks the required precision because of quantization which motivated the authors to develop the so called <b>RoI Align</b> layer. RoI Align avoids quantization and instead uses <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation">Bilinear Interpolation</a> to compute the exact values of the input features at four regularly sampled locations in each RoI bin and aggregate the result (max or avg pooling). This leads to large improvements in mask prediction quality. To get a good understanding of how RoI Align works, take a look at the following <a href="https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193">blogpost</a>.
</p>
<p align="justify">
The authors also demonstrate the generality of the proposed framework by experimenting with different backbones (ResNet, ResNext, FPN, ...) and heads for prediction of bboxes, classification and mask prediction. Moreover, they show that the framework is able to predict keypoints with minimal adjustments. For details on this, take a look at the <a href="https://arxiv.org/abs/1703.06870">paper</a>. For training the multi-task loss of Faster R-CNN for each RoI gets extended by adding a mask loss:
</p>
<p>$$
L_{mask} = -\frac{1}{m^{2}} \sum_{1 \leq i, j \leq m} [ y_{ij} \log \hat{y}_{ij}^{k} + (1-y_{ij}) \log (1 - \hat{y}_{ij}^{k}) ]
$$</p>
<p align="justify">
$y_{ij}$ describes the label of a cell (i,j) in the true mask whereas $\hat{y}_{ij}^{k}$ represents the predicted value of cell (i,j). The loss is defined as the average binary cross entropy loss, only including the k-th mask if the region is associated with the ground truth class $k$. The mask branch outputs $K * m^{2}$ binary masks but only the k-th mask contributes to the loss (rest gets ignored). This provides a decoupling between class and mask predictions.
</p>
<h2 id="summary">Summary</h2>
<p align="justify">
To sum everything up and give a broad overview of all architectures covered in this post take a look at the following overview created by Lilian Weng.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_1/rcnn-family-summary.png#center"
         alt="R-CNN model family" width="100%"/> <figcaption>
            <p>Figure 7. Overview R-CNN model family</p>
        </figcaption>
</figure>

<p align="justify">
For a general overview of the field of Deep Learning based Object Detection I highly recommend <a href="https://arxiv.org/pdf/1809.02165v1.pdf">this survey</a> ("Deep Learning for Generic Object Detection: A Survey") from 2018 which in my opinion gives a great overview for beginners.
</p>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/pdf/1311.2524.pdf">[1]</a> R. Girshick, J. Donahue, T. Darrell, J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation, 2014.</p>
<p><a href="https://arxiv.org/pdf/1504.08083.pdf">[2]</a> Ross Girshick. Fast R-CNN, 2015.</p>
<p><a href="https://arxiv.org/pdf/1506.01497.pdf">[3]</a> S. Ren, K. He, R. Girshick, J. Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, 2016.</p>
<p><a href="https://arxiv.org/pdf/1703.06870.pdf">[4]</a> k. He, G. Gkioxari, P. Dollr, R. Girshick. Mask R-CNN, 2017.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Overview - Human Pose Estimation</title>
      <link>http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/</link>
      <pubDate>Fri, 18 Dec 2020 10:23:16 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/</guid>
      <description>Overview of Human Pose Estimation Algorithms, Datasets and Benchmarks.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>When people in the Machine Learning community talk about Pose Estimation, one can usually assume that they are talking about Human Pose Estimation (HPE). All of the known benchmarks or datasets in the field of pose estimation are based on images of people. Maybe this is due to the many potential applications of such models, e.g. Action/Activity Recognition, movies and animation, Human Computer Interaction, medical assistance, sports motion, self-driving [5]. HPE is a very difficult and challenging problem due to possible strong articulations, small and barely visible joints or keypoints, occlusions, self-similar parts, and a high variance in clothing and lighting. But first of all, what exactly is HPE?</p>
<p>Basically you can differentiate between 2D and 3D pose estimation. In <strong>2D Pose Estimation</strong> a 2D pose of (x,y) coordinates for each joint from a RGB image are estimated. In <strong>3D Pose Estimation</strong> you also incorporate the prediction of a third coordinate z. In this article I will only talk about Deep Learning-based HPE models because nowadays nearly all of these models consist of a Deep Learning part (in most cases Convolutional Neural Networks (CNN)). The first CNN was applied in 2013 by Jain et al. [15]. Before this the best approaches for this task were based on body part detectors (multiple stages of processing). In this blogpost we are going to look closer at the field of deep learning-based 2D HPE (green area in Figure 1) where the basis are exclusively 2D images or videos.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hpe_cat.png#center"
         alt="Categorization of HPE methods" width="100%"/> <figcaption>
            <p>Figure 1. Categorization of HPE methods</p>
        </figcaption>
</figure>

<p>Until today a lot has been done in the field of HPE. A wide range of approaches have been researched to make progress in this challenging area. The goal of this blogpost is to give you an overview of the field of HPE, different datasets, evaluation metrics and popular architectures. First of all, there are so many papers and models out there that I cover only a few of them but I try to incorporate the most popular ones. For a more scientific view on this I encourage you to check out this recent review here [5]. Since I am a pretty normal human being and far from perfect, there can be mistakes in this blogpost, so if you find one I encourage you to send me an email and then Ill fix it. Nevertheless, I hope that you like the post and that it is useful for you or exactly what you were looking for.</p>
<h2 id="approaches">Approaches</h2>
<p>There are many more possibilities on how to categorize this broad field (e.g. generative vs. discriminative, regression-based vs. detection-based, one-stage vs. multi-stage). In this blogpost I will focus on multi-person HPE because this automatically implies the single-person pose estimation domain (what I mean by this will be clear in a minute). I differentiate between <em>Top-down</em> and <em>Bottom-up</em> approaches.</p>
<p><strong>Top-down:</strong> This is a two stage approach where you combine a human detection model (like Faster-RCNN) with a single-person pose estimation model. The detection model first predicts bounding boxes of the people in an image which you feed the pose estimation model which then predicts the keypoint coordinates for the person in the bounding box. This approach depends on the performance of the upstream detection model and can be computationally expensive.</p>
<p><strong>Bottom-up:</strong> The typical bottom-up HPE framework includes two stages, keypoint detection and grouping. Approaches which work this way and which are part of this blogpost are DeeperCut, OpenPose and Associative Embedding. The challenge with these models lies primarily in the correct grouping or assignment of the keypoints to the corresponding person especially in the case of unnatural poses, distortions or heavy occlusions. Important strengths of these approaches, as opposed to top-down approaches, are scalability and runtime because they dont rely on the person detector.</p>
<h2 id="datasets">Datasets</h2>
<p>Datasets are essential for fair comparison of different algorithms and bring more challenges and complexity through their expansion and improvement in recent years. In this section we present you the most important and popular datasets in the domain of 2D HPE. For a very nice and short overview take a look at Table 7 in [5].</p>
<h3 id="frames-labeled-in-cinema-flic">Frames Labeled in Cinema (FLIC)</h3>
<p>The FLIC dataset [30] contains 5,003 images collected from popular Hollywood movies. For every tenth frame of 30 movies, a person detector was run to obtain about 20,000 person candidates. The groundtruth labels (10 upper body joints) were obtained through the Amazon Mechanical Turk crowdsourcing marketplace (median-of-five labeling). Those labeled images were checked and rejected manually by the authors if the person was occluded or severly non-frontal. The <em>FLIC-full</em> dataset is the full set of frames including occluded, non-frontal and plain mislabeled frames. Because many of the training set images contain samples from the test set (1,016 images) scenes which allows unfair overtraining on the FLIC test set Tompson et al. [34] proposed a new dataset called <em>FLIC-plus</em> which is a 17380 images subset from the <em>FLIC-full</em> dataset without these problems.</p>
<h3 id="leeds-sport-pose">Leeds Sport Pose</h3>
<p>The Leeds Sport Pose dataset (LSP) [17] contains 2,000 images of full-body poses collected from Flickr by downloading with 8 sports tags: athletics, badminton, baseball, gymnastics, parkour, soccer, tennis and volleyball. The annotations include up to 14 different visible joints. Figure 2 shows a few examples from the original dataset.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/lsp_examples.png#center"
         alt="Examples from the Leeds Sport Pose Dataset" width="90%"/> <figcaption>
            <p>Figure 2. Some examples from the original LSP dataset [17]</p>
        </figcaption>
</figure>

<p>A year later Johnson et al. [18] published the extended version of this dataset containing 10,000 images collected from Flickr searches with the 3 most challenging tags: parkour, gymnastics and athletics. As for FLIC the annotations were generated by the Amazon Mechanical Turk service. The problem with this dataset is that the authors cannot guarantee for the quality of the labels.</p>
<h3 id="mpii-human-pose-dataset">MPII Human Pose Dataset</h3>
<p>The Human Pose dataset from the Max Planck Institute for Informatics (MPII) [2] is one of the state-of-the-art benchmarks for HPE with rich annotations. The dataset includes around 25,000 images with more than 40,000 people with 16 annotated joints. Furthermore the dataset covers 410 human activities which is also part of the image annotations. The images were extracted from YoutTube videos. Figure 3 shows some random images from the dataset and the associated activites.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/mpii_examples.png#center"
         alt="Some examples from the original MPII dataset" width="80%"/> <figcaption>
            <p>Figure 3. Some examples from the original MPII dataset [2]</p>
        </figcaption>
</figure>

<p>Images in MPII have various body poses and are suitable for many tasks such as 2D single- and multiple HPE, action recognition, etc.</p>
<h3 id="coco-keypoints">COCO Keypoints</h3>
<p>The Microsoft Common Objects in Context (COCO) dataset [22] is a large-scale dataset that was originally proposed for object detection and segmentation in natural environments. Over time the dataset was improved and extended e.g. with image captions and keypoint labels. The images were collected from Google, Bing and Flickr image search with isolated as well as pairwise object categories. The annotations were also conducted on Amazon Mechanical Turk.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/coco_keypoint_examples_2020.png#center"
         alt="Some examples from the COCO 2020 keypoint detection task" width="90%"/> <figcaption>
            <p>Figure 4. Some examples from the COCO 2020 keypoint detection task [2]</p>
        </figcaption>
</figure>

<p>The whole dataset contains more than 200,000 images and 250,000 labeled person instances. Annotations on train and validation sets with over 150,000 people and 1.7 million labeled keypoints are publicly available. The annotations for each person include 17 body joints with visibility and left/right labels, and instance human body segmentation. Moreover the COCO dataset contains about 120,000 unlabeled images following the same class distribution as the labeled images which can be used for unsupervised or semi-supervised learning.</p>
<h3 id="aic-hkd-dataset">AIC-HKD Dataset</h3>
<p>The AI Challenger Human Keypoint Dataset (AIC-HKD) [37] is a subset of the large-scale dataset called AI Challenger. It is the biggest benchmark dataset out there containing 210,000 images for training, 30,000 for validation and 60,000 for testing. For the 210 000 images in training set, there are 378,374 human figures with almost 5 million keypoints. Among all the human keypoints 78.4% of them are labeled as visible(v = 1) and the rest of them are labeled as not visible(v = 2). All of these images were collected from Internet search engines. Inappropriate images (e.g. politicians, sexual contents) were removed manually. In addition, images with too many human figures (e.g. crowds in a soccer stadium) and those with very small human figures were excluded. The skeleton consists of 14 different human skeletal keypoints with one of three possible visibility flags: labeled and visible, labeled but not visible or not labeled.</p>
<p>In addition to the presented 2D image datasets there exist video-based datasets for the same purpose e.g. the Penn Action Dataset [41], the Joint-annotated Human Motion Database (J-HMDB) [16], PoseTrack [1]. You can also check them out if you are interested in this.</p>
<br>
<h2 id="evaluation">Evaluation</h2>
<p>Because of the fact that different datasets have different features, e.g. various range of human body sizes, full or only upper human body, in combination with different task requirements (single- or multi-person) there exist several evaluation metrics for 2D HPE. In the following I will describe the most common evaluation metrics in HPE which are regularly used to compare different approaches</p>
<h3 id="percentage-of-correct-parts">Percentage of Correct Parts</h3>
<p>Percentage of Correct Parts (PCP) [10] is widely used in early research as evaluation metric. A limb is considered detected if the distance between the two predicted joint locations and the true limb joint locations is less than 50% of the limb length (commonly denoted as <a href="mailto:PCP@0.5">PCP@0.5</a>). The smaller the PCP value, the better the performance of the model. The drawback of this metric is that it penalizes shorter limbs because shorter limbs like lower arms which are usually harder to detect. Moreover there exist a few slightly modified versions of this metric.</p>
<h3 id="percentage-of-correct-keypoints">Percentage of Correct Keypoints</h3>
<p>The Percentage of Correct Keypoints (PCK) [31] measures the accuracy of the localization of body joints where a candidate joint is considered as correct if it is within a certain distance or threshold of the groundtruth joint. The threshold can e.g. be a fraction of the person bounding box [40] or 50% of the head bone link, which is denoted as <a href="mailto:PCKh@0.5">PCKh@0.5</a> [3]. This alleviates the problem with shorter limbs mentioned before and is e.g. the standard evaluation metric for MPII evaluation. Another possibility is to use a pixel radius normalized by the torso height of each test sample as threshold value [30], which is also denoted as Percentage of Detected Joints (PDJ) by [35].</p>
<h3 id="average-precision--recall">Average Precision &amp; Recall</h3>
<p>Imagine you have a dataset with groundtruth labels for different keypoints but without the bounding boxes. How do you evaluate your approach on such a dataset? You can use Average Presicion (AP) similar to object detection. If a predicted joint falls within a threshold of the groundtruth joint location, it is counted as true positive. For MPPE evaluation, all predicted poses are assigned to the groundtruth poses one by one based on the PCKh score order while unassigned predictions are counted as false positives. The mean AP (mAP) is reported from the AP of each body joint. For the COCO dataset AP, AR and their variants are reported based on the object keypoint similarity (OKS, more on this down below) which plays a similar role as the Intersection over Union (IoU).</p>
<h3 id="object-keypoint-similarity">Object Keypoint Similarity</h3>
<p>The Object Keypoint Similarity evaluation metric (OKS) is used for the Keypoint Evaluation in the COCO benchmark. For each object in this dataset the ground truth keypoints have the form $([x_{1}, y_{1}, v_{1}, &hellip;, x_{k}, y_{k}, v_{k}])$ where $(x)$ and $(y)$ are the keypoint locations and $(v)$ is a visibility flag (0 : not labeled; 1 : labeled but not visible; 2 : labeled and visible). On top of that each ground truth label has a scale $(s)$ which is defined as the square root of the object segment area.</p>
<p>The OKS is defined as followed:</p>
<p>$$OKS = \frac{\sum_{i} exp(-d_{i}^{2} / 2s^{2}k_{i}^{2}) \delta (v_{i} &gt; 0)}{\sum_{i} \delta (v_{i} &gt; 0)}$$</p>
<p>Here $d_{i}$ also describes the euclidean distance between the ground truth keypoint and the detection and $k_{i}$ is a per-keypoint constant that controls falloff.</p>
<h3 id="computational-performance">Computational Performance</h3>
<p>The computational performance metrics are also very important in the field of HPE. The <em>Frame Rate</em> indicates the processing speed of input data, generally expressed by Frames per Second (FPS) or seconds per image (s/image). This is an important metrics e.g. for real-world applications which require real-time-estimation. The <em>Number of Weights/Parameters</em> and the <em>GFLOPs</em> (Giga Floating-point Operations per second) are two key performance indicators often mentioned. They show the efficiency of the network and the specific use of GPUs or CPUs.</p>
<h2 id="popular-architectures">Popular architectures</h2>
<p>In the following I will describe a few popular architectures in Single- and Multi-Human Pose Estimation but there are much more out there.</p>
<h3 id="learning-human-pose-estimation-features-with-convolutional-networks-2013">Learning Human Pose Estimation Features with Convolutional Networks (2013)</h3>
<p>This paper [15] describes the first deep learning approach to tackle the problem of single-person full body human pose estimation with convolutional neural networks. In this approach the authors trained multiple independent binary classification networks, one network per keypoint. The model is applied in a sliding window approach and outputs a response map indicating the confidence of the body part at that location. Figure 5 shows the architecture of the classification models.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/model_architecture_jain_et_al_2014.png#center"
         alt="The model architecture of convolutional network" width="70%"/> <figcaption>
            <p>Figure 5. The model architecture of [15]</p>
        </figcaption>
</figure>

<p>The input is of shape 64x64 pixel and locally contrast normalized (LCN). As activation functions ReLU is used. To reduce computational complexity max pooling is applied twice which leeds to some spatial information loss. After the three convoliutional layers follow three fully connected layers. To reduce overfitting, L2 regularization and dropout are applied in the fully connected layers. The output layer is a single logistic unit, representing the probability of a body part being in the patch. Moreover the authors use part priors for the final prediction. For a detailed breakdown of how all this works you should have a look at the paper. The model was evaluated on the FLIC dataset based on the PCK metric described before [31]. Figure 6 shows the performance on the wrist, elbow and shoulder joints of 351 FLIC test set images.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/conv_flic.png#center"
         alt="PCK on wrist, elbow and shoulder joints of FLIC test set" width="80%"/> <figcaption>
            <p>Figure 6. PCK on wrist, elbow and shoulder joints of FLIC test set [15]</p>
        </figcaption>
</figure>

<h3 id="convolutional-pose-machines-2016">Convolutional Pose Machines (2016)</h3>
<p>A Convolutional Pose Machine [36] is a single-person Human Pose Estimation model which incorporates convolutional networks into the pose machine framework and inherits its benefits like the implicit learning of long-range spatial dependencies and a modular sequential design. This results in a differentiable architecture that allows for end-to-end training with backpropagation on large amounts of data. Figure 7 shows the overall architecture of the model.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpm_architecture.png#center"
         alt="The model architecture of Convolutional Pose Machines" width="75%"/> <figcaption>
            <p>Figure 7. The model architecture of [36]</p>
        </figcaption>
</figure>

<p>It consists of a sequence of stages (ConvNets) which produce 2D belief maps (heatmaps) for each part/keypoint. Before the images are fed into the network they are scaled down to a size of 368x368 pixels. The first stage consists of seven convolutional and three pooling layers with different kernel sizes. The second and all following stages are different from the first one. Here you use the first layers (share weights) of stage one to produce a belief map which is then concatenatet to the output map of the previous stage. After that you feed the concatenated maps into five more convolutional layers. Every stage outputs P+1 belief maps of 46x46 pixels where P is the number of parts and the additional belief map is for the background. At every stage of the model a loss (MSE) is computed based on these belief maps and divided by the number of pixel values (46x46x15). This also addresses the problem of vanishing gradients. In the end, these individual losses are added together to form an overall loss. At every stage the prediction quality is refined as you can see in Figure 8.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpm_joint_detections.png#center"
         alt="Convolutional Pose Machines joint detections" width="75%"/> <figcaption>
            <p>Figure 8. CPM joint detections [36]</p>
        </figcaption>
</figure>

<p>In the first and second stage the model isnt sure which of the two wrists is the right one but in the third stage it seems to be certain. The same goes for the elbows. The model is evaluated on the MPII, FLIC and the Leeds Sports Pose datasets (for detailed information on the performance on all of these, take a look at the original paper). Figure 9 shows the results on MPII in comparison to other state-of-the-art models at that time. The evaluation is based on PCKh metric.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpm_mpii.png#center"
         alt="Convolutional Pose Machines MPII results" width="100%"/> <figcaption>
            <p>Figure 9. CPM MPII results [36]</p>
        </figcaption>
</figure>

<h3 id="deepercut-2016">Deep(er)Cut (2016)</h3>
<p>DeeperCut [13] is an improved version of the DeepCut [28] model which is a multi-person pose estimation approach based on integer-linear programming (ILP) that jointly estimates poses of all people present in an image by minimizing a joint objective. One of the differences between Deep and DeeperCut is that they replace the VGG backbone from the part detector with a modified ResNet-152 for computing part probability scoremaps. This also increases the size of the receptive field from 400 to 1000px which allows a better incorporation of context in the predictions. In addition location refinement is performed by predicting offsets from the locations on the scoremap grid to the ground truth joint locations. During training sigmoid activations and cross entropy loss function are applied.</p>
<p>The large receptive field contains enough information to reason about locations of other parts/joints in the vicinity which is why the authors also use deep networks to make pairwise part-to-part predictions. Those are subsequently used to compute pairwise probabilities. For detailed information on the computations and the incorporation of these pairwise probabilities I encourage you to take a look at both original papers. For me personally it was pretty hard to follow their thinking and their calculations. Maybe it helps to inspect their implementation on github to deeply understand this. Moreover I dont know anything about ILP.</p>
<p>The two main problems of the DeepCut ILP approach are long computation times for solving the optimization problem and the fact that no distinction is made between reliable and less-reliable detections. Therefore in DeeperCut not one but several instances of ILPs are solved based on the reliabilities of the part detections. Figure 10 displays the results on the MPII validation set.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/deepercut_results.png#center"
         alt="DeeperCut MPII results" width="80%"/> <figcaption>
            <p>Figure 10. DeeperCut MPII results [13]</p>
        </figcaption>
</figure>

<h3 id="stacked-hourglass-networks-2016">Stacked Hourglass Networks (2016)</h3>
<p>The Stacked Hourglass model [27] is an approach for single-person pose estimation based on repeated bottom-up, top-down processing used in conjunction with intermediate supervision. Its name originates from its modular design and symmetric topology which you can see in Figure 11. One important operational difference between this approach and related work is that they dont use unpooling or deconvolutional layers instead nearest neighbour upsampling and skip connections for top-down processing are incorporated. But why to use this form of network design?</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/stacked_hourglass_architecture.png#center"
         alt="Stacked Hourglass architecture" width="80%"/> <figcaption>
            <p>Figure 11. Stacked Hourglass architecture [27]</p>
        </figcaption>
</figure>

<p>This is because of the need to capture information at every scale. A final pose estimate requires a coherent understanding of the full body. The hourglass is a simple minimal design that has the capacity to capture important features like e.g. the persons orientation, the arrangement of their limbs and the relationship of adjacent joints. Figure 12 shows one hourglass module which consists of multiple convolutional and max pooling layers used to process features down to a very low resolution. At each max pooling step the network branches off and applies more convolutions at the original pre-pooled resolution. The network reaches its lowest resolution at 4x4 pixels and starts the top-down process of nearest neighbor upsampling followed by elementwise addition operations. After the last hourglass module, two consecutive 1x1 convolutional layers are added to produce the final network predictions which is a set of heatmaps containing the probability of a joints presence at each and every pixel.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hourglass_module.png#center"
         alt="Hourglass module" width="80%"/> <figcaption>
            <p>Figure 12. Hourglass module [27]</p>
        </figcaption>
</figure>

<p>The highest hourglass output resolution is 64x64 due to memory limits. As mentioned in the beginning intermediate supervision is essential for the approach which is realized by the prediction of intermediate heatmaps (between HG modules) upon which a loss can be applied. The final design consists of eight consecutive hourglass modules and a MSE loss is applied to the predictions of all modules using the same ground truth heatmap, consisting of a 2D gaussian with standard deviation of 1px centered on the joint location (similar to the Convolutional Pose Machines approach). The evaluation is carried out on the FLIC and MPII datasets using PCK normalized by torso size for FLIC and head size (PCKh) for MPII. Figure 13 shows the results on MPII. The model has mostly problems with images containing multiple people and occluded joints.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/stacked_hourglass_MPII.png#center"
         alt="Results on MPII Human Pose (PCKh@0.5)" width="90%"/> <figcaption>
            <p>Figure 13. Results on MPII Human Pose (<a href="mailto:PCKh@0.5">PCKh@0.5</a>) [27]</p>
        </figcaption>
</figure>

<h3 id="prm-2017">PRM (2017)</h3>
<p>The proposed model is based on the Stacked Hourglass architecture incorporating so called Pyramid Residual Modules (PRM) and a new initialization scheme for multi-branch networks [39]. The goal of the introduction of PRMs is to enhance the robustness of DCNNs against scale variations of visual patterns by learning multi-scale features. Many of the popular CNN architectures are multi-branch networks, e.g. Inception, ResNets, Convolutional Pose Machines, Stacked Hourglass Network for which the existing weight initialization schemes arent proper. Therefore the authors propose a new method to initialize multi-branch layers that takes the number of branches into consideration. In this article we will focus on the PRMs so for detailed information on the initialization problem please take a look at the original paper.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/prm_architecture.png#center"
         alt="PRM based model architecture" width="75%"/> <figcaption>
            <p>Figure 14. PRM based model architecture [39]</p>
        </figcaption>
</figure>

<p>The approach presented in this paper is based on the Stacked Hourglass Network which is adopted and the residual units are replaced by PRMs. Figure 14 gives a nice overview of the proposed new framework. As you can see the input image is 256x256px cropped from a resized image according to the annotated body position and scale. Instead of max or average pooling, fractional max pooling is used to obtain input feature maps of different resolutions.
Now lets take a closer look at the proposed PRM and its inner workings. I am more of a visual guy and the provided overview of the different PRM versions helped me understand them better. Mathematically the PRM can be formulated as</p>
<p>$$\mathbf{x}^{(l+1)} = \mathbf{x}^{(l)} + \mathcal{P} (\mathbf{x}^{(l)}, \mathbf{W}^{(l)} )$$</p>
<p>where $(\mathcal{P}(\mathbf{x}^{(l)}; \mathbf{W}^{(l)}))$ is Feature Pyramids decomposed as</p>
<p>$$\mathcal{P}(\mathbf{x}^{(l)}, \mathbf{W}^{(l)}) = g(\displaystyle\sum_{c=1}^{C} f_{c} (\mathbf{x}^{(l)}; \mathbf{w}^{(l)}_{f_{c}}); \mathbf{w}^{(l)}_{g}) + f_{0}(\mathbf{x}^{(l)}; \mathbf{w}^{(l)}_{f_{0}})$$</p>
<p>Here $\mathbf{x}^{(l)}$ and $\mathbf{W}^{(l)}$ are the input and the filter of the $(l)$-th layer respectively. The $C$ denotes the number of pyramid levels and $\mathbf{W}^{(l)} = {\mathbf{w}_{f_{c}}^{(l)}, \mathbf{w}_{g}^{(l)}}_{c=0}^{C}$ is the set of parameters. To reduce computational and space complexity each $f_{c}(\cdot)$ is designed as a bottleneck structure. As you can see in the Figure 15 the authors experimented with four slightly different PRM architectures which all performed better than the baseline in terms of accuracy, number of parameters and complexity. Between the four new modules PRM-B showed the best balance in terms of accuracy and complexity. The number of pyramid levels $C$ was varied between 3 and 5. The authors observed that increasing $C$ generally improves the performance.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/prm_variants.png#center"
         alt="PRM variants" width="100%"/> <figcaption>
            <p>Figure 15. PRM variants [39]</p>
        </figcaption>
</figure>

<p>Important to mention is that the PRM is a general module (not only suitable for HPE) that can be used as the basic building block for various other CNN architectures e.g. in image classification networks. Regarding the training procedure and inference the proposed approach is pretty similar to common HPE approaches, as score maps (heatmaps) generated from a Gaussian represent the body joint locations. A loss is attached at the end of each stack defined by the squared error. The approach was tested on MPII and LSP. The authors used <a href="mailto:PCK@0.2">PCK@0.2</a> for LSP evaluation and <a href="mailto:PCKh@0.5">PCKh@0.5</a> for MPII. Figure 16 displays the evaluation results.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/prm_results.png#center"
         alt="PRM results on MPII" width="65%"/> <figcaption>
            <p>Figure 16. PRM results on MPII [39]</p>
        </figcaption>
</figure>

<p>The input images are 256x256 cropped from a resized image according to the annotated body position and scale. The training data is augmented by scaling, rotation, flipping and adding color and noise. In comparison to the baseline hourglass model the complexity is increased by 13.5% (from 23.7M to 26.9M parameters) and the number of GFLOPs (45.9) for a 256x256 image is also increased by 11.4%.</p>
<h3 id="associative-embedding-2017">Associative Embedding (2017)</h3>
<p>The problem of bottom-up MPPE can be broken down into detection and grouping: detecting body joints and grouping them into individual people. Associative Embedding [26] is a single-stage method to tackle this task end-to-end by grouping detections through a process called tagging. A tag (real number) associates a detection with other detections of a specific group.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/asso_architecture.png#center"
         alt="Associative Embedding architecture" width="100%"/> <figcaption>
            <p>Figure 17. Associative Embedding architecture [26]</p>
        </figcaption>
</figure>

<p>Figure 17 gives an overview of the approach. Here a stacked hourglass network is used to predict a detection score at each pixel location for each body joint, regardless of person identity. In addition the network automatically groups detections into individual poses. For each joint heatmap a corresponding tag heatmap is produced (2m output channels for $m$ body joints). NMS is used to parse detection into individual people by retrieving their corresponding tags at those pixel locations. Then detections are grouped across body parts by comparing the tag values of detections and matching up those which are close enough. Such a group forms then the human pose estimate. The authors use a combination of a detection loss and a grouping loss on the output heatmaps for training the network. The detection loss computes the MSE between the predicted and the groundtruth heatmaps, as we have seen in approaches before. The grouping loss assess how well the predicted tags agree with the groundtruth grouping. Therefore for each person a reference embedding gets produced instead of enforcing the loss across all possible pairs of keypoints. Within an individual, the squared distance between the reference embedding and the predicted embedding for each joint are calculated. Then between pairs of people, their reference embeddings are compared to each other with a penalty that drops exponentially to zero as the distance between two tags increases. Formally this looks the following way. Let $h_{k} \in \mathbb{R}^{W \times H}$ be the predicted tagging heatmap for the $k$-th body joint, where $h(x)$ is a tag value at pixel location $x$. Given $N$ people, let the groundtruth joint locations be $T={(n_{nk})}$, $n=1,&hellip;,N$, $k=1,&hellip;,K$, where $n_{nk}$ is the groundtruth pixel location of the $k$-th joint of the $n$-th person. The reference embedding for the $k$-th person would be:</p>
<p>$$\bar{a}_{n} = \frac{1}{K} \displaystyle \sum_{k} h_{k} (n_{nk})$$</p>
<p>The grouping loss can then be defined as</p>
<p>$$L_{g}(h, T) = \frac{1}{N} \displaystyle \sum_{n} \displaystyle \sum_{k} (\bar{h}_{n} - h_{k}(x_{nk}))^{2} + \frac{1}{N^{2}} \displaystyle \sum_{n} \displaystyle \sum_{\bar{n}} \exp { - \frac{1}{2 \sigma^{2}} (\bar{h}_{n} - \bar{h}_{\bar{n}})^{2} }$$</p>
<p>Figure 18 shows an example of the tags produced by the network. In this case the tags are well separated (1D embedding) and the decoding process into groups is straightforward. A final set of predictions is produced by iterating over each joint, starting from the head and taking all activations above a certain threshold after NMS, which from the basis of an initial pool of people. Step by step the other detections are matched and assigned to a person. Important to note here is that no steps are taken to ensure anatomical correctness or reasonable spatial relationships between pairs of joints.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/asso_example.png#center"
         alt="Associative Embedding example" width="90%"/> <figcaption>
            <p>Figure 18. Associative Embedding example [26]</p>
        </figcaption>
</figure>

<p>The architecture was evaluated on MPII multi-person and COCO (results in Figure 19). The network used here consists of four stacked hourglass modules with an input size of 512x512 and an output resolution of 128x128. The model outperforms the state-of-the-art models at that time in most of the COCO test-dev metrics.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/asso_coco.png#center"
         alt="COCO test-dev results" width="90%"/> <figcaption>
            <p>Figure 19. COCO test-dev results [26]</p>
        </figcaption>
</figure>

<h3 id="rmpe-2017">RMPE (2017)</h3>
<p>The Regional-Multi-Person Pose Estimation (RMPE) [9] framework imroves the performance of single-person pose estimation (SPPE) based HPE algorithms and therefore addresses two major problems of top-down HPE approaches: the localization error and redundant detections.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/rmpe_architecture.png#center"
         alt="RMPE architecture" width="100%"/> <figcaption>
            <p>Figure 20. RMPE architecture [9]</p>
        </figcaption>
</figure>

<p>It consists of three components which are shown in Figure 20: a Symmetric Spatial Transformer Network (SSTN), parametric Pose NMS and a Pose Guided Proposals Generator (PGPG). The bounding box proposals are obtained by an object detection model and fed into the SSTN+SPPE module which generates pose proposals. The SSTN extracts a high-quality single person region from an inaccurate bounding box. Figure 21 gives a detailed overview of the SSTN+SPPE module itself including the parallel SPPE for training. On the right side of the image you see an example from the original spatial transformer paper which illustrates pretty well what it does (in my opinion) with a few different examples from MNIST. The first of the three images is the input image, the second one illustrates the the result of the localisation network which predicts a transformation to apply to the input image. The third one displays the result.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/rmpe_sstn.png#center"
         alt="SSTN procedure" width="100%"/> <figcaption>
            <p>Figure 21. SSTN procedure (left) [9], Spatial Transformer example from [14]</p>
        </figcaption>
</figure>

<p>The Spatial De-Transformer Network (SDTN) is required for remapping the resulting pose estimation to the original human proposal coordinates (SDTN is an inverse procedure of STN). During training an additional parallel SPPE branch is added to improve the STN extraction quality. This branch omits the SDTN and its output is directly compared to labels of center-located groundtruth poses. The weights of this branch are fixed and is is used to backpropagate pose errors to the STN module. For more detailed information on the STN model I recommend to take a look at the original paper [14].</p>
<p>The generated pose proposals are refined by parametric Pose NMS (eliminate redundancies). This process works in the following way: first the most confident pose is selected as reference and some poses close to it are subject to elimination by applying an elimination criterion (repeat until redundant poses are eliminated) which can be mathematically expressed as follows:</p>
<p>$$f(P_{i}, P_{j}\mid\Lambda,\eta) = 1[d(P_{i},P_{j}\mid\Lambda,\lambda) \leq \eta]$$</p>
<p>$P_{i}$ and $P_{j}$ are poses with $m$ joints, denoted as ${\langle k_{i}^{1}, c_{i}^{1} \rangle, &hellip; ,\langle k_{i}^{m}, c_{i}^{m} \rangle}$ where $k_{i}^{j}$ and $c_{i}^{j}$ are the $j^{th}$ joint location and cofidence score. $d(P_{i}, P_{j}\mid\Lambda)$ is a pose distance metric with a parameter set of $\Lambda$ and a threshold $\eta$ as elimination criterion. The distance function consists of two components and can be written as</p>
<p>$$d(P_{i}, P_{j}\mid\Lambda) = K_{sim}(P_{i}, P_{j}\mid\sigma_{1}) + \lambda H_{sim}(P_{i}, P_{j}\mid\sigma_{2})$$</p>
<p>where $\lambda$ is a weight balancing the two distances and $\Lambda={\sigma_{1},\sigma_{2},\lambda}$. For more details on the two components $K_{sim}$ and $H_{sim}$ you should take a look at the original paper. The parameters described can be determined in a data driven manner.
For better adaptation to imperfect human proposals the authors created a Pose-Guided Proposal Generator which generates a large sample of additional augmented training proposals with the same distribution as the output of the human detector. The distribution of the relative offset between the detected bounding boxes and the groundtruth boxes varies across different poses, so there exists a distribution $P(\delta B\mid atom(P))$ where $\delta B$ is the offsets and $atom(P)$ denotes the atomic pose (determined through k-mean clustering) of $P$. During training for each annotated pose in the training sample, first the atomic pose is looked up and then additional offsets are generated by dense sampling according to $P(\delta B \mid a)$ to produce augmented training proposals.</p>
<p>The new framework was tested on MPII multi-person and MSCOCO keypoints 2016. The authors used the VGG-based [32] SSD-512 [23] model as human detector. In order to guarantee that the entire person region will be extracted, detected human proposals are extended by 30% along both the height and the width. As SPPE the stacked hourglass model, presented before, was incorporated. For the STN network the authors adopted a ResNet-18 as localization network and they used a smaller 4-stack hourglass network as the parallel SPPE.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/rmpe_eval.png#center"
         alt="MPII (left) and COCO (right) results" width="100%"/> <figcaption>
            <p>Figure 22. MPII (left) and COCO (right) results [9]</p>
        </figcaption>
</figure>

<p>In Figure 22 you can see the results on both benchmarks. On MPII it is most notable that the framework increases the mAP on difficult joints like wrists, elbows, ankles and knees significantly. On MSCOCO the framework performs as good as CMU-Pose (bottom-up approach) and much better than other approaches the authors looked at. In their ablation studies they evaluated the effectiveness of all three proposed components.</p>
<h3 id="cascaded-pyramid-network-2018">Cascaded Pyramid Network (2018)</h3>
<p>The Cascaded Pyramid Network (CPN) [6] is a network structure which targets to relieve the problem of pose estimation from especially hard-to-predict keypoints which include occluded and invisible ones as well as complex backgrounds and crowded scenes. The authors name two reasons for the difficulty. First the harder to detect joints cannot be simply recognized based on their appearance features only (e.g. the torso point). Second they are not explicitly addressed during the training process. Therefore this new algorithm includes two stages: GlobalNet and RefineNet (Figure 23).</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpn_architecture.png#center"
         alt="CPN architecture" width="100%"/> <figcaption>
            <p>Figure 23. CPN architecture [6]</p>
        </figcaption>
</figure>

<p>It is a top-down approach incorporating a Feature Pyramid Network (FPN) [21] as human detection model. The ROIPooling in FPN is replaced with ROIAlign from Mask-RCNN [12] and trained on all COCO categories first. The GlobalNet structure is based on the ResNet backbone. 3x3 convolutional filters are applied on the feature maps outputted by the residual blocks 2 to 5 to generate heatmaps for keypoints. The basic idea is the same as in FPN, that the shallower feature maps contain the high spatial resolution for localization but low semantic information for recognition. On the other hand the feature maps produced by deeper layers have more semantic and less spatial resolution. Figure 24 shows the output heatmaps of both the GlobalNet and RefineNet.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpn_output.png#center"
         alt="CPN output heatmaps" width="100%"/> <figcaption>
            <p>Figure 24. CPN output heatmaps [6]</p>
        </figcaption>
</figure>

<p>As you can see GlobalNet is able to effectively locate the easier keypoints like eyes but struggles with the harder ones. To improve the performance on harder keypoints RefineNet is attached to GlobalNet. It transmits the information across different levels and integrates them via upsampling and concatenation. The hard keypoints are explicitly selected based on the training loss (called <em>online hard keypoints mining</em>) and their gradients are backpropagated. Figure 24 shows the results of the proposed approach on the COCO test-dev dataset where the + sign indicates results using ensembled models. Each human detection box is extended to a fixed aspect ratio, e.g. height : width = 256 : 192 and then cropped from the image without distorting the images aspect ratio. Moreover data augmentation is important for the learning of scale- and rotation-invariance. After the cropping, random flipping, random rotation ($-45^\circ \sim +45^\circ$) and random scaling ($0.7 \sim 1.35$) are applied. For more detailed information about the training procedure, ablation studies and the results you should take a look at the original paper.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpn_coco.png#center"
         alt="CPN results on COCO test-dev" width="100%"/> <figcaption>
            <p>Figure 24. CPN results on COCO test-dev [6]</p>
        </figcaption>
</figure>

<h3 id="openpose-2019">OpenPose (2019)</h3>
<p>OpenPose [4] is the first open-source realtime system for bottom-up multi-person 2D pose estimation and its one of the most popular frameworks out there (OpenCV included it). One of the main problems of top-down approaches in HPE is their early commitment, which means that if the person detector fails (this is often the case when people are in close proximity) there is no recourse to recovery. Another problem is the computational complexity and the runtime which is proportional to the number of people in the image. OpenPose works bottom-up, like DeeperCut, but much much faster. Figure 25 illustrates the whole pipeline of this approach.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_architecture.png#center"
         alt="Pipeline of OpenPose" width="100%"/> <figcaption>
            <p>Figure 25. Pipeline of OpenPose [4]</p>
        </figcaption>
</figure>

<p>It takes as input an image of size $w \times h$ and produces the 2D locations of anatomical keypoints for each human in the image. In the first step, a convolutional neural network predicts a set of 2D confidence maps (heatmaps) $\mathbf{S}$ of body part locations (Figure 25 (b)) and a set of 2D vector fields $\mathbf{L}$ called part affinity fields (PAFs). These encode the degree of association between parts (Figure 25 (c)). The set $\mathbf{S} = ( \mathbf{S}_{1}, \mathbf{S}_{2}, \dots , \mathbf{S}_{J} )$ has $J$ confidence maps., one per part, where $\mathbf{S}_{J} \in \mathbb{R}^{w \times h}$ , $j \in {1,\dots,J}$. The set $\mathbf{L} = (\mathbf{L}_{1}, \mathbf{L}_{2}, \dots, \mathbf{L}_{C} )$ has $C$ vector fields, one per limb, where $\mathbf{L}_{c} \in \mathbb{R}^{w \times h \times 2}$ , $c \in {1,\dots,C}$. In this context limbs are part pairs. Then a parsing step creates a set of bipartite matchings to associate the body part candidates. Finally the candidates are assembled into full body poses. Figure 26 shows the architecture of the model, which iteratively predicts affinity fields (blue) and detection confidence maps (beige). The predictions are refined over the successive stages $t \in {1,\dots,T}$ including intermediate supervision at each stage similar to the Convolutional Pose Machines approach.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_model.png#center"
         alt="Architecture of OpenPose" width="70%"/> <figcaption>
            <p>Figure 26. Architecture of OpenPose [4]</p>
        </figcaption>
</figure>

<p>Before the image is fed to the first stage it gets transformed in a set of feature maps $\mathbf{F}$ by the first 10 layers of a pretrained and finetuned VGG19 network. Based on these maps the first stage produces a set of PAFs $\mathbf{L}_{1} = \phi^{1}(\mathbf{F})$ where $\phi^{1}$ refers to the operations in stage 1. In each subsequent stage the predictions from the previous stage and the original image features $\mathbf{F}$ are concatenated and used to produce refined predictions,</p>
<p>$$\mathbf{L}^{t}=\phi^{t}(\mathbf{F}, \mathbf{L}^{t-1}), \forall 2 \leq t \leq T_{p},$$</p>
<p>where $t$ denotes the stage and $T_{p}$ refers to the total number of PAF stages. After $T_{p}$ iterations the process is repeated for the confidence maps detection,</p>
<p>$$\mathbf{S}^{T_{p}}=p^{t}(\mathbf{F},\mathbf{L}^{T_{p}}), \forall t = T_{p}$$</p>
<p>$$\mathbf{S}^{t}=p^{t}(\mathbf{F},\mathbf{L}^{T_{p}}, \mathbf{S}^{t-1}), \forall T_{p} &lt; t \leq T_{p} + T_{c}$$</p>
<p>where $p_t$ refers to the CNNs for inference at stage $t$ and $T_c$ to the number of total confidence map stages. At the end of each stage a $L_{2}$ loss between the estimated predictions and the groundtruth maps and fields is applied. The loss functions are:</p>
<p>$$ f_{\mathbf{L}^t_{i}} = \displaystyle\sum_{c=1}^C \displaystyle\sum_{\mathbf{p}} \mathbf{W} (\mathbf{p}) \cdot \lVert \mathbf{L}^{t_{i}} (\mathbf{p}) - \mathbf{L}^{*}_{c} (\mathbf{p}) \rVert_{2}^{2}$$</p>
<p>$$ f_{\mathbf{S}^{t}_{k}} = \displaystyle\sum_{j=1}^{J} \displaystyle\sum_{\mathbf{p}} \mathbf{W}(\mathbf{p}) \cdot | \mathbf{S}_{j}^{t_{k}}(\mathbf{p}) - \mathbf{S}_{j}^{*}(\mathbf{p}) |_{2}^2 $$</p>
<p>where $\mathbf{L}_{c}^{*}$ is the groundtruth PAF, $\mathbf{S}_{j}^{*}$ is the groundtruth part confidence map and $\mathbf{W}$ is a binary mask with $\mathbf{W}(\mathbf{p})=0$ when the annotation of a person or part is missing at the pixel $\mathbf{p}$ (in many datasets not all humans in an image are labelled). The overall objective is:</p>
<p>$$f=\displaystyle \sum_{t=1}^{T_{p}} f_{\mathbf{L}}^{t} + \displaystyle \sum_{t = T_{p} + 1}^{T_{p} + T_{c}} f_{\mathbf{S}}^{t}$$</p>
<p>Now lets take a closer look on how the confidence maps and the PAFs are generated. The confidence maps $\mathbf{S}_{j,k}^{*}$ for each person $k$ and the joint $j$ are defined as</p>
<p>$$\mathbf{S}_{j,k}^{*}(\mathbf{p})=\exp(- \frac{| \mathbf{p} - \mathbf{x}_{j,k} |_{2}^{2}}{\sigma^{2}}),$$</p>
<p>where $x_{j,k}$ is the location of the body part and $\sigma$ controls the spread of the peak. At test time the predicted body part candidates are obtained by performing NMS.
Now lets get to the PAFs. Each of them is a 2D vector field for each limb. For every pixel in the area belonging to a particular limb, a 2D vector encodes the direction that points from one part of the limb to the other. Let $x_{j_{1},k}$ and $x_{j_{2},k}$ be the groundtruth positions of body parts $j_{1}$ and $j_{2}$ from the limb $c$ for person $k$ in the image. If a point $\mathbf{p}$ lies on the limb, the value at $\mathbf{L}^{*}_{c,k}(\mathbf{p})$ is a unit vector that points from $j_{1}$ to $j_{2}$. For all other points the vector is zero valued. The groundtruth PAF $\mathbf{L}^{*}_{c,k}$ at point $\mathbf{p}$ is defined as</p>
<p>$$\mathbf{L}^{*}_{c,k} (\mathbf{p}) =
\begin{cases}
\mathbf{v}      &amp; \quad \text{if } p \text{ on limb } c, k\
0  &amp; \quad \text{otherwise}
\end{cases}$$</p>
<p>where $\mathbf{v} = (x_{j_{2},k} - x_{j_{1},k}) / | x_{j_{2},k} - x_{j_{1},k} |_{2}$ is the unit vector in the direction of the limb. So now, how do you define the set of points which lie on the limb? This is accomplished by a distance threshold of the line segment, e.g. those points $\mathbf{p}$ for which</p>
<p>$$0 \leq \mathbf{v} \cdot (\mathbf{p} - x_{j_{1},k}) \leq l_{c,k} \text{and} | \mathbf{v}_{\bot} \cdot (\mathbf{p} - x_{j_{1},k}) | \leq \sigma_{l},$$</p>
<p>where the limb width $\sigma_{l}$ is a distance in pixel, $l_{c,k} =  | x_{j_{2},k} - x_{j_{1},k} |_{2}$ is the limb length and $\mathbf{v}_{\bot}$ is a vector perpendicular to $\mathbf{v}$. The groundtruth PAF averages the affinity fields of all people in the image,</p>
<p>$$\mathbf{L}^{*}(\mathbf{p}) = \frac{1}{n_{c}(\mathbf{p})} \displaystyle \sum_{k} \mathbf{L}_{c,k}^{*} (\mathbf{p})$$</p>
<p>where $n_c(\mathbf{p})$ is the number of non-zero vectors at point $\mathbf{p}$ across all people. The association between part candidates is measured by computing the line integral over the corresponding PAF along the line segment connecting the candidate part locations. This measures the aligment of the predicted PAF with the candidate limb that would be formed. For two candidate part locations $\mathbf{d}_{j_{1}}$ and $\mathbf{d}_{j_{2}}$, the predicted PAF $L_{c}$ gets sampled along the line segment to measure the confidence in their association:</p>
<p>$$E= \int_{u=0}^{u=1} \mathbf{L}_{c}(\mathbf{p}(u)) \cdot \frac{\mathbf{d}_{j_{2}} - \mathbf{d}_{j_{1}}}{ | \mathbf{d}_{j_{2}} - \mathbf{d}_{j_{1}} |_{2}} du,$$</p>
<p>where $p(u)$ interpolates the position of the two body parts $\mathbf{d}_{j_{1}}$ and $\mathbf{d}_{j_{2}}$,</p>
<p>$$\mathbf{p}(u) = (1-u) \mathbf{d}_{j_{1}} + u \mathbf{d}_{j_{2}}$$</p>
<p>To obtain a discrete set of part candidate locations NMS on the confidence maps is performed, which form a large set of possible limbs. Each limb candidate is scored by the line integral described before. The resulting $K-dim$ matching problem is known to be NP-hard but in the special case of HPE two relaxations can be added. First, instead of the whole graph a minimal number of edges to obtain a spanning tree skeleton can be chosen. Second, the matching problem can be decomposed into a set of bipartite matching subproblems. The authors demonstrate that a minimal greedy inference well-approximates the global solution at a fraction of the computational cost.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_mpii.png#center"
         alt="MPII results of OpenPose" width="70%"/> <figcaption>
            <p>Figure 27. MPII results of OpenPose [4]</p>
        </figcaption>
</figure>

<p>The method is evaluated on three MPPE datasets: MPII multi-person, COCO and a self created foot dataset. Figure 27 shows the results on the COCO test-dev set based on the OKS evaluation metric. One of the main strengths of this approach, in comparison to top-down approaches, is its inference runtime which is independent from the number of people per image. Figure 28 illustrates its superior performance in this regard. This realtime capability is also very important for many real-world applications and one of the reasons why this approach is very well known.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_runtime.png#center"
         alt="Runtime of OpenPose" width="85%"/> <figcaption>
            <p>Figure 28. Runtime of OpenPose [4]</p>
        </figcaption>
</figure>

<h3 id="hrnet-2019">HRNet (2019)</h3>
<p>In [33] the authors present a novel model architecture for SPPE, called <em>High-Resolution Net</em> (HRNet) which is able to maintain high-resolution representations through the estimation process. Figure 29 illustrates the architecture of this approach. It consists of parallel high-to-low resolution subnetworks starting from a high-resolution subnetwork. Gradually high-to-low resolution subnetworks are added to form more stages. These multi-resolution subnetworks are connected in parallel. Multi-scale fusions processes are carried out to enable the information exchange across these parallel subnetworks.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hrnet_architecture.png#center"
         alt="Architecture of HRNet" width="70%"/> <figcaption>
            <p>Figure 29. Architecture of HRNet [33]</p>
        </figcaption>
</figure>

<p>The two main benefits of this approach in comparison to existing methods are a higher spatial precision due to the maintenance of high resolution (instead of recovering of high from low resolution) and the parallelization instead of serialization. Moreover the network outputs more accurate heatmaps because of the repeated multi-scale fusions. Now lets take a closer look at the details.
The goal is to detect the locations of $K$ keypoints in an image $\mathbf{I}$ of size $W \times H \times 3$. The problem is transformed to an estimation of $K$ heatmaps of size $W^{\prime} \times H^{\prime}, { \mathbf{H}_{1}, \mathbf{H}_{2} ,&hellip; , \mathbf{H}_{K}}$ where each heatmap $\mathbf{H}_{k}$ indicates the location confidence of the $k$th keypoint. The HRNet model is composed of a stem consisting of two strided convolutions decreasing the resolution to $1/4$ of the input size (input: 384x288 to output: 96x72; or input: 256x192 to output: 64x48). The main body which is displayed in Figure 29 outputs the feature maps in the same resolution as the inputs. It consists of parallel multi-resolution subnetworks at different stages. Let $\mathcal{N}_{sr}$ be the subnetwork in the $s$th stage and $r$ be the resolution index, this is an example containing 4 parallel subnetworks</p>
<p>$$\begin{matrix}
\mathcal{N}_{11} &amp; \rightarrow &amp; \mathcal{N}_{21} &amp; \rightarrow &amp; \mathcal{N}_{31} &amp; \rightarrow &amp; \mathcal{N}_{41} \newline
&amp; \searrow &amp; \mathcal{N}_{22} &amp; \rightarrow &amp; \mathcal{N}_{32} &amp; \rightarrow &amp; \mathcal{N}_{42} \newline
&amp; &amp; &amp; \searrow &amp; \mathcal{N}_{33} &amp; \rightarrow &amp; \mathcal{N}_{43} \newline
&amp; &amp; &amp; &amp; &amp; \searrow &amp; \mathcal{N}_{44}
\end{matrix}$$</p>
<p>As you can see in Figure 29, the subnetworks are connected to each other in parallel through so called <em>exchange units</em> which aggregate the information from high, medium and low resolutions (HRNet has 8 of them in total). The authors call this process repeated multi-scale fusion which is illustrated in Figure 30.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hrnet_exchange_units.png#center"
         alt="HRNet exchange units" width="60%"/> <figcaption>
            <p>Figure 30. HRNet exchange units [33]</p>
        </figcaption>
</figure>

<p>Lets look at the formulation of these exchange units a bit closer. The inputs are $s$ response maps: ${\mathbf{X}_{1}, \mathbf{X}_{2}, &hellip; , \mathbf{X}_{s}}$. The outputs are also $s$ response maps: ${\mathbf{Y}_{1}, \mathbf{Y}_{2}, &hellip; , \mathbf{Y}_{s}}$ whose resolutions and widths are the same to the input. Each output is an aggregation of the input maps $\mathbf{Y}_{k} = \sum_{i=1}^{s} a(\mathbf{X}_{i}, k)$. The function $a(\mathbf{X}_{i}, k)$ consists of upsampling (nearest neighbor sampling + 1x1 convolution) or downsampling (strided 3x3 convolution) $\mathbf{X}_{i}$ from resolution $i$ to resolution $k$. The heatmap estimation is done based on the high-resolution output by the last exchange unit. The groundtruth heatmaps are generated through 2D Gaussian with a standard deviation of 1 pixel centered on the location of the keypoint. The loss function applied on the heatmaps is Mean Squared Error. Since HRNet is a top-down model for SPPE, the authors used groundtruth bounding box labels in the training process without a separate detection model. The bounding box is extended in height or width to a fixed aspect ratio of height : width = 4 : 3 and then the box is cropped from the image and resized (256 x 192 or 384 x 288). The data augmentation includes random rotation, random scale and flipping. For more details on the training process take a look at the paper (its very well written in my opinion). Figure 31 shows the results and comparisons with other state-of-the-art models on the CODO test-dev set. In addition the authors tested the model on MPII and PoseTrack.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hrnet_coco.png#center"
         alt="Results on COCO test-dev" width="100%"/> <figcaption>
            <p>Figure 31. Results on COCO test-dev [33]</p>
        </figcaption>
</figure>

<h3 id="efficientpose-2020">EfficientPose (2020)</h3>
<p>One of the main problems of state-of-the-art models is their high degree computational complexity which makes them cumbersome to optimize, hard to replicate and impractical to embed into real-world applications. Because of this a novel approach for single-person pose estimation from 2D images called EfficientPose [11] was introduced. The authors compare this to OpenPose because this model is one of the most applied HPE methods in real-world applications and the first open-source real-time HPE system. The problems with OpenPose are its low resolution output (46x46px) and the computational expense (160 billion floating point operations per inference). To overcome these shortcomings the EfficientPose architecture is based on OpenPose including many important modifications to improve the level of precision, decrease computational cost and model size.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/efficientpose_architecture.png#center"
         alt="EfficientPose architecture" width="100%"/> <figcaption>
            <p>Figure 32. EfficientPose architecture [11]</p>
        </figcaption>
</figure>

<p>The first modification are two inputs, one high- and one low-resolution image (Figure 32 - 1a) and 1b)). The low resolution input is downsampled to half the pixel height and width of the high resolution input by an initial average pooling layer. Both inputs are fed into scalable EfficientNet backbones, pretrained on ImageNet, which are used as feature extractors. High-level semantic information is obtained from the high resolution image using the initial three blocks of a high-scale EfficientNet (B2-B7) which outputs C feature maps. Low-level information is extracted from the low resolution image by the first two blocks of a lower scale EfficientNet (B0-B3). For detailed information on the structure of the before mentioned EfficientNet blocks take a look at the paper. After that the extracted low- and high-level features are concatenated to yield cross-resolution features. The keypoint localization is carried out through an iterative detection process exploiting intermediate supervision. The iterative decision process consists of three passes through detection blocks which all have the same basic structure (Mobile DenseNets). Detailed information on the architecture of these detection blocks can be found in the paper. The detection is carried out in two rounds. First the overall pose is anticipated through a single pass of skeleton estimation (Figure 32 - 5a). The goal of this first pass is to facilitate detection of feasible poses and to avoid confusion if there are multiple people present in an image. Part Affinity Fields are utilized when performing skeleton estimation. Following this step, two detection passes are carried out to estimate heatmaps for keypoints of interest. The former pass acts as a coarse detector (Figure 32 - 5b) while the second one (Figure 32 - 5c) refines localization to improve the accuracy of the outputs. In the sixth and last step the level of detail of the outputs is increased by three transposed convolutions (each of them increases the map size by a factor of two) performing bilinear upsampling.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/efficientpose_variants.png#center"
         alt="EfficientPose variants" width="100%"/> <figcaption>
            <p>Figure 33. EfficientPose variants [11]</p>
        </figcaption>
</figure>

<p>Five different variants of EfficientPose models are presented, details in Figure 33. EfficientPoseRT is a single-resolution model, matching the scale of the smallest EfficientNet model, providing HPE in extremely low latency applications. The evaluation of the models is based on MPII single-person dataset (train-val-split: 26K, 3K). During training the authors use data augmentation (random horizontal flipping, scaling, rotation). For evaluation the PCKh metric is calculated and compared to OpenPose and other models. In Figure 34 the results in comparison to OpenPose are displayed.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/efficientpose_openpose_comparison.png#center"
         alt="Comparison between EfficientPose and OpenPose" width="100%"/> <figcaption>
            <p>Figure 34. Comparison between EfficientPose and OpenPose [11]</p>
        </figcaption>
</figure>

<p>The presented approach is 4-5.6x smaller than OpenPose regarding the model size (number of parameters) and realizes a 2.2 - 184x reduction in FLOPS. In addition to that the new approach converges faster. The comparison here was between EfficientPoseII and OpenPose because of the similar input size. Potentially the model can also be interesting for multi-person-pose-estimation in a bottom-up fashion based on Part Affinity Fields like OpenPose.</p>
<h3 id="evopose2d-2020">EvoPose2D (2020)</h3>
<p>EvoPose2D [24] is the first SPPE network design based on neuroevolution, which is a form of neural architecture search (NAS). The main benefit of networks designed this way is the elimination of human bias leading to more accurate and computationally efficient models. The authors also present a new flexible weight transfer scheme in combination with large-batch (up to 2048 256x192 images) training on Tensor Processing Units (TPUs) that reduces the computational expense of neuroevolution and has no loss in accuracy. For detailed information on neuroevolution take a look at [29]. In this case it is used because of its simplicity compared to alternative approaches based on reinforcement learning, one-shot or gradient-based NAS.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/evopose_weight_transfer.png#center"
         alt="Weight transfer scheme" width="70%"/> <figcaption>
            <p>Figure 35. Weight transfer scheme [24]</p>
        </figcaption>
</figure>

<p>Figure 35 illustrates the weight transfer scheme pretty well. $W^{(l)} \in \mathbb{R}^{k_{p1} \times k_{p2} \times i_{p} \times o_{p}}$ are the weights of layer $l$ used by the parent network and $V^{(l)} \in \mathbb{R}^{k_{c1} \times k_{c2} \times i_{c} \times o_{c}}$ denote the weights of the mutated child network where $k$ is the kernel size, $i$ is the number of input channels and $o$ is the number of output channels. Figure 35 shows two examples ($W \to V_{1}, W \to V_{2}$) of this weight transfer process. The trained weights (shown in blue) in the parent convolutional filter $W$ are transferred, either in part or in full to the corresponding filter $V$ in the mutated child network. The weight transfer extends to all output channels in the same manner as depicted here for the input channels. As a result the mutated network can be trained using fewer iterations which accelerates the neuroevolution.
But keep in mind NAS is far from perfect and you as a human have to design and limit the search space carefully. The starting point in this work was the Simple-Baseline [38] architecture. For each module the authors searched for the optimal kernel size, number of inverted residual blocks, output channels and in the last three modules they additionally searched for the optimal stride (the resulting search space can produce $10^{14}$ unique backbones).
A multi-objective fitness function was applied to create a balance between computational efficiency and accuracy including the validation loss and the number of network parameters. The fitness of the network $\mathcal{N}$ can be defined as:</p>
<p>$$\mathcal{J}(\mathcal{N}) = \left(\frac{T}{n(\theta^{\mathcal{N}})}\right)^{\Gamma} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L} (\mathcal{N}, \mathbf{I}_{i})$$</p>
<p>where $N$ is the number of samples in the validation set, $\mathbf{I}$ is the input image, $n(\theta^{\mathcal{N}})$ is the number of parameters in $\mathcal{N}$, $T$ is the target number of parameters and $\Gamma$ controls the fitness tradeoff between the number of parameters and the validation loss $\mathcal{L} (\mathcal{N}, \mathbf{I}_{i})$. Now lets take a look at the evolutionary strategy. The first generation is a manually defined model and trained for $e_{0}$ epochs. In the next generation $\lambda$ children are generated by mutating the ancestor network (including weight transfer) and then trained for $e$ epochs ($e \ll e_{0}$). After training, the $\mu$ networks with the best fitness become the parents in the next generation and so on. This process continues until manual termination. Motivated by the success of compound scaling (like in EfficientNet) the authors also scaled the base network to different input resolutions using the following depth ($c_{d}$) and width ($c_{w}$) coefficients:</p>
<p>$$c_{d} = \alpha^{\phi} \quad c_{w} = \beta^{\phi} \quad \phi = \frac{\log r - \log r_{s}}{\log \gamma}$$</p>
<p>where $r_{s}$ is the search resolution, $r$ is the desired resolution and $\alpha$, $\beta$, $\gamma$ are scaling parameters.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/evopose_architecture.png#center"
         alt="EvoPose2D-S architecture" width="50%"/> <figcaption>
            <p>Figure 36. EvoPose2D-S architecture [24]</p>
        </figcaption>
</figure>

<p>Figure 36 shows the architecture of the second smallest version of the EvoPose2D models, called EvoPose2D-S, designed via neuroevolution. With an input width and height of $256 \times 192$ and $17$ keypoints to predict, it only contains $2.53M$ parameters and $1.07$ GFLOPs. It has a performance on COCO similar to Simple-Baseline (ResNet50) which has $34.1M$ parameters and $5.21$ GFLOPs. The evaluation of the models was done on the COCO keypoints 2017 dataset. As you can see in Figure 37 the EvoPose2D models have up to 13.5 times less parameters, the performance is similar to state-of-the-art architectures like HRNet-W48 and the input resolution is higher.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/evopose_results.png#center"
         alt="EvoPose2D COCO results" width="100%"/> <figcaption>
            <p>Figure 37. EvoPose2D COCO results [24]</p>
        </figcaption>
</figure>

<h3 id="more-architectures">More architectures</h3>
<p>As mentioned in the beginning of the section there exist so many different approaches to 2D HPE out there and I only covered a few of them here. If you want to learn more you can take a look at the following approaches which may interest you as well:</p>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/1702.07432.pdf">Multi-context attention for human pose estimation</a> (2017) [8]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1804.06208.pdf">SimpleBaselines</a> (2018) [38]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1911.10529.pdf">SimplePose</a> (2019) [19]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1908.10357.pdf">HigherHRNet</a> (2019) [7]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1812.03595.pdf">PoseFix</a> (2019) [25]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1901.00148.pdf">MSPN</a> (2019) [20]</p>
</li>
</ul>
<p>You can find all the papers in the References. Maybe I will update this post in the future including more up to date models as well. Moreover you can find this post as pdf file in the assets folder.</p>
<br>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/pdf/1710.10000.pdf">[1]</a> M. Andriluka, U. Iqbal, E. Ensafutdinov, L. Pishchulin, A. Milan, J. Gall, and S. B. PoseTrack: A benchmark for human pose estimation and tracking. In CVPR, 2018.</p>
<p><a href="https://ieeexplore.ieee.org/document/6909866">[2]</a> M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.</p>
<p><a href="https://ieeexplore.ieee.org/document/6909866">[3]</a> M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pages 36863693, 2014.</p>
<p><a href="https://arxiv.org/pdf/1812.08008.pdf">[4]</a> Z. Cao, G. H. Martinez, T. Simon, S.-E. Wei, and Y. A. Sheikh. Openpose: realtime multi-person 2d pose estimation using part affinity fields. IEEE transactions on pattern analysis and machine intelligence, 2019.</p>
<p><a href="https://arxiv.org/pdf/2006.01423.pdf">[5]</a> Y. Chen, Y. Tian, and M. He. Monocular human pose estimation: A survey of deep learning-based methods. Computer Vision and Image Understanding, 192:102897, 2020.</p>
<p><a href="https://arxiv.org/pdf/1711.07319.pdf">[6]</a> Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun. Cascaded pyramid network for multi-person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 71037112, 2018.</p>
<p><a href="https://arxiv.org/pdf/1908.10357.pdf">[7]</a> B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang. Higherhrnet: Scale- aware representation learning for bottom-up human pose estimation. arXiv preprint arXiv:1908.10357, 2019.</p>
<p><a href="https://arxiv.org/pdf/1702.07432.pdf">[8]</a> X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang. Multi-context attention for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 18311840, 2017.</p>
<p><a href="https://arxiv.org/pdf/1612.00137.pdf">[9]</a> H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu. Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2334 2343, 2017.</p>
<p><a href="https://ieeexplore.ieee.org/document/4587468">[10]</a> V. Ferrari, M. Marin-Jimenez, and A. Zisserman. Progressive search space reduction for human pose estimation. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 18. IEEE, 2008.</p>
<p><a href="https://arxiv.org/pdf/2004.12186.pdf">[11]</a> D. Groos, H. Ramampiaro, and E. Ihlen. Efficientpose: Scalable single-person pose estimation. arXiv preprint arXiv:2004.12186, 2020.</p>
<p><a href="https://arxiv.org/pdf/1703.06870.pdf">[12]</a> K. He, G. Gkioxari, P. Doll ar, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017.</p>
<p><a href="https://arxiv.org/pdf/1605.03170.pdf">[13]</a> E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 3450. Springer, 2016.</p>
<p><a href="https://arxiv.org/pdf/1506.02025.pdf">[14]</a> M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pages 20172025, 2015.</p>
<p><a href="https://arxiv.org/pdf/1312.7302.pdf">[15]</a> A. Jain, J. Tompson, M. Andriluka, G. W. Taylor, and C. Bregler. Learning human pose estimation features with convolutional networks. arXiv, pages arXiv1312, 2013.</p>
<p><a href="https://ieeexplore.ieee.org/document/6751508">[16]</a> H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black. Towards understanding action recognition. In International Conf. on Computer Vision (ICCV), pages 31923199, Dec. 2013.</p>
<p><a href="http://www.bmva.org/bmvc/2010/conference/paper12/paper12.pdf">[17]</a> S. Johnson and M. Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In Proceedings of the British Machine Vision Conference, 2010. doi:10.5244/C.24.12.</p>
<p><a href="https://ieeexplore.ieee.org/document/5995318">[18]</a> S. Johnson and M. Everingham. Learning effective human pose estimation from inaccu- rate annotation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2011.</p>
<p><a href="https://arxiv.org/pdf/1911.10529.pdf">[19]</a> J. Li, W. Su, and Z. Wang. Simple pose: Rethinking and improving a bottom-up approach for multi-person pose estimation. CoRR, abs/1911.10529, 2019.</p>
<p><a href="https://arxiv.org/pdf/1901.00148.pdf">[20]</a> W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei, and J. Sun. Rethinking on multi-stage networks for human pose estimation. arXiv preprint arXiv:1901.00148, 2019.</p>
<p><a href="https://arxiv.org/pdf/1612.03144.pdf">[21]</a> T.-Y. Lin, P. Doll ar, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21172125, 2017.</p>
<p><a href="https://arxiv.org/pdf/1405.0312.pdf">[22]</a> T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll ar, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014.</p>
<p><a href="https://arxiv.org/pdf/1512.02325.pdf">[23]</a> W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 2137. Springer, 2016.</p>
<p><a href="https://arxiv.org/pdf/2011.08446.pdf">[24]</a> W. McNally, K. Vats, A. Wong, and J. McPhee. Evopose2d: Pushing the boundaries of 2d human pose estimation using neuroevolution. arXiv preprint arXiv:2011.08446, 2020.</p>
<p><a href="https://arxiv.org/pdf/1812.03595.pdf">[25]</a> G. Moon, J. Y. Chang, and K. M. Lee. Posefix: Model-agnostic general human pose refinement network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 77737781, 2019.</p>
<p><a href="https://arxiv.org/pdf/1611.05424.pdf">[26]</a> A. Newell, Z. Huang, and J. Deng. Associative embedding: End-to-end learning for joint detection and grouping. In Advances in neural information processing systems, pages 22772287, 2017.</p>
<p><a href="https://arxiv.org/pdf/1603.06937.pdf">[27]</a> A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In European conference on computer vision, pages 483499. Springer, 2016.</p>
<p><a href="https://arxiv.org/pdf/1511.06645.pdf">[28]</a> L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49294937, 2016.</p>
<p><a href="https://arxiv.org/pdf/1703.01041.pdf">[29]</a> E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. Le, and A. Kurakin. Large-scale evolution of image classifiers. arXiv preprint arXiv:1703.01041, 2017.</p>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sapp_MODEC_Multimodal_Decomposable_2013_CVPR_paper.pdf">[30]</a> B. Sapp and B. Taskar. Modec: Multimodal decomposable models for human pose estimation. In In Proc. CVPR, 2013.</p>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sapp_MODEC_Multimodal_Decomposable_2013_CVPR_paper.pdf">[31]</a> B. Sapp and B. Taskar. Modec: Multimodal decomposable models for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 36743681, 2013.</p>
<p><a href="https://arxiv.org/pdf/1409.1556.pdf">[32]</a> K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.</p>
<p><a href="https://arxiv.org/pdf/1902.09212.pdf">[33]</a> K. Sun, B. Xiao, D. Liu, and J. Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56935703, 2019.</p>
<p><a href="https://arxiv.org/pdf/1406.2984.pdf">[34]</a> J. Tompson, A. Jain, Y. Lecun, and C. Bregler. Joint training of a convolutional network and a graphical model for human pose estimation. NIPS, 2014.</p>
<p><a href="https://arxiv.org/pdf/1312.4659.pdf">[35]</a> A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16531660, 2014.</p>
<p><a href="https://arxiv.org/pdf/1602.00134.pdf">[36]</a> S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Convolutional pose machines. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 47244732, 2016.</p>
<p><a href="https://arxiv.org/pdf/1711.06475.pdf">[37]</a> J. Wu, H. Zheng, B. Zhao, Y. Li, B. Yan, R. Liang, W. Wang, S. Zhou, G. Lin, Y. Fu, et al. Ai challenger: A large-scale dataset for going deeper in image understanding. arXiv preprint arXiv:1711.06475, 2017.</p>
<p><a href="https://arxiv.org/pdf/1804.06208.pdf">[38]</a> B. Xiao, H. Wu, and Y. Wei. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), pages 466481, 2018.</p>
<p><a href="https://arxiv.org/pdf/1708.01101.pdf">[39]</a> W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang. Learning feature pyramids for human pose estimation. In proceedings of the IEEE international conference on computer vision, pages 12811290, 2017.</p>
<p><a href="http://www.cs.cmu.edu/~deva/papers/pose_pami.pdf">[40]</a> Y. Yang and D. Ramanan. Articulated human detection with flexible mixtures of parts. IEEE transactions on pattern analysis and machine intelligence, 35(12):28782890, 2012.</p>
<p><a href="https://ieeexplore.ieee.org/document/6751390">[41]</a> W. Zhang, M. Zhu, and K. G. Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In Proceedings of the IEEE Interna- tional Conference on Computer Vision, pages 22482255, 2013.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Imitation Learning</title>
      <link>http://localhost:1313/posts/2023-10-01-imitation-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/2023-10-01-imitation-learning/</guid>
      <description>Introduction to Imitation Learning.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
As we've learned in a previous post, the goal of Reinforcement Learning is to learn an optimal policy which maximizes the long-term cumulative rewards. Generally many of these methods perform pretty well but in some cases it can be very challenging to learn a even a good policy. This is especially true for environments where the rewards are sparse, e.g. a game where the reward is only received at the end. In such cases it can be very helpful to design a reward function which provide the agent with more frequent rewards. Moreover there are a lot of use cases especially in real world scenarios where it is extremly complicated to design a reward function, e.g. in autonomuous driving.  
</p>
<p align="justify">
Imitation Learning (IL) can be a straightforward and feasible solution for these problems. In IL instead of trying to learn from sparse rewards or complicated and imperfect reward functions, we utilize expert demonstrations which we try to mimic.  
</p>
<h2 id="imitation-learning-in-a-nutshell">Imitation Learning in a Nutshell</h2>
<p align="justify">
</p>
<h2 id="types-of-imitation-learning">Types of Imitation Learning</h2>
<ul>
<li>Behavioral Cloning</li>
<li>Inverse Reinforcement Learning</li>
<li>Direct Policy Learning</li>
</ul>
<h2 id="formal-definition">Formal Definition</h2>
<p align="justify">
</p>
<h2 id="difference-between-il-and-offline-rl">Difference between IL and Offline RL</h2>
<p align="justify">
- very similar, in IL you assume the expert policy is optimal and you try to recover it
- in offline RL the goal is "order out the chaos", find a better policy than you have seen in the data
- offline RL methods must do two things:
	1. stay close to the provided data
	2. maximize reward
- the reward maximization is what is missing in IL    
</p>
<h2 id="inverse-reinforcement-learning">Inverse Reinforcement Learning</h2>
<p align="justify">
- learn the reward function 
- with and without model 
</p>
<h2 id="direct-policy-learning">Direct Policy Learning</h2>
<p align="justify">
</p>
<h2 id="behavior-cloning">Behavior Cloning</h2>
<p align="justify">
<ul>
<li>it is really simple and easy to use
<ul>
<li>very stable (supervised learning)</li>
<li>easy to debug and validate</li>
<li>scales well to large datasets</li>
</ul>
</li>
</ul>
</p>
<h2 id="conditional-imitation-learning">Conditional Imitation Learning</h2>
<p align="justify">
</p>
<h2 id="references">References</h2>
<p>Zoltan Lorincz - A brief overview of Imitation Learning (2019)
<a href="https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c">https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c</a></p>
<p>Yue, Le - Imitation Learning Tutorial (ICML 2018)
<a href="https://sites.google.com/view/icml2018-imitation-learning/">https://sites.google.com/view/icml2018-imitation-learning/</a></p>
<p>Sergey Levine - Imitation Learning vs. Offline Reinforcement Learning
<a href="https://www.youtube.com/watch?v=sVPm7zOrBxM&amp;ab_channel=RAIL">https://www.youtube.com/watch?v=sVPm7zOrBxM&amp;ab_channel=RAIL</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
