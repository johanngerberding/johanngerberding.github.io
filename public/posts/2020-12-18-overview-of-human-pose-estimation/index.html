<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Overview - Human Pose Estimation | Johanns Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Overview of Human Pose Estimation Algorithms, Datasets and Benchmarks.">
<meta name="author" content="Johann Gerberding">
<link rel="canonical" href="http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.2bc90b9ba234f5c20dbe846ca9c0f1952ff8ab3a239187a5083779f65790577f.css" integrity="sha256-K8kLm6I09cINvoRsqcDxlS/4qzojkYelCDd59leQV38=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/">

<meta name="twitter:title" content="Overview - Human Pose Estimation | Johanns Blog" />
<meta name="twitter:description" content="Overview of Human Pose Estimation Algorithms, Datasets and Benchmarks." />
<meta property="og:title" content="Overview - Human Pose Estimation | Johanns Blog" />
<meta property="og:description" content="Overview of Human Pose Estimation Algorithms, Datasets and Benchmarks." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2020-12-18T10:23:16&#43;02:00" />
  <meta property="article:modified_time" content="2020-12-18T10:23:16&#43;02:00" /><meta property="og:site_name" content="Johann&#39;s Blog" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Overview - Human Pose Estimation",
      "item": "http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Overview - Human Pose Estimation | Johanns Blog",
  "name": "Overview - Human Pose Estimation",
  "description": "Overview of Human Pose Estimation Algorithms, Datasets and Benchmarks.",
  "keywords": [
    
  ],
  "wordCount" : "10156",
  "inLanguage": "en",
  "datePublished": "2020-12-18T10:23:16+02:00",
  "dateModified": "2020-12-18T10:23:16+02:00",
  "author":{
    "@type": "Person",
    "name": "Johann Gerberding"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Johanns Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" dark type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'dark';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Johanns Blog (Alt + H)">Johanns Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="posts" class="active"
                ><i class='fa fa-heart'></i>posts
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/reading/" title="reading"
                ><i class='fa fa-heart'></i>reading
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about-me/" title="about"
                ><i class='fa fa-heart'></i>about
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Overview - Human Pose Estimation</h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>December 18, 2020</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select: text;"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" style="user-select: text;"></path><polyline points="14 2 14 8 20 8" style="user-select: text;"></polyline><line x1="16" y1="13" x2="8" y2="13" style="user-select: text;"></line><line x1="16" y1="17" x2="8" y2="17" style="user-select: text;"></line><polyline points="10 9 9 9 8 9" style="user-select: text;"></polyline></svg>
  <span>10156 words</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>48 min</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#approaches" aria-label="Approaches">Approaches</a></li>
                <li>
                    <a href="#datasets" aria-label="Datasets">Datasets</a><ul>
                        
                <li>
                    <a href="#frames-labeled-in-cinema-flic" aria-label="Frames Labeled in Cinema (FLIC)">Frames Labeled in Cinema (FLIC)</a></li>
                <li>
                    <a href="#leeds-sport-pose" aria-label="Leeds Sport Pose">Leeds Sport Pose</a></li>
                <li>
                    <a href="#mpii-human-pose-dataset" aria-label="MPII Human Pose Dataset">MPII Human Pose Dataset</a></li>
                <li>
                    <a href="#coco-keypoints" aria-label="COCO Keypoints">COCO Keypoints</a></li>
                <li>
                    <a href="#aic-hkd-dataset" aria-label="AIC-HKD Dataset">AIC-HKD Dataset</a></li></ul>
                </li>
                <li>
                    <a href="#evaluation" aria-label="Evaluation">Evaluation</a><ul>
                        
                <li>
                    <a href="#percentage-of-correct-parts" aria-label="Percentage of Correct Parts">Percentage of Correct Parts</a></li>
                <li>
                    <a href="#percentage-of-correct-keypoints" aria-label="Percentage of Correct Keypoints">Percentage of Correct Keypoints</a></li>
                <li>
                    <a href="#average-precision--recall" aria-label="Average Precision &amp;amp; Recall">Average Precision &amp; Recall</a></li>
                <li>
                    <a href="#object-keypoint-similarity" aria-label="Object Keypoint Similarity">Object Keypoint Similarity</a></li>
                <li>
                    <a href="#computational-performance" aria-label="Computational Performance">Computational Performance</a></li></ul>
                </li>
                <li>
                    <a href="#popular-architectures" aria-label="Popular architectures">Popular architectures</a><ul>
                        
                <li>
                    <a href="#learning-human-pose-estimation-features-with-convolutional-networks-2013" aria-label="Learning Human Pose Estimation Features with Convolutional Networks (2013)">Learning Human Pose Estimation Features with Convolutional Networks (2013)</a></li>
                <li>
                    <a href="#convolutional-pose-machines-2016" aria-label="Convolutional Pose Machines (2016)">Convolutional Pose Machines (2016)</a></li>
                <li>
                    <a href="#deepercut-2016" aria-label="Deep(er)Cut (2016)">Deep(er)Cut (2016)</a></li>
                <li>
                    <a href="#stacked-hourglass-networks-2016" aria-label="Stacked Hourglass Networks (2016)">Stacked Hourglass Networks (2016)</a></li>
                <li>
                    <a href="#prm-2017" aria-label="PRM (2017)">PRM (2017)</a></li>
                <li>
                    <a href="#associative-embedding-2017" aria-label="Associative Embedding (2017)">Associative Embedding (2017)</a></li>
                <li>
                    <a href="#rmpe-2017" aria-label="RMPE (2017)">RMPE (2017)</a></li>
                <li>
                    <a href="#cascaded-pyramid-network-2018" aria-label="Cascaded Pyramid Network (2018)">Cascaded Pyramid Network (2018)</a></li>
                <li>
                    <a href="#openpose-2019" aria-label="OpenPose (2019)">OpenPose (2019)</a></li>
                <li>
                    <a href="#hrnet-2019" aria-label="HRNet (2019)">HRNet (2019)</a></li>
                <li>
                    <a href="#efficientpose-2020" aria-label="EfficientPose (2020)">EfficientPose (2020)</a></li>
                <li>
                    <a href="#evopose2d-2020" aria-label="EvoPose2D (2020)">EvoPose2D (2020)</a></li>
                <li>
                    <a href="#more-architectures" aria-label="More architectures">More architectures</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">¶</a></h2>
<p>When people in the Machine Learning community talk about Pose Estimation, one can usually assume that they are talking about Human Pose Estimation (HPE). All of the known benchmarks or datasets in the field of pose estimation are based on images of people. Maybe this is due to the many potential applications of such models, e.g. Action/Activity Recognition, movies and animation, Human Computer Interaction, medical assistance, sports motion, self-driving [5]. HPE is a very difficult and challenging problem due to possible strong articulations, small and barely visible joints or keypoints, occlusions, self-similar parts, and a high variance in clothing and lighting. But first of all, what exactly is HPE?</p>
<p>Basically you can differentiate between 2D and 3D pose estimation. In <strong>2D Pose Estimation</strong> a 2D pose of (x,y) coordinates for each joint from a RGB image are estimated. In <strong>3D Pose Estimation</strong> you also incorporate the prediction of a third coordinate z. In this article I will only talk about Deep Learning-based HPE models because nowadays nearly all of these models consist of a Deep Learning part (in most cases Convolutional Neural Networks (CNN)). The first CNN was applied in 2013 by Jain et al. [15]. Before this the best approaches for this task were based on body part detectors (multiple stages of processing). In this blogpost we are going to look closer at the field of deep learning-based 2D HPE (green area in Figure 1) where the basis are exclusively 2D images or videos.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hpe_cat.png#center"
         alt="Categorization of HPE methods" width="100%"/> <figcaption>
            <p>Figure 1. Categorization of HPE methods</p>
        </figcaption>
</figure>

<p>Until today a lot has been done in the field of HPE. A wide range of approaches have been researched to make progress in this challenging area. The goal of this blogpost is to give you an overview of the field of HPE, different datasets, evaluation metrics and popular architectures. First of all, there are so many papers and models out there that I cover only a few of them but I try to incorporate the most “popular” ones. For a more scientific view on this I encourage you to check out this recent review here [5]. Since I am a pretty normal human being and far from perfect, there can be mistakes in this blogpost, so if you find one I encourage you to send me an email and then I’ll fix it. Nevertheless, I hope that you like the post and that it is useful for you or exactly what you were looking for.</p>
<h2 id="approaches">Approaches<a hidden class="anchor" aria-hidden="true" href="#approaches">¶</a></h2>
<p>There are many more possibilities on how to categorize this broad field (e.g. generative vs. discriminative, regression-based vs. detection-based, one-stage vs. multi-stage). In this blogpost I will focus on multi-person HPE because this automatically implies the single-person pose estimation domain (what I mean by this will be clear in a minute). I differentiate between <em>Top-down</em> and <em>Bottom-up</em> approaches.</p>
<p><strong>Top-down:</strong> This is a two stage approach where you combine a human detection model (like Faster-RCNN) with a single-person pose estimation model. The detection model first predicts bounding boxes of the people in an image which you feed the pose estimation model which then predicts the keypoint coordinates for the person in the bounding box. This approach depends on the performance of the upstream detection model and can be computationally expensive.</p>
<p><strong>Bottom-up:</strong> The typical bottom-up HPE framework includes two stages, keypoint detection and grouping. Approaches which work this way and which are part of this blogpost are DeeperCut, OpenPose and Associative Embedding. The challenge with these models lies primarily in the correct grouping or assignment of the keypoints to the corresponding person especially in the case of unnatural poses, distortions or heavy occlusions. Important strengths of these approaches, as opposed to top-down approaches, are scalability and runtime because they don’t rely on the person detector.</p>
<h2 id="datasets">Datasets<a hidden class="anchor" aria-hidden="true" href="#datasets">¶</a></h2>
<p>Datasets are essential for fair comparison of different algorithms and bring more challenges and complexity through their expansion and improvement in recent years. In this section we present you the most important and popular datasets in the domain of 2D HPE. For a very nice and short overview take a look at Table 7 in [5].</p>
<h3 id="frames-labeled-in-cinema-flic">Frames Labeled in Cinema (FLIC)<a hidden class="anchor" aria-hidden="true" href="#frames-labeled-in-cinema-flic">¶</a></h3>
<p>The FLIC dataset [30] contains 5,003 images collected from popular Hollywood movies. For every tenth frame of 30 movies, a person detector was run to obtain about 20,000 person candidates. The groundtruth labels (10 upper body joints) were obtained through the Amazon Mechanical Turk crowdsourcing marketplace (median-of-five labeling). Those labeled images were checked and rejected manually by the authors if the person was occluded or severly non-frontal. The <em>FLIC-full</em> dataset is the full set of frames including occluded, non-frontal and plain mislabeled frames. Because many of the training set images contain samples from the test set (1,016 images) scenes which allows unfair overtraining on the FLIC test set Tompson et al. [34] proposed a new dataset called <em>FLIC-plus</em> which is a 17380 images subset from the <em>FLIC-full</em> dataset without these problems.</p>
<h3 id="leeds-sport-pose">Leeds Sport Pose<a hidden class="anchor" aria-hidden="true" href="#leeds-sport-pose">¶</a></h3>
<p>The Leeds Sport Pose dataset (LSP) [17] contains 2,000 images of full-body poses collected from Flickr by downloading with 8 sports tags: athletics, badminton, baseball, gymnastics, parkour, soccer, tennis and volleyball. The annotations include up to 14 different visible joints. Figure 2 shows a few examples from the original dataset.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/lsp_examples.png#center"
         alt="Examples from the Leeds Sport Pose Dataset" width="90%"/> <figcaption>
            <p>Figure 2. Some examples from the original LSP dataset [17]</p>
        </figcaption>
</figure>

<p>A year later Johnson et al. [18] published the extended version of this dataset containing 10,000 images collected from Flickr searches with the 3 most challenging tags: parkour, gymnastics and athletics. As for FLIC the annotations were generated by the Amazon Mechanical Turk service. The problem with this dataset is that the authors cannot guarantee for the quality of the labels.</p>
<h3 id="mpii-human-pose-dataset">MPII Human Pose Dataset<a hidden class="anchor" aria-hidden="true" href="#mpii-human-pose-dataset">¶</a></h3>
<p>The Human Pose dataset from the Max Planck Institute for Informatics (MPII) [2] is one of the state-of-the-art benchmarks for HPE with rich annotations. The dataset includes around 25,000 images with more than 40,000 people with 16 annotated joints. Furthermore the dataset covers 410 human activities which is also part of the image annotations. The images were extracted from YoutTube videos. Figure 3 shows some random images from the dataset and the associated activites.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/mpii_examples.png#center"
         alt="Some examples from the original MPII dataset" width="80%"/> <figcaption>
            <p>Figure 3. Some examples from the original MPII dataset [2]</p>
        </figcaption>
</figure>

<p>Images in MPII have various body poses and are suitable for many tasks such as 2D single- and multiple HPE, action recognition, etc.</p>
<h3 id="coco-keypoints">COCO Keypoints<a hidden class="anchor" aria-hidden="true" href="#coco-keypoints">¶</a></h3>
<p>The Microsoft Common Objects in Context (COCO) dataset [22] is a large-scale dataset that was originally proposed for object detection and segmentation in natural environments. Over time the dataset was improved and extended e.g. with image captions and keypoint labels. The images were collected from Google, Bing and Flickr image search with isolated as well as pairwise object categories. The annotations were also conducted on Amazon Mechanical Turk.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/coco_keypoint_examples_2020.png#center"
         alt="Some examples from the COCO 2020 keypoint detection task" width="90%"/> <figcaption>
            <p>Figure 4. Some examples from the COCO 2020 keypoint detection task [2]</p>
        </figcaption>
</figure>

<p>The whole dataset contains more than 200,000 images and 250,000 labeled person instances. Annotations on train and validation sets with over 150,000 people and 1.7 million labeled keypoints are publicly available. The annotations for each person include 17 body joints with visibility and left/right labels, and instance human body segmentation. Moreover the COCO dataset contains about 120,000 unlabeled images following the same class distribution as the labeled images which can be used for unsupervised or semi-supervised learning.</p>
<h3 id="aic-hkd-dataset">AIC-HKD Dataset<a hidden class="anchor" aria-hidden="true" href="#aic-hkd-dataset">¶</a></h3>
<p>The AI Challenger Human Keypoint Dataset (AIC-HKD) [37] is a subset of the large-scale dataset called AI Challenger. It is the biggest benchmark dataset out there containing 210,000 images for training, 30,000 for validation and 60,000 for testing. For the 210 000 images in training set, there are 378,374 human figures with almost 5 million keypoints. Among all the human keypoints 78.4% of them are labeled as visible(v = 1) and the rest of them are labeled as not visible(v = 2). All of these images were collected from Internet search engines. Inappropriate images (e.g. politicians, sexual contents) were removed manually. In addition, images with too many human figures (e.g. crowds in a soccer stadium) and those with very small human figures were excluded. The skeleton consists of 14 different human skeletal keypoints with one of three possible visibility flags: labeled and visible, labeled but not visible or not labeled.</p>
<p>In addition to the presented 2D image datasets there exist video-based datasets for the same purpose e.g. the Penn Action Dataset [41], the Joint-annotated Human Motion Database (J-HMDB) [16], PoseTrack [1]. You can also check them out if you are interested in this.</p>
<br>
<h2 id="evaluation">Evaluation<a hidden class="anchor" aria-hidden="true" href="#evaluation">¶</a></h2>
<p>Because of the fact that different datasets have different features, e.g. various range of human body sizes, full or only upper human body, in combination with different task requirements (single- or multi-person) there exist several evaluation metrics for 2D HPE. In the following I will describe the most common evaluation metrics in HPE which are regularly used to compare different approaches</p>
<h3 id="percentage-of-correct-parts">Percentage of Correct Parts<a hidden class="anchor" aria-hidden="true" href="#percentage-of-correct-parts">¶</a></h3>
<p>Percentage of Correct Parts (PCP) [10] is widely used in early research as evaluation metric. A limb is considered detected if the distance between the two predicted joint locations and the true limb joint locations is less than 50% of the limb length (commonly denoted as <a href="mailto:PCP@0.5">PCP@0.5</a>). The smaller the PCP value, the better the performance of the model. The drawback of this metric is that it penalizes shorter limbs because shorter limbs like lower arms which are usually harder to detect. Moreover there exist a few slightly modified versions of this metric.</p>
<h3 id="percentage-of-correct-keypoints">Percentage of Correct Keypoints<a hidden class="anchor" aria-hidden="true" href="#percentage-of-correct-keypoints">¶</a></h3>
<p>The Percentage of Correct Keypoints (PCK) [31] measures the accuracy of the localization of body joints where a candidate joint is considered as correct if it is within a certain distance or threshold of the groundtruth joint. The threshold can e.g. be a fraction of the person bounding box [40] or 50% of the head bone link, which is denoted as <a href="mailto:PCKh@0.5">PCKh@0.5</a> [3]. This alleviates the problem with shorter limbs mentioned before and is e.g. the standard evaluation metric for MPII evaluation. Another possibility is to use a pixel radius normalized by the torso height of each test sample as threshold value [30], which is also denoted as Percentage of Detected Joints (PDJ) by [35].</p>
<h3 id="average-precision--recall">Average Precision &amp; Recall<a hidden class="anchor" aria-hidden="true" href="#average-precision--recall">¶</a></h3>
<p>Imagine you have a dataset with groundtruth labels for different keypoints but without the bounding boxes. How do you evaluate your approach on such a dataset? You can use Average Presicion (AP) similar to object detection. If a predicted joint falls within a threshold of the groundtruth joint location, it is counted as true positive. For MPPE evaluation, all predicted poses are assigned to the groundtruth poses one by one based on the PCKh score order while unassigned predictions are counted as false positives. The mean AP (mAP) is reported from the AP of each body joint. For the COCO dataset AP, AR and their variants are reported based on the object keypoint similarity (OKS, more on this down below) which plays a similar role as the Intersection over Union (IoU).</p>
<h3 id="object-keypoint-similarity">Object Keypoint Similarity<a hidden class="anchor" aria-hidden="true" href="#object-keypoint-similarity">¶</a></h3>
<p>The Object Keypoint Similarity evaluation metric (OKS) is used for the Keypoint Evaluation in the COCO benchmark. For each object in this dataset the ground truth keypoints have the form $([x_{1}, y_{1}, v_{1}, &hellip;, x_{k}, y_{k}, v_{k}])$ where $(x)$ and $(y)$ are the keypoint locations and $(v)$ is a visibility flag (0 : not labeled; 1 : labeled but not visible; 2 : labeled and visible). On top of that each ground truth label has a scale $(s)$ which is defined as the square root of the object segment area.</p>
<p>The OKS is defined as followed:</p>
<p>$$OKS = \frac{\sum_{i} exp(-d_{i}^{2} / 2s^{2}k_{i}^{2}) \delta (v_{i} &gt; 0)}{\sum_{i} \delta (v_{i} &gt; 0)}$$</p>
<p>Here $d_{i}$ also describes the euclidean distance between the ground truth keypoint and the detection and $k_{i}$ is a per-keypoint constant that controls falloff.</p>
<h3 id="computational-performance">Computational Performance<a hidden class="anchor" aria-hidden="true" href="#computational-performance">¶</a></h3>
<p>The computational performance metrics are also very important in the field of HPE. The <em>Frame Rate</em> indicates the processing speed of input data, generally expressed by Frames per Second (FPS) or seconds per image (s/image). This is an important metrics e.g. for real-world applications which require real-time-estimation. The <em>Number of Weights/Parameters</em> and the <em>GFLOPs</em> (Giga Floating-point Operations per second) are two key performance indicators often mentioned. They show the efficiency of the network and the specific use of GPUs or CPUs.</p>
<h2 id="popular-architectures">Popular architectures<a hidden class="anchor" aria-hidden="true" href="#popular-architectures">¶</a></h2>
<p>In the following I will describe a few popular architectures in Single- and Multi-Human Pose Estimation but there are much more out there.</p>
<h3 id="learning-human-pose-estimation-features-with-convolutional-networks-2013">Learning Human Pose Estimation Features with Convolutional Networks (2013)<a hidden class="anchor" aria-hidden="true" href="#learning-human-pose-estimation-features-with-convolutional-networks-2013">¶</a></h3>
<p>This paper [15] describes the first deep learning approach to tackle the problem of single-person full body human pose estimation with convolutional neural networks. In this approach the authors trained multiple independent binary classification networks, one network per keypoint. The model is applied in a sliding window approach and outputs a response map indicating the confidence of the body part at that location. Figure 5 shows the architecture of the classification models.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/model_architecture_jain_et_al_2014.png#center"
         alt="The model architecture of convolutional network" width="70%"/> <figcaption>
            <p>Figure 5. The model architecture of [15]</p>
        </figcaption>
</figure>

<p>The input is of shape 64x64 pixel and locally contrast normalized (LCN). As activation functions ReLU is used. To reduce computational complexity max pooling is applied twice which leeds to some spatial information loss. After the three convoliutional layers follow three fully connected layers. To reduce overfitting, L2 regularization and dropout are applied in the fully connected layers. The output layer is a single logistic unit, representing the probability of a body part being in the patch. Moreover the authors use part priors for the final prediction. For a detailed breakdown of how all this works you should have a look at the paper. The model was evaluated on the FLIC dataset based on the PCK metric described before [31]. Figure 6 shows the performance on the wrist, elbow and shoulder joints of 351 FLIC test set images.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/conv_flic.png#center"
         alt="PCK on wrist, elbow and shoulder joints of FLIC test set" width="80%"/> <figcaption>
            <p>Figure 6. PCK on wrist, elbow and shoulder joints of FLIC test set [15]</p>
        </figcaption>
</figure>

<h3 id="convolutional-pose-machines-2016">Convolutional Pose Machines (2016)<a hidden class="anchor" aria-hidden="true" href="#convolutional-pose-machines-2016">¶</a></h3>
<p>A Convolutional Pose Machine [36] is a single-person Human Pose Estimation model which incorporates convolutional networks into the pose machine framework and inherits its benefits like the implicit learning of long-range spatial dependencies and a modular sequential design. This results in a differentiable architecture that allows for end-to-end training with backpropagation on large amounts of data. Figure 7 shows the overall architecture of the model.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpm_architecture.png#center"
         alt="The model architecture of Convolutional Pose Machines" width="75%"/> <figcaption>
            <p>Figure 7. The model architecture of [36]</p>
        </figcaption>
</figure>

<p>It consists of a sequence of stages (ConvNets) which produce 2D belief maps (heatmaps) for each part/keypoint. Before the images are fed into the network they are scaled down to a size of 368x368 pixels. The first stage consists of seven convolutional and three pooling layers with different kernel sizes. The second and all following stages are different from the first one. Here you use the first layers (share weights) of stage one to produce a belief map which is then concatenatet to the output map of the previous stage. After that you feed the concatenated maps into five more convolutional layers. Every stage outputs P+1 belief maps of 46x46 pixels where P is the number of parts and the additional belief map is for the background. At every stage of the model a loss (MSE) is computed based on these belief maps and divided by the number of pixel values (46x46x15). This also addresses the problem of vanishing gradients. In the end, these individual losses are added together to form an overall loss. At every stage the prediction quality is refined as you can see in Figure 8.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpm_joint_detections.png#center"
         alt="Convolutional Pose Machines joint detections" width="75%"/> <figcaption>
            <p>Figure 8. CPM joint detections [36]</p>
        </figcaption>
</figure>

<p>In the first and second stage the model isn’t sure which of the two wrists is the right one but in the third stage it seems to be certain. The same goes for the elbows. The model is evaluated on the MPII, FLIC and the Leeds Sports Pose datasets (for detailed information on the performance on all of these, take a look at the original paper). Figure 9 shows the results on MPII in comparison to other state-of-the-art models at that time. The evaluation is based on PCKh metric.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpm_mpii.png#center"
         alt="Convolutional Pose Machines MPII results" width="100%"/> <figcaption>
            <p>Figure 9. CPM MPII results [36]</p>
        </figcaption>
</figure>

<h3 id="deepercut-2016">Deep(er)Cut (2016)<a hidden class="anchor" aria-hidden="true" href="#deepercut-2016">¶</a></h3>
<p>DeeperCut [13] is an improved version of the DeepCut [28] model which is a multi-person pose estimation approach based on integer-linear programming (ILP) that jointly estimates poses of all people present in an image by minimizing a joint objective. One of the differences between Deep and DeeperCut is that they replace the VGG backbone from the part detector with a modified ResNet-152 for computing part probability scoremaps. This also increases the size of the receptive field from 400 to 1000px which allows a better incorporation of context in the predictions. In addition location refinement is performed by predicting offsets from the locations on the scoremap grid to the ground truth joint locations. During training sigmoid activations and cross entropy loss function are applied.</p>
<p>The large receptive field contains enough information to reason about locations of other parts/joints in the vicinity which is why the authors also use deep networks to make pairwise part-to-part predictions. Those are subsequently used to compute pairwise probabilities. For detailed information on the computations and the incorporation of these pairwise probabilities I encourage you to take a look at both original papers. For me personally it was pretty hard to follow their thinking and their calculations. Maybe it helps to inspect their implementation on github to deeply understand this. Moreover I don’t know anything about ILP.</p>
<p>The two main problems of the DeepCut ILP approach are long computation times for solving the optimization problem and the fact that no distinction is made between reliable and less-reliable detections. Therefore in DeeperCut not one but several instances of ILP’s are solved based on the reliabilities of the part detections. Figure 10 displays the results on the MPII validation set.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/deepercut_results.png#center"
         alt="DeeperCut MPII results" width="80%"/> <figcaption>
            <p>Figure 10. DeeperCut MPII results [13]</p>
        </figcaption>
</figure>

<h3 id="stacked-hourglass-networks-2016">Stacked Hourglass Networks (2016)<a hidden class="anchor" aria-hidden="true" href="#stacked-hourglass-networks-2016">¶</a></h3>
<p>The Stacked Hourglass model [27] is an approach for single-person pose estimation based on repeated bottom-up, top-down processing used in conjunction with intermediate supervision. Its name originates from its modular design and symmetric topology which you can see in Figure 11. One important operational difference between this approach and related work is that they don’t use unpooling or deconvolutional layers instead nearest neighbour upsampling and skip connections for top-down processing are incorporated. But why to use this form of network design?</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/stacked_hourglass_architecture.png#center"
         alt="Stacked Hourglass architecture" width="80%"/> <figcaption>
            <p>Figure 11. Stacked Hourglass architecture [27]</p>
        </figcaption>
</figure>

<p>This is because of the need to capture information at every scale. A final pose estimate requires a coherent understanding of the full body. The hourglass is a simple minimal design that has the capacity to capture important features like e.g. the persons orientation, the arrangement of their limbs and the relationship of adjacent joints. Figure 12 shows one hourglass module which consists of multiple convolutional and max pooling layers used to process features down to a very low resolution. At each max pooling step the network branches off and applies more convolutions at the original pre-pooled resolution. The network reaches its lowest resolution at 4x4 pixels and starts the top-down process of nearest neighbor upsampling followed by elementwise addition operations. After the last hourglass module, two consecutive 1x1 convolutional layers are added to produce the final network predictions which is a set of heatmaps containing the probability of a joints presence at each and every pixel.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hourglass_module.png#center"
         alt="Hourglass module" width="80%"/> <figcaption>
            <p>Figure 12. Hourglass module [27]</p>
        </figcaption>
</figure>

<p>The highest hourglass output resolution is 64x64 due to memory limits. As mentioned in the beginning intermediate supervision is essential for the approach which is realized by the prediction of intermediate heatmaps (between HG modules) upon which a loss can be applied. The final design consists of eight consecutive hourglass modules and a MSE loss is applied to the predictions of all modules using the same ground truth heatmap, consisting of a 2D gaussian with standard deviation of 1px centered on the joint location (similar to the Convolutional Pose Machines approach). The evaluation is carried out on the FLIC and MPII datasets using PCK normalized by torso size for FLIC and head size (PCKh) for MPII. Figure 13 shows the results on MPII. The model has mostly problems with images containing multiple people and occluded joints.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/stacked_hourglass_MPII.png#center"
         alt="Results on MPII Human Pose (PCKh@0.5)" width="90%"/> <figcaption>
            <p>Figure 13. Results on MPII Human Pose (<a href="mailto:PCKh@0.5">PCKh@0.5</a>) [27]</p>
        </figcaption>
</figure>

<h3 id="prm-2017">PRM (2017)<a hidden class="anchor" aria-hidden="true" href="#prm-2017">¶</a></h3>
<p>The proposed model is based on the Stacked Hourglass architecture incorporating so called Pyramid Residual Modules (PRM) and a new initialization scheme for multi-branch networks [39]. The goal of the introduction of PRMs is to enhance the robustness of DCNNs against scale variations of visual patterns by learning multi-scale features. Many of the popular CNN architectures are multi-branch networks, e.g. Inception, ResNets, Convolutional Pose Machines, Stacked Hourglass Network for which the existing weight initialization schemes aren’t proper. Therefore the authors propose a new method to initialize multi-branch layers that takes the number of branches into consideration. In this article we will focus on the PRMs so for detailed information on the initialization problem please take a look at the original paper.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/prm_architecture.png#center"
         alt="PRM based model architecture" width="75%"/> <figcaption>
            <p>Figure 14. PRM based model architecture [39]</p>
        </figcaption>
</figure>

<p>The approach presented in this paper is based on the Stacked Hourglass Network which is adopted and the residual units are replaced by PRMs. Figure 14 gives a nice overview of the proposed new framework. As you can see the input image is 256x256px cropped from a resized image according to the annotated body position and scale. Instead of max or average pooling, fractional max pooling is used to obtain input feature maps of different resolutions.
Now lets take a closer look at the proposed PRM and its inner workings. I am more of a visual guy and the provided overview of the different PRM versions helped me understand them better. Mathematically the PRM can be formulated as</p>
<p>$$\mathbf{x}^{(l+1)} = \mathbf{x}^{(l)} + \mathcal{P} (\mathbf{x}^{(l)}, \mathbf{W}^{(l)} )$$</p>
<p>where $(\mathcal{P}(\mathbf{x}^{(l)}; \mathbf{W}^{(l)}))$ is Feature Pyramids decomposed as</p>
<p>$$\mathcal{P}(\mathbf{x}^{(l)}, \mathbf{W}^{(l)}) = g(\displaystyle\sum_{c=1}^{C} f_{c} (\mathbf{x}^{(l)}; \mathbf{w}^{(l)}_{f_{c}}); \mathbf{w}^{(l)}_{g}) + f_{0}(\mathbf{x}^{(l)}; \mathbf{w}^{(l)}_{f_{0}})$$</p>
<p>Here $\mathbf{x}^{(l)}$ and $\mathbf{W}^{(l)}$ are the input and the filter of the $(l)$-th layer respectively. The $C$ denotes the number of pyramid levels and $\mathbf{W}^{(l)} = {\mathbf{w}_{f_{c}}^{(l)}, \mathbf{w}_{g}^{(l)}}_{c=0}^{C}$ is the set of parameters. To reduce computational and space complexity each $f_{c}(\cdot)$ is designed as a bottleneck structure. As you can see in the Figure 15 the authors experimented with four slightly different PRM architectures which all performed better than the baseline in terms of accuracy, number of parameters and complexity. Between the four new modules PRM-B showed the best balance in terms of accuracy and complexity. The number of pyramid levels $C$ was varied between 3 and 5. The authors observed that increasing $C$ generally improves the performance.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/prm_variants.png#center"
         alt="PRM variants" width="100%"/> <figcaption>
            <p>Figure 15. PRM variants [39]</p>
        </figcaption>
</figure>

<p>Important to mention is that the PRM is a general module (not only suitable for HPE) that can be used as the basic building block for various other CNN architectures e.g. in image classification networks. Regarding the training procedure and inference the proposed approach is pretty similar to common HPE approaches, as score maps (heatmaps) generated from a Gaussian represent the body joint locations. A loss is attached at the end of each stack defined by the squared error. The approach was tested on MPII and LSP. The authors used <a href="mailto:PCK@0.2">PCK@0.2</a> for LSP evaluation and <a href="mailto:PCKh@0.5">PCKh@0.5</a> for MPII. Figure 16 displays the evaluation results.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/prm_results.png#center"
         alt="PRM results on MPII" width="65%"/> <figcaption>
            <p>Figure 16. PRM results on MPII [39]</p>
        </figcaption>
</figure>

<p>The input images are 256x256 cropped from a resized image according to the annotated body position and scale. The training data is augmented by scaling, rotation, flipping and adding color and noise. In comparison to the baseline hourglass model the complexity is increased by 13.5% (from 23.7M to 26.9M parameters) and the number of GFLOPs (45.9) for a 256x256 image is also increased by 11.4%.</p>
<h3 id="associative-embedding-2017">Associative Embedding (2017)<a hidden class="anchor" aria-hidden="true" href="#associative-embedding-2017">¶</a></h3>
<p>The problem of bottom-up MPPE can be broken down into detection and grouping: detecting body joints and grouping them into individual people. Associative Embedding [26] is a single-stage method to tackle this task end-to-end by grouping detections through a process called “tagging”. A tag (real number) associates a detection with other detections of a specific group.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/asso_architecture.png#center"
         alt="Associative Embedding architecture" width="100%"/> <figcaption>
            <p>Figure 17. Associative Embedding architecture [26]</p>
        </figcaption>
</figure>

<p>Figure 17 gives an overview of the approach. Here a stacked hourglass network is used to predict a detection score at each pixel location for each body joint, regardless of person identity. In addition the network automatically groups detections into individual poses. For each joint heatmap a corresponding “tag” heatmap is produced (2m output channels for $m$ body joints). NMS is used to parse detection into individual people by retrieving their corresponding tags at those pixel locations. Then detections are grouped across body parts by comparing the tag values of detections and matching up those which are close enough. Such a group forms then the human pose estimate. The authors use a combination of a detection loss and a grouping loss on the output heatmaps for training the network. The detection loss computes the MSE between the predicted and the groundtruth heatmaps, as we have seen in approaches before. The grouping loss assess how well the predicted tags agree with the groundtruth grouping. Therefore for each person a reference embedding gets produced instead of enforcing the loss across all possible pairs of keypoints. Within an individual, the squared distance between the reference embedding and the predicted embedding for each joint are calculated. Then between pairs of people, their reference embeddings are compared to each other with a penalty that drops exponentially to zero as the distance between two tags increases. Formally this looks the following way. Let $h_{k} \in \mathbb{R}^{W \times H}$ be the predicted tagging heatmap for the $k$-th body joint, where $h(x)$ is a tag value at pixel location $x$. Given $N$ people, let the groundtruth joint locations be $T={(n_{nk})}$, $n=1,&hellip;,N$, $k=1,&hellip;,K$, where $n_{nk}$ is the groundtruth pixel location of the $k$-th joint of the $n$-th person. The reference embedding for the $k$-th person would be:</p>
<p>$$\bar{a}_{n} = \frac{1}{K} \displaystyle \sum_{k} h_{k} (n_{nk})$$</p>
<p>The grouping loss can then be defined as</p>
<p>$$L_{g}(h, T) = \frac{1}{N} \displaystyle \sum_{n} \displaystyle \sum_{k} (\bar{h}_{n} - h_{k}(x_{nk}))^{2} + \frac{1}{N^{2}} \displaystyle \sum_{n} \displaystyle \sum_{\bar{n}} \exp { - \frac{1}{2 \sigma^{2}} (\bar{h}_{n} - \bar{h}_{\bar{n}})^{2} }$$</p>
<p>Figure 18 shows an example of the tags produced by the network. In this case the tags are well separated (1D embedding) and the decoding process into groups is straightforward. A final set of predictions is produced by iterating over each joint, starting from the head and taking all activations above a certain threshold after NMS, which from the basis of an initial pool of people. Step by step the other detections are matched and assigned to a person. Important to note here is that no steps are taken to ensure anatomical correctness or reasonable spatial relationships between pairs of joints.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/asso_example.png#center"
         alt="Associative Embedding example" width="90%"/> <figcaption>
            <p>Figure 18. Associative Embedding example [26]</p>
        </figcaption>
</figure>

<p>The architecture was evaluated on MPII multi-person and COCO (results in Figure 19). The network used here consists of four stacked hourglass modules with an input size of 512x512 and an output resolution of 128x128. The model outperforms the state-of-the-art models at that time in most of the COCO test-dev metrics.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/asso_coco.png#center"
         alt="COCO test-dev results" width="90%"/> <figcaption>
            <p>Figure 19. COCO test-dev results [26]</p>
        </figcaption>
</figure>

<h3 id="rmpe-2017">RMPE (2017)<a hidden class="anchor" aria-hidden="true" href="#rmpe-2017">¶</a></h3>
<p>The Regional-Multi-Person Pose Estimation (RMPE) [9] framework imroves the performance of single-person pose estimation (SPPE) based HPE algorithms and therefore addresses two major problems of top-down HPE approaches: the localization error and redundant detections.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/rmpe_architecture.png#center"
         alt="RMPE architecture" width="100%"/> <figcaption>
            <p>Figure 20. RMPE architecture [9]</p>
        </figcaption>
</figure>

<p>It consists of three components which are shown in Figure 20: a Symmetric Spatial Transformer Network (SSTN), parametric Pose NMS and a Pose Guided Proposals Generator (PGPG). The bounding box proposals are obtained by an object detection model and fed into the SSTN+SPPE module which generates pose proposals. The SSTN extracts a high-quality single person region from an inaccurate bounding box. Figure 21 gives a detailed overview of the SSTN+SPPE module itself including the parallel SPPE for training. On the right side of the image you see an example from the original spatial transformer paper which illustrates pretty well what it does (in my opinion) with a few different examples from MNIST. The first of the three images is the input image, the second one illustrates the the result of the localisation network which predicts a transformation to apply to the input image. The third one displays the result.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/rmpe_sstn.png#center"
         alt="SSTN procedure" width="100%"/> <figcaption>
            <p>Figure 21. SSTN procedure (left) [9], Spatial Transformer example from [14]</p>
        </figcaption>
</figure>

<p>The Spatial De-Transformer Network (SDTN) is required for remapping the resulting pose estimation to the original human proposal coordinates (SDTN is an inverse procedure of STN). During training an additional parallel SPPE branch is added to improve the STN extraction quality. This branch omits the SDTN and its output is directly compared to labels of center-located groundtruth poses. The weights of this branch are fixed and is is used to backpropagate pose errors to the STN module. For more detailed information on the STN model I recommend to take a look at the original paper [14].</p>
<p>The generated pose proposals are refined by parametric Pose NMS (eliminate redundancies). This process works in the following way: first the most confident pose is selected as reference and some poses close to it are subject to elimination by applying an elimination criterion (repeat until redundant poses are eliminated) which can be mathematically expressed as follows:</p>
<p>$$f(P_{i}, P_{j}\mid\Lambda,\eta) = 1[d(P_{i},P_{j}\mid\Lambda,\lambda) \leq \eta]$$</p>
<p>$P_{i}$ and $P_{j}$ are poses with $m$ joints, denoted as ${\langle k_{i}^{1}, c_{i}^{1} \rangle, &hellip; ,\langle k_{i}^{m}, c_{i}^{m} \rangle}$ where $k_{i}^{j}$ and $c_{i}^{j}$ are the $j^{th}$ joint location and cofidence score. $d(P_{i}, P_{j}\mid\Lambda)$ is a pose distance metric with a parameter set of $\Lambda$ and a threshold $\eta$ as elimination criterion. The distance function consists of two components and can be written as</p>
<p>$$d(P_{i}, P_{j}\mid\Lambda) = K_{sim}(P_{i}, P_{j}\mid\sigma_{1}) + \lambda H_{sim}(P_{i}, P_{j}\mid\sigma_{2})$$</p>
<p>where $\lambda$ is a weight balancing the two distances and $\Lambda={\sigma_{1},\sigma_{2},\lambda}$. For more details on the two components $K_{sim}$ and $H_{sim}$ you should take a look at the original paper. The parameters described can be determined in a data driven manner.
For better adaptation to imperfect human proposals the authors created a Pose-Guided Proposal Generator which generates a large sample of additional augmented training proposals with the same distribution as the output of the human detector. The distribution of the relative offset between the detected bounding boxes and the groundtruth boxes varies across different poses, so there exists a distribution $P(\delta B\mid atom(P))$ where $\delta B$ is the offsets and $atom(P)$ denotes the atomic pose (determined through k-mean clustering) of $P$. During training for each annotated pose in the training sample, first the atomic pose is looked up and then additional offsets are generated by dense sampling according to $P(\delta B \mid a)$ to produce augmented training proposals.</p>
<p>The new framework was tested on MPII multi-person and MSCOCO keypoints 2016. The authors used the VGG-based [32] SSD-512 [23] model as human detector. In order to guarantee that the entire person region will be extracted, detected human proposals are extended by 30% along both the height and the width. As SPPE the stacked hourglass model, presented before, was incorporated. For the STN network the authors adopted a ResNet-18 as localization network and they used a smaller 4-stack hourglass network as the parallel SPPE.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/rmpe_eval.png#center"
         alt="MPII (left) and COCO (right) results" width="100%"/> <figcaption>
            <p>Figure 22. MPII (left) and COCO (right) results [9]</p>
        </figcaption>
</figure>

<p>In Figure 22 you can see the results on both benchmarks. On MPII it is most notable that the framework increases the mAP on difficult joints like wrists, elbows, ankles and knees significantly. On MSCOCO the framework performs as good as CMU-Pose (bottom-up approach) and much better than other approaches the authors looked at. In their ablation studies they evaluated the effectiveness of all three proposed components.</p>
<h3 id="cascaded-pyramid-network-2018">Cascaded Pyramid Network (2018)<a hidden class="anchor" aria-hidden="true" href="#cascaded-pyramid-network-2018">¶</a></h3>
<p>The Cascaded Pyramid Network (CPN) [6] is a network structure which targets to relieve the problem of pose estimation from especially hard-to-predict keypoints which include occluded and invisible ones as well as complex backgrounds and crowded scenes. The authors name two reasons for the difficulty. First the “harder” to detect joints cannot be simply recognized based on their appearance features only (e.g. the torso point). Second they are not explicitly addressed during the training process. Therefore this new algorithm includes two stages: GlobalNet and RefineNet (Figure 23).</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpn_architecture.png#center"
         alt="CPN architecture" width="100%"/> <figcaption>
            <p>Figure 23. CPN architecture [6]</p>
        </figcaption>
</figure>

<p>It is a top-down approach incorporating a Feature Pyramid Network (FPN) [21] as human detection model. The ROIPooling in FPN is replaced with ROIAlign from Mask-RCNN [12] and trained on all COCO categories first. The GlobalNet structure is based on the ResNet backbone. 3x3 convolutional filters are applied on the feature maps outputted by the residual blocks 2 to 5 to generate heatmaps for keypoints. The basic idea is the same as in FPN, that the shallower feature maps contain the high spatial resolution for localization but low semantic information for recognition. On the other hand the feature maps produced by deeper layers have more semantic and less spatial resolution. Figure 24 shows the output heatmaps of both the GlobalNet and RefineNet.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpn_output.png#center"
         alt="CPN output heatmaps" width="100%"/> <figcaption>
            <p>Figure 24. CPN output heatmaps [6]</p>
        </figcaption>
</figure>

<p>As you can see GlobalNet is able to effectively locate the “easier” keypoints like eyes but struggles with the “harder” ones. To improve the performance on harder keypoints RefineNet is attached to GlobalNet. It transmits the information across different levels and integrates them via upsampling and concatenation. The hard keypoints are explicitly selected based on the training loss (called <em>online hard keypoints mining</em>) and their gradients are backpropagated. Figure 24 shows the results of the proposed approach on the COCO test-dev dataset where the “+” sign indicates results using ensembled models. Each human detection box is extended to a fixed aspect ratio, e.g. height : width = 256 : 192 and then cropped from the image without distorting the images aspect ratio. Moreover data augmentation is important for the learning of scale- and rotation-invariance. After the cropping, random flipping, random rotation ($-45^\circ \sim +45^\circ$) and random scaling ($0.7 \sim 1.35$) are applied. For more detailed information about the training procedure, ablation studies and the results you should take a look at the original paper.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/cpn_coco.png#center"
         alt="CPN results on COCO test-dev" width="100%"/> <figcaption>
            <p>Figure 24. CPN results on COCO test-dev [6]</p>
        </figcaption>
</figure>

<h3 id="openpose-2019">OpenPose (2019)<a hidden class="anchor" aria-hidden="true" href="#openpose-2019">¶</a></h3>
<p>OpenPose [4] is the first open-source realtime system for bottom-up multi-person 2D pose estimation and it’s one of the most popular frameworks out there (OpenCV included it). One of the main problems of top-down approaches in HPE is their early commitment, which means that if the person detector fails (this is often the case when people are in close proximity) there is no recourse to recovery. Another problem is the computational complexity and the runtime which is proportional to the number of people in the image. OpenPose works bottom-up, like DeeperCut, but much much faster. Figure 25 illustrates the whole pipeline of this approach.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_architecture.png#center"
         alt="Pipeline of OpenPose" width="100%"/> <figcaption>
            <p>Figure 25. Pipeline of OpenPose [4]</p>
        </figcaption>
</figure>

<p>It takes as input an image of size $w \times h$ and produces the 2D locations of anatomical keypoints for each human in the image. In the first step, a convolutional neural network predicts a set of 2D confidence maps (heatmaps) $\mathbf{S}$ of body part locations (Figure 25 (b)) and a set of 2D vector fields $\mathbf{L}$ called part affinity fields (PAFs). These encode the degree of association between parts (Figure 25 (c)). The set $\mathbf{S} = ( \mathbf{S}_{1}, \mathbf{S}_{2}, \dots , \mathbf{S}_{J} )$ has $J$ confidence maps., one per part, where $\mathbf{S}_{J} \in \mathbb{R}^{w \times h}$ , $j \in {1,\dots,J}$. The set $\mathbf{L} = (\mathbf{L}_{1}, \mathbf{L}_{2}, \dots, \mathbf{L}_{C} )$ has $C$ vector fields, one per limb, where $\mathbf{L}_{c} \in \mathbb{R}^{w \times h \times 2}$ , $c \in {1,\dots,C}$. In this context limbs are part pairs. Then a parsing step creates a set of bipartite matchings to associate the body part candidates. Finally the candidates are assembled into full body poses. Figure 26 shows the architecture of the model, which iteratively predicts affinity fields (blue) and detection confidence maps (beige). The predictions are refined over the successive stages $t \in {1,\dots,T}$ including intermediate supervision at each stage similar to the Convolutional Pose Machines approach.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_model.png#center"
         alt="Architecture of OpenPose" width="70%"/> <figcaption>
            <p>Figure 26. Architecture of OpenPose [4]</p>
        </figcaption>
</figure>

<p>Before the image is fed to the first stage it gets transformed in a set of feature maps $\mathbf{F}$ by the first 10 layers of a pretrained and finetuned VGG19 network. Based on these maps the first stage produces a set of PAFs $\mathbf{L}_{1} = \phi^{1}(\mathbf{F})$ where $\phi^{1}$ refers to the operations in stage 1. In each subsequent stage the predictions from the previous stage and the original image features $\mathbf{F}$ are concatenated and used to produce refined predictions,</p>
<p>$$\mathbf{L}^{t}=\phi^{t}(\mathbf{F}, \mathbf{L}^{t-1}), \forall 2 \leq t \leq T_{p},$$</p>
<p>where $t$ denotes the stage and $T_{p}$ refers to the total number of PAF stages. After $T_{p}$ iterations the process is repeated for the confidence maps detection,</p>
<p>$$\mathbf{S}^{T_{p}}=p^{t}(\mathbf{F},\mathbf{L}^{T_{p}}), \forall t = T_{p}$$</p>
<p>$$\mathbf{S}^{t}=p^{t}(\mathbf{F},\mathbf{L}^{T_{p}}, \mathbf{S}^{t-1}), \forall T_{p} &lt; t \leq T_{p} + T_{c}$$</p>
<p>where $p_t$ refers to the CNNs for inference at stage $t$ and $T_c$ to the number of total confidence map stages. At the end of each stage a $L_{2}$ loss between the estimated predictions and the groundtruth maps and fields is applied. The loss functions are:</p>
<p>$$ f_{\mathbf{L}^t_{i}} = \displaystyle\sum_{c=1}^C \displaystyle\sum_{\mathbf{p}} \mathbf{W} (\mathbf{p}) \cdot \lVert \mathbf{L}^{t_{i}} (\mathbf{p}) - \mathbf{L}^{*}_{c} (\mathbf{p}) \rVert_{2}^{2}$$</p>
<p>$$ f_{\mathbf{S}^{t}_{k}} = \displaystyle\sum_{j=1}^{J} \displaystyle\sum_{\mathbf{p}} \mathbf{W}(\mathbf{p}) \cdot | \mathbf{S}_{j}^{t_{k}}(\mathbf{p}) - \mathbf{S}_{j}^{*}(\mathbf{p}) |_{2}^2 $$</p>
<p>where $\mathbf{L}_{c}^{*}$ is the groundtruth PAF, $\mathbf{S}_{j}^{*}$ is the groundtruth part confidence map and $\mathbf{W}$ is a binary mask with $\mathbf{W}(\mathbf{p})=0$ when the annotation of a person or part is missing at the pixel $\mathbf{p}$ (in many datasets not all humans in an image are labelled). The overall objective is:</p>
<p>$$f=\displaystyle \sum_{t=1}^{T_{p}} f_{\mathbf{L}}^{t} + \displaystyle \sum_{t = T_{p} + 1}^{T_{p} + T_{c}} f_{\mathbf{S}}^{t}$$</p>
<p>Now lets take a closer look on how the confidence maps and the PAFs are generated. The confidence maps $\mathbf{S}_{j,k}^{*}$ for each person $k$ and the joint $j$ are defined as</p>
<p>$$\mathbf{S}_{j,k}^{*}(\mathbf{p})=\exp(- \frac{| \mathbf{p} - \mathbf{x}_{j,k} |_{2}^{2}}{\sigma^{2}}),$$</p>
<p>where $x_{j,k}$ is the location of the body part and $\sigma$ controls the spread of the peak. At test time the predicted body part candidates are obtained by performing NMS.
Now lets get to the PAFs. Each of them is a 2D vector field for each limb. For every pixel in the area belonging to a particular limb, a 2D vector encodes the direction that points from one part of the limb to the other. Let $x_{j_{1},k}$ and $x_{j_{2},k}$ be the groundtruth positions of body parts $j_{1}$ and $j_{2}$ from the limb $c$ for person $k$ in the image. If a point $\mathbf{p}$ lies on the limb, the value at $\mathbf{L}^{*}_{c,k}(\mathbf{p})$ is a unit vector that points from $j_{1}$ to $j_{2}$. For all other points the vector is zero valued. The groundtruth PAF $\mathbf{L}^{*}_{c,k}$ at point $\mathbf{p}$ is defined as</p>
<p>$$\mathbf{L}^{*}_{c,k} (\mathbf{p}) =
\begin{cases}
\mathbf{v}      &amp; \quad \text{if } p \text{ on limb } c, k\
0  &amp; \quad \text{otherwise}
\end{cases}$$</p>
<p>where $\mathbf{v} = (x_{j_{2},k} - x_{j_{1},k}) / | x_{j_{2},k} - x_{j_{1},k} |_{2}$ is the unit vector in the direction of the limb. So now, how do you define the set of points which lie on the limb? This is accomplished by a distance threshold of the line segment, e.g. those points $\mathbf{p}$ for which</p>
<p>$$0 \leq \mathbf{v} \cdot (\mathbf{p} - x_{j_{1},k}) \leq l_{c,k} \text{and} | \mathbf{v}_{\bot} \cdot (\mathbf{p} - x_{j_{1},k}) | \leq \sigma_{l},$$</p>
<p>where the limb width $\sigma_{l}$ is a distance in pixel, $l_{c,k} =  | x_{j_{2},k} - x_{j_{1},k} |_{2}$ is the limb length and $\mathbf{v}_{\bot}$ is a vector perpendicular to $\mathbf{v}$. The groundtruth PAF averages the affinity fields of all people in the image,</p>
<p>$$\mathbf{L}^{*}(\mathbf{p}) = \frac{1}{n_{c}(\mathbf{p})} \displaystyle \sum_{k} \mathbf{L}_{c,k}^{*} (\mathbf{p})$$</p>
<p>where $n_c(\mathbf{p})$ is the number of non-zero vectors at point $\mathbf{p}$ across all people. The association between part candidates is measured by computing the line integral over the corresponding PAF along the line segment connecting the candidate part locations. This measures the aligment of the predicted PAF with the candidate limb that would be formed. For two candidate part locations $\mathbf{d}_{j_{1}}$ and $\mathbf{d}_{j_{2}}$, the predicted PAF $L_{c}$ gets sampled along the line segment to measure the confidence in their association:</p>
<p>$$E= \int_{u=0}^{u=1} \mathbf{L}_{c}(\mathbf{p}(u)) \cdot \frac{\mathbf{d}_{j_{2}} - \mathbf{d}_{j_{1}}}{ | \mathbf{d}_{j_{2}} - \mathbf{d}_{j_{1}} |_{2}} du,$$</p>
<p>where $p(u)$ interpolates the position of the two body parts $\mathbf{d}_{j_{1}}$ and $\mathbf{d}_{j_{2}}$,</p>
<p>$$\mathbf{p}(u) = (1-u) \mathbf{d}_{j_{1}} + u \mathbf{d}_{j_{2}}$$</p>
<p>To obtain a discrete set of part candidate locations NMS on the confidence maps is performed, which form a large set of possible limbs. Each limb candidate is scored by the line integral described before. The resulting $K-dim$ matching problem is known to be NP-hard but in the special case of HPE two relaxations can be added. First, instead of the whole graph a minimal number of edges to obtain a spanning tree skeleton can be chosen. Second, the matching problem can be decomposed into a set of bipartite matching subproblems. The authors demonstrate that a minimal greedy inference well-approximates the global solution at a fraction of the computational cost.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_mpii.png#center"
         alt="MPII results of OpenPose" width="70%"/> <figcaption>
            <p>Figure 27. MPII results of OpenPose [4]</p>
        </figcaption>
</figure>

<p>The method is evaluated on three MPPE datasets: MPII multi-person, COCO and a self created foot dataset. Figure 27 shows the results on the COCO test-dev set based on the OKS evaluation metric. One of the main strengths of this approach, in comparison to top-down approaches, is its inference runtime which is independent from the number of people per image. Figure 28 illustrates its superior performance in this regard. This realtime capability is also very important for many real-world applications and one of the reasons why this approach is very well known.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/openpose_runtime.png#center"
         alt="Runtime of OpenPose" width="85%"/> <figcaption>
            <p>Figure 28. Runtime of OpenPose [4]</p>
        </figcaption>
</figure>

<h3 id="hrnet-2019">HRNet (2019)<a hidden class="anchor" aria-hidden="true" href="#hrnet-2019">¶</a></h3>
<p>In [33] the authors present a novel model architecture for SPPE, called <em>High-Resolution Net</em> (HRNet) which is able to maintain high-resolution representations through the estimation process. Figure 29 illustrates the architecture of this approach. It consists of parallel high-to-low resolution subnetworks starting from a high-resolution subnetwork. Gradually high-to-low resolution subnetworks are added to form more stages. These multi-resolution subnetworks are connected in parallel. Multi-scale fusions processes are carried out to enable the information exchange across these parallel subnetworks.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hrnet_architecture.png#center"
         alt="Architecture of HRNet" width="70%"/> <figcaption>
            <p>Figure 29. Architecture of HRNet [33]</p>
        </figcaption>
</figure>

<p>The two main benefits of this approach in comparison to existing methods are a higher spatial precision due to the maintenance of high resolution (instead of recovering of high from low resolution) and the parallelization instead of serialization. Moreover the network outputs more accurate heatmaps because of the repeated multi-scale fusions. Now lets take a closer look at the details.
The goal is to detect the locations of $K$ keypoints in an image $\mathbf{I}$ of size $W \times H \times 3$. The problem is transformed to an estimation of $K$ heatmaps of size $W^{\prime} \times H^{\prime}, { \mathbf{H}_{1}, \mathbf{H}_{2} ,&hellip; , \mathbf{H}_{K}}$ where each heatmap $\mathbf{H}_{k}$ indicates the location confidence of the $k$th keypoint. The HRNet model is composed of a stem consisting of two strided convolutions decreasing the resolution to $1/4$ of the input size (input: 384x288 to output: 96x72; or input: 256x192 to output: 64x48). The main body which is displayed in Figure 29 outputs the feature maps in the same resolution as the inputs. It consists of parallel multi-resolution subnetworks at different stages. Let $\mathcal{N}_{sr}$ be the subnetwork in the $s$th stage and $r$ be the resolution index, this is an example containing 4 parallel subnetworks</p>
<p>$$\begin{matrix}
\mathcal{N}_{11} &amp; \rightarrow &amp; \mathcal{N}_{21} &amp; \rightarrow &amp; \mathcal{N}_{31} &amp; \rightarrow &amp; \mathcal{N}_{41} \newline
&amp; \searrow &amp; \mathcal{N}_{22} &amp; \rightarrow &amp; \mathcal{N}_{32} &amp; \rightarrow &amp; \mathcal{N}_{42} \newline
&amp; &amp; &amp; \searrow &amp; \mathcal{N}_{33} &amp; \rightarrow &amp; \mathcal{N}_{43} \newline
&amp; &amp; &amp; &amp; &amp; \searrow &amp; \mathcal{N}_{44}
\end{matrix}$$</p>
<p>As you can see in Figure 29, the subnetworks are connected to each other in parallel through so called <em>exchange units</em> which aggregate the information from high, medium and low resolutions (HRNet has 8 of them in total). The authors call this process repeated multi-scale fusion which is illustrated in Figure 30.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hrnet_exchange_units.png#center"
         alt="HRNet exchange units" width="60%"/> <figcaption>
            <p>Figure 30. HRNet exchange units [33]</p>
        </figcaption>
</figure>

<p>Lets look at the formulation of these exchange units a bit closer. The inputs are $s$ response maps: ${\mathbf{X}_{1}, \mathbf{X}_{2}, &hellip; , \mathbf{X}_{s}}$. The outputs are also $s$ response maps: ${\mathbf{Y}_{1}, \mathbf{Y}_{2}, &hellip; , \mathbf{Y}_{s}}$ whose resolutions and widths are the same to the input. Each output is an aggregation of the input maps $\mathbf{Y}_{k} = \sum_{i=1}^{s} a(\mathbf{X}_{i}, k)$. The function $a(\mathbf{X}_{i}, k)$ consists of upsampling (nearest neighbor sampling + 1x1 convolution) or downsampling (strided 3x3 convolution) $\mathbf{X}_{i}$ from resolution $i$ to resolution $k$. The heatmap estimation is done based on the high-resolution output by the last exchange unit. The groundtruth heatmaps are generated through 2D Gaussian with a standard deviation of 1 pixel centered on the location of the keypoint. The loss function applied on the heatmaps is Mean Squared Error. Since HRNet is a top-down model for SPPE, the authors used groundtruth bounding box labels in the training process without a separate detection model. The bounding box is extended in height or width to a fixed aspect ratio of height : width = 4 : 3 and then the box is cropped from the image and resized (256 x 192 or 384 x 288). The data augmentation includes random rotation, random scale and flipping. For more details on the training process take a look at the paper (it’s very well written in my opinion). Figure 31 shows the results and comparisons with other state-of-the-art models on the CODO test-dev set. In addition the authors tested the model on MPII and PoseTrack.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/hrnet_coco.png#center"
         alt="Results on COCO test-dev" width="100%"/> <figcaption>
            <p>Figure 31. Results on COCO test-dev [33]</p>
        </figcaption>
</figure>

<h3 id="efficientpose-2020">EfficientPose (2020)<a hidden class="anchor" aria-hidden="true" href="#efficientpose-2020">¶</a></h3>
<p>One of the main problems of state-of-the-art models is their high degree computational complexity which makes them cumbersome to optimize, hard to replicate and impractical to embed into real-world applications. Because of this a novel approach for single-person pose estimation from 2D images called EfficientPose [11] was introduced. The authors compare this to OpenPose because this model is one of the most applied HPE methods in real-world applications and the first open-source real-time HPE system. The problems with OpenPose are its low resolution output (46x46px) and the computational expense (160 billion floating point operations per inference). To overcome these shortcomings the EfficientPose architecture is based on OpenPose including many important modifications to improve the level of precision, decrease computational cost and model size.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/efficientpose_architecture.png#center"
         alt="EfficientPose architecture" width="100%"/> <figcaption>
            <p>Figure 32. EfficientPose architecture [11]</p>
        </figcaption>
</figure>

<p>The first modification are two inputs, one high- and one low-resolution image (Figure 32 - 1a) and 1b)). The low resolution input is downsampled to half the pixel height and width of the high resolution input by an initial average pooling layer. Both inputs are fed into scalable EfficientNet backbones, pretrained on ImageNet, which are used as feature extractors. High-level semantic information is obtained from the high resolution image using the initial three blocks of a high-scale EfficientNet (B2-B7) which outputs C feature maps. Low-level information is extracted from the low resolution image by the first two blocks of a lower scale EfficientNet (B0-B3). For detailed information on the structure of the before mentioned EfficientNet blocks take a look at the paper. After that the extracted low- and high-level features are concatenated to yield cross-resolution features. The keypoint localization is carried out through an iterative detection process exploiting intermediate supervision. The iterative decision process consists of three passes through detection blocks which all have the same basic structure (Mobile DenseNets). Detailed information on the architecture of these detection blocks can be found in the paper. The detection is carried out in two rounds. First the overall pose is anticipated through a single pass of skeleton estimation (Figure 32 - 5a). The goal of this first pass is to facilitate detection of feasible poses and to avoid confusion if there are multiple people present in an image. Part Affinity Fields are utilized when performing skeleton estimation. Following this step, two detection passes are carried out to estimate heatmaps for keypoints of interest. The former pass acts as a coarse detector (Figure 32 - 5b) while the second one (Figure 32 - 5c) refines localization to improve the accuracy of the outputs. In the sixth and last step the level of detail of the outputs is increased by three transposed convolutions (each of them increases the map size by a factor of two) performing bilinear upsampling.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/efficientpose_variants.png#center"
         alt="EfficientPose variants" width="100%"/> <figcaption>
            <p>Figure 33. EfficientPose variants [11]</p>
        </figcaption>
</figure>

<p>Five different variants of EfficientPose models are presented, details in Figure 33. EfficientPoseRT is a single-resolution model, matching the scale of the smallest EfficientNet model, providing HPE in extremely low latency applications. The evaluation of the models is based on MPII single-person dataset (train-val-split: 26K, 3K). During training the authors use data augmentation (random horizontal flipping, scaling, rotation). For evaluation the PCKh metric is calculated and compared to OpenPose and other models. In Figure 34 the results in comparison to OpenPose are displayed.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/efficientpose_openpose_comparison.png#center"
         alt="Comparison between EfficientPose and OpenPose" width="100%"/> <figcaption>
            <p>Figure 34. Comparison between EfficientPose and OpenPose [11]</p>
        </figcaption>
</figure>

<p>The presented approach is 4-5.6x smaller than OpenPose regarding the model size (number of parameters) and realizes a 2.2 - 184x reduction in FLOPS. In addition to that the new approach converges faster. The comparison here was between EfficientPoseII and OpenPose because of the similar input size. Potentially the model can also be interesting for multi-person-pose-estimation in a bottom-up fashion based on Part Affinity Fields like OpenPose.</p>
<h3 id="evopose2d-2020">EvoPose2D (2020)<a hidden class="anchor" aria-hidden="true" href="#evopose2d-2020">¶</a></h3>
<p>EvoPose2D [24] is the first SPPE network design based on neuroevolution, which is a form of neural architecture search (NAS). The main benefit of networks designed this way is the elimination of human bias leading to more accurate and computationally efficient models. The authors also present a new flexible weight transfer scheme in combination with large-batch (up to 2048 256x192 images) training on Tensor Processing Units (TPUs) that reduces the computational expense of neuroevolution and has no loss in accuracy. For detailed information on neuroevolution take a look at [29]. In this case it is used because of its simplicity compared to alternative approaches based on reinforcement learning, one-shot or gradient-based NAS.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/evopose_weight_transfer.png#center"
         alt="Weight transfer scheme" width="70%"/> <figcaption>
            <p>Figure 35. Weight transfer scheme [24]</p>
        </figcaption>
</figure>

<p>Figure 35 illustrates the weight transfer scheme pretty well. $W^{(l)} \in \mathbb{R}^{k_{p1} \times k_{p2} \times i_{p} \times o_{p}}$ are the weights of layer $l$ used by the parent network and $V^{(l)} \in \mathbb{R}^{k_{c1} \times k_{c2} \times i_{c} \times o_{c}}$ denote the weights of the mutated child network where $k$ is the kernel size, $i$ is the number of input channels and $o$ is the number of output channels. Figure 35 shows two examples ($W \to V_{1}, W \to V_{2}$) of this weight transfer process. The trained weights (shown in blue) in the parent convolutional filter $W$ are transferred, either in part or in full to the corresponding filter $V$ in the mutated child network. The weight transfer extends to all output channels in the same manner as depicted here for the input channels. As a result the mutated network can be trained using fewer iterations which accelerates the neuroevolution.
But keep in mind NAS is far from perfect and you as a human have to design and limit the search space carefully. The starting point in this work was the Simple-Baseline [38] architecture. For each module the authors searched for the optimal kernel size, number of inverted residual blocks, output channels and in the last three modules they additionally searched for the optimal stride (the resulting search space can produce $10^{14}$ unique backbones).
A multi-objective fitness function was applied to create a balance between computational efficiency and accuracy including the validation loss and the number of network parameters. The fitness of the network $\mathcal{N}$ can be defined as:</p>
<p>$$\mathcal{J}(\mathcal{N}) = \left(\frac{T}{n(\theta^{\mathcal{N}})}\right)^{\Gamma} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L} (\mathcal{N}, \mathbf{I}_{i})$$</p>
<p>where $N$ is the number of samples in the validation set, $\mathbf{I}$ is the input image, $n(\theta^{\mathcal{N}})$ is the number of parameters in $\mathcal{N}$, $T$ is the target number of parameters and $\Gamma$ controls the fitness tradeoff between the number of parameters and the validation loss $\mathcal{L} (\mathcal{N}, \mathbf{I}_{i})$. Now lets take a look at the evolutionary strategy. The first generation is a manually defined model and trained for $e_{0}$ epochs. In the next generation $\lambda$ children are generated by mutating the ancestor network (including weight transfer) and then trained for $e$ epochs ($e \ll e_{0}$). After training, the $\mu$ networks with the best fitness become the parents in the next generation and so on. This process continues until manual termination. Motivated by the success of compound scaling (like in EfficientNet) the authors also scaled the base network to different input resolutions using the following depth ($c_{d}$) and width ($c_{w}$) coefficients:</p>
<p>$$c_{d} = \alpha^{\phi} \quad c_{w} = \beta^{\phi} \quad \phi = \frac{\log r - \log r_{s}}{\log \gamma}$$</p>
<p>where $r_{s}$ is the search resolution, $r$ is the desired resolution and $\alpha$, $\beta$, $\gamma$ are scaling parameters.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/evopose_architecture.png#center"
         alt="EvoPose2D-S architecture" width="50%"/> <figcaption>
            <p>Figure 36. EvoPose2D-S architecture [24]</p>
        </figcaption>
</figure>

<p>Figure 36 shows the architecture of the second smallest version of the EvoPose2D models, called EvoPose2D-S, designed via neuroevolution. With an input width and height of $256 \times 192$ and $17$ keypoints to predict, it only contains $2.53M$ parameters and $1.07$ GFLOPs. It has a performance on COCO similar to Simple-Baseline (ResNet50) which has $34.1M$ parameters and $5.21$ GFLOPs. The evaluation of the models was done on the COCO keypoints 2017 dataset. As you can see in Figure 37 the EvoPose2D models have up to 13.5 times less parameters, the performance is similar to state-of-the-art architectures like HRNet-W48 and the input resolution is higher.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/human_pose_estimation/evopose_results.png#center"
         alt="EvoPose2D COCO results" width="100%"/> <figcaption>
            <p>Figure 37. EvoPose2D COCO results [24]</p>
        </figcaption>
</figure>

<h3 id="more-architectures">More architectures<a hidden class="anchor" aria-hidden="true" href="#more-architectures">¶</a></h3>
<p>As mentioned in the beginning of the section there exist so many different approaches to 2D HPE out there and I only covered a few of them here. If you want to learn more you can take a look at the following approaches which may interest you as well:</p>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/1702.07432.pdf">Multi-context attention for human pose estimation</a> (2017) [8]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1804.06208.pdf">SimpleBaselines</a> (2018) [38]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1911.10529.pdf">SimplePose</a> (2019) [19]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1908.10357.pdf">HigherHRNet</a> (2019) [7]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1812.03595.pdf">PoseFix</a> (2019) [25]</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1901.00148.pdf">MSPN</a> (2019) [20]</p>
</li>
</ul>
<p>You can find all the papers in the References. Maybe I will update this post in the future including more up to date models as well. Moreover you can find this post as pdf file in the assets folder.</p>
<br>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">¶</a></h2>
<p><a href="https://arxiv.org/pdf/1710.10000.pdf">[1]</a> M. Andriluka, U. Iqbal, E. Ensafutdinov, L. Pishchulin, A. Milan, J. Gall, and S. B. PoseTrack: A benchmark for human pose estimation and tracking. In CVPR, 2018.</p>
<p><a href="https://ieeexplore.ieee.org/document/6909866">[2]</a> M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.</p>
<p><a href="https://ieeexplore.ieee.org/document/6909866">[3]</a> M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pages 3686–3693, 2014.</p>
<p><a href="https://arxiv.org/pdf/1812.08008.pdf">[4]</a> Z. Cao, G. H. Martinez, T. Simon, S.-E. Wei, and Y. A. Sheikh. Openpose: realtime multi-person 2d pose estimation using part affinity fields. IEEE transactions on pattern analysis and machine intelligence, 2019.</p>
<p><a href="https://arxiv.org/pdf/2006.01423.pdf">[5]</a> Y. Chen, Y. Tian, and M. He. Monocular human pose estimation: A survey of deep learning-based methods. Computer Vision and Image Understanding, 192:102897, 2020.</p>
<p><a href="https://arxiv.org/pdf/1711.07319.pdf">[6]</a> Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun. Cascaded pyramid network for multi-person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7103–7112, 2018.</p>
<p><a href="https://arxiv.org/pdf/1908.10357.pdf">[7]</a> B. Cheng, B. Xiao, J. Wang, H. Shi, T. S. Huang, and L. Zhang. Higherhrnet: Scale- aware representation learning for bottom-up human pose estimation. arXiv preprint arXiv:1908.10357, 2019.</p>
<p><a href="https://arxiv.org/pdf/1702.07432.pdf">[8]</a> X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang. Multi-context attention for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1831–1840, 2017.</p>
<p><a href="https://arxiv.org/pdf/1612.00137.pdf">[9]</a> H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu. Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2334– 2343, 2017.</p>
<p><a href="https://ieeexplore.ieee.org/document/4587468">[10]</a> V. Ferrari, M. Marin-Jimenez, and A. Zisserman. Progressive search space reduction for human pose estimation. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2008.</p>
<p><a href="https://arxiv.org/pdf/2004.12186.pdf">[11]</a> D. Groos, H. Ramampiaro, and E. Ihlen. Efficientpose: Scalable single-person pose estimation. arXiv preprint arXiv:2004.12186, 2020.</p>
<p><a href="https://arxiv.org/pdf/1703.06870.pdf">[12]</a> K. He, G. Gkioxari, P. Doll ́ar, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961–2969, 2017.</p>
<p><a href="https://arxiv.org/pdf/1605.03170.pdf">[13]</a> E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger, and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50. Springer, 2016.</p>
<p><a href="https://arxiv.org/pdf/1506.02025.pdf">[14]</a> M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pages 2017–2025, 2015.</p>
<p><a href="https://arxiv.org/pdf/1312.7302.pdf">[15]</a> A. Jain, J. Tompson, M. Andriluka, G. W. Taylor, and C. Bregler. Learning human pose estimation features with convolutional networks. arXiv, pages arXiv–1312, 2013.</p>
<p><a href="https://ieeexplore.ieee.org/document/6751508">[16]</a> H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J. Black. Towards understanding action recognition. In International Conf. on Computer Vision (ICCV), pages 3192–3199, Dec. 2013.</p>
<p><a href="http://www.bmva.org/bmvc/2010/conference/paper12/paper12.pdf">[17]</a> S. Johnson and M. Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In Proceedings of the British Machine Vision Conference, 2010. doi:10.5244/C.24.12.</p>
<p><a href="https://ieeexplore.ieee.org/document/5995318">[18]</a> S. Johnson and M. Everingham. Learning effective human pose estimation from inaccu- rate annotation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2011.</p>
<p><a href="https://arxiv.org/pdf/1911.10529.pdf">[19]</a> J. Li, W. Su, and Z. Wang. Simple pose: Rethinking and improving a bottom-up approach for multi-person pose estimation. CoRR, abs/1911.10529, 2019.</p>
<p><a href="https://arxiv.org/pdf/1901.00148.pdf">[20]</a> W. Li, Z. Wang, B. Yin, Q. Peng, Y. Du, T. Xiao, G. Yu, H. Lu, Y. Wei, and J. Sun. Rethinking on multi-stage networks for human pose estimation. arXiv preprint arXiv:1901.00148, 2019.</p>
<p><a href="https://arxiv.org/pdf/1612.03144.pdf">[21]</a> T.-Y. Lin, P. Doll ́ar, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117–2125, 2017.</p>
<p><a href="https://arxiv.org/pdf/1405.0312.pdf">[22]</a> T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll ́ar, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer, 2014.</p>
<p><a href="https://arxiv.org/pdf/1512.02325.pdf">[23]</a> W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21–37. Springer, 2016.</p>
<p><a href="https://arxiv.org/pdf/2011.08446.pdf">[24]</a> W. McNally, K. Vats, A. Wong, and J. McPhee. Evopose2d: Pushing the boundaries of 2d human pose estimation using neuroevolution. arXiv preprint arXiv:2011.08446, 2020.</p>
<p><a href="https://arxiv.org/pdf/1812.03595.pdf">[25]</a> G. Moon, J. Y. Chang, and K. M. Lee. Posefix: Model-agnostic general human pose refinement network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7773–7781, 2019.</p>
<p><a href="https://arxiv.org/pdf/1611.05424.pdf">[26]</a> A. Newell, Z. Huang, and J. Deng. Associative embedding: End-to-end learning for joint detection and grouping. In Advances in neural information processing systems, pages 2277–2287, 2017.</p>
<p><a href="https://arxiv.org/pdf/1603.06937.pdf">[27]</a> A. Newell, K. Yang, and J. Deng. Stacked hourglass networks for human pose estimation. In European conference on computer vision, pages 483–499. Springer, 2016.</p>
<p><a href="https://arxiv.org/pdf/1511.06645.pdf">[28]</a> L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4929–4937, 2016.</p>
<p><a href="https://arxiv.org/pdf/1703.01041.pdf">[29]</a> E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. Le, and A. Kurakin. Large-scale evolution of image classifiers. arXiv preprint arXiv:1703.01041, 2017.</p>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sapp_MODEC_Multimodal_Decomposable_2013_CVPR_paper.pdf">[30]</a> B. Sapp and B. Taskar. Modec: Multimodal decomposable models for human pose estimation. In In Proc. CVPR, 2013.</p>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sapp_MODEC_Multimodal_Decomposable_2013_CVPR_paper.pdf">[31]</a> B. Sapp and B. Taskar. Modec: Multimodal decomposable models for human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674–3681, 2013.</p>
<p><a href="https://arxiv.org/pdf/1409.1556.pdf">[32]</a> K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.</p>
<p><a href="https://arxiv.org/pdf/1902.09212.pdf">[33]</a> K. Sun, B. Xiao, D. Liu, and J. Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5693–5703, 2019.</p>
<p><a href="https://arxiv.org/pdf/1406.2984.pdf">[34]</a> J. Tompson, A. Jain, Y. Lecun, and C. Bregler. Joint training of a convolutional network and a graphical model for human pose estimation. NIPS, 2014.</p>
<p><a href="https://arxiv.org/pdf/1312.4659.pdf">[35]</a> A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1653–1660, 2014.</p>
<p><a href="https://arxiv.org/pdf/1602.00134.pdf">[36]</a> S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Convolutional pose machines. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 4724–4732, 2016.</p>
<p><a href="https://arxiv.org/pdf/1711.06475.pdf">[37]</a> J. Wu, H. Zheng, B. Zhao, Y. Li, B. Yan, R. Liang, W. Wang, S. Zhou, G. Lin, Y. Fu, et al. Ai challenger: A large-scale dataset for going deeper in image understanding. arXiv preprint arXiv:1711.06475, 2017.</p>
<p><a href="https://arxiv.org/pdf/1804.06208.pdf">[38]</a> B. Xiao, H. Wu, and Y. Wei. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), pages 466–481, 2018.</p>
<p><a href="https://arxiv.org/pdf/1708.01101.pdf">[39]</a> W. Yang, S. Li, W. Ouyang, H. Li, and X. Wang. Learning feature pyramids for human pose estimation. In proceedings of the IEEE international conference on computer vision, pages 1281–1290, 2017.</p>
<p><a href="http://www.cs.cmu.edu/~deva/papers/pose_pami.pdf">[40]</a> Y. Yang and D. Ramanan. Articulated human detection with flexible mixtures of parts. IEEE transactions on pattern analysis and machine intelligence, 35(12):2878–2890, 2012.</p>
<p><a href="https://ieeexplore.ieee.org/document/6751390">[41]</a> W. Zhang, M. Zhu, and K. G. Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In Proceedings of the IEEE Interna- tional Conference on Computer Vision, pages 2248–2255, 2013.</p>


  </div>

  <footer class="post-footer">
  </footer>
    <div class="comments-separator"></div><div class="comments">
  <div class="title">
    <span>Comments</span>
    <span class="counter"><span class="remark42__counter" data-url="http://localhost:1313/posts/2020-12-18-overview-of-human-pose-estimation/"></span></span>
  </div>
  <div id="remark42">
  </div>
</div>

<script>
  var remark_config = {
    host: 'https:\/\/remark42.johannsblog.com',
    site_id: 'johanngerberding.github.io',
    components: ['embed', 'counter'],
    max_shown_comments: 20,
    theme: 'light',
    simple_view: true,
    admonition: JSON.stringify(['Please subscribe by email to receive reply notifications.']),
  };
  if (isDarkTheme()) {
    remark_config.theme = 'dark'
  }

  (function() {
    
    const key = 'remark42'
    if (!toggleThemeCallbacks.hasOwnProperty(key)) {
      toggleThemeCallbacks[key] = (isDark) => {
        const remark42 = window.REMARK42
        if (!remark42 || !document.querySelector('#remark42')) {
          return;
        }
        if (isDark) {
          remark42.changeTheme('light');
        } else {
          remark42.changeTheme('dark');
        }
      }
    }

    
    const remark42 = window.REMARK42
    if (remark42) {
      remark42.destroy()
      remark42.createInstance(remark_config)
    } else {
      for (const component of remark_config.components) {
        var d = document, s = d.createElement('script');
        s.src = `${remark_config.host}/web/${component}.mjs`;
        s.type = 'module';
        s.defer = true;
        
        s.setAttribute('data-no-instant', '')
        d.head.appendChild(s);
      }
    }
  })();
</script>

</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2024 <a href="http://localhost:1313/">Johanns Blog</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

</body>

</html>
