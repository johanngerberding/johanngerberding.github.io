<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Object Detection - Faster Models | Johanns Blog</title>
<meta name="keywords" content="deep-learning, computer-vision, convolutional-neural-networks">
<meta name="description" content="One-Stage Object Detection Models.">
<meta name="author" content="Johann Gerberding">
<link rel="canonical" href="http://localhost:1313/posts/2021-05-02-object-detection-faster-models/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.64af4b41dbd89d6dde28b6a76b3a09ff56317a63b390d279aa1872102bc802d7.css" integrity="sha256-ZK9LQdvYnW3eKLanazoJ/1YxemOzkNJ5qhhyECvIAtc=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2021-05-02-object-detection-faster-models/">

<meta name="twitter:title" content="Object Detection - Faster Models | Johanns Blog" />
<meta name="twitter:description" content="One-Stage Object Detection Models." />
<meta property="og:title" content="Object Detection - Faster Models | Johanns Blog" />
<meta property="og:description" content="One-Stage Object Detection Models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/2021-05-02-object-detection-faster-models/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2021-05-02T10:45:16&#43;02:00" />
  <meta property="article:modified_time" content="2021-05-02T10:45:16&#43;02:00" /><meta property="og:site_name" content="Johann&#39;s Blog" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Object Detection - Faster Models",
      "item": "http://localhost:1313/posts/2021-05-02-object-detection-faster-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Object Detection - Faster Models | Johanns Blog",
  "name": "Object Detection - Faster Models",
  "description": "One-Stage Object Detection Models.",
  "keywords": [
    "deep-learning", "computer-vision", "convolutional-neural-networks"
  ],
  "wordCount" : "2394",
  "inLanguage": "en",
  "datePublished": "2021-05-02T10:45:16+02:00",
  "dateModified": "2021-05-02T10:45:16+02:00",
  "author":{
    "@type": "Person",
    "name": "Johann Gerberding"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/2021-05-02-object-detection-faster-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Johanns Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" dark type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'dark';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Johanns Blog (Alt + H)">Johanns Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="posts" class="active"
                ><i class='fa fa-heart'></i>posts
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags"
                ><i class='fa fa-heart'></i>tags
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/reading/" title="reading"
                ><i class='fa fa-heart'></i>reading
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about-me/" title="about"
                ><i class='fa fa-heart'></i>about
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Object Detection - Faster Models</h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>May 2, 2021</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select: text;"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z" style="user-select: text;"></path><line x1="7" y1="7" x2="7" y2="7" style="user-select: text;"></line></svg>
  <span class="post-tags"><a href="http://localhost:1313/tags/deep-learning/">Deep-Learning</a><a href="http://localhost:1313/tags/computer-vision/">Computer-Vision</a><a href="http://localhost:1313/tags/convolutional-neural-networks/">Convolutional-Neural-Networks</a></span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select: text;"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" style="user-select: text;"></path><polyline points="14 2 14 8 20 8" style="user-select: text;"></polyline><line x1="16" y1="13" x2="8" y2="13" style="user-select: text;"></line><line x1="16" y1="17" x2="8" y2="17" style="user-select: text;"></line><polyline points="10 9 9 9 8 9" style="user-select: text;"></polyline></svg>
  <span>2394 words</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>12 min</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#yolo" aria-label="YOLO">YOLO</a><ul>
                        
                <li>
                    <a href="#how-it-works" aria-label="How it works">How it works</a></li>
                <li>
                    <a href="#training" aria-label="Training">Training</a></li>
                <li>
                    <a href="#shortcomings" aria-label="Shortcomings">Shortcomings</a></li></ul>
                </li>
                <li>
                    <a href="#yolov2" aria-label="YOLOv2">YOLOv2</a></li>
                <li>
                    <a href="#retinanet" aria-label="RetinaNet">RetinaNet</a><ul>
                        
                <li>
                    <a href="#focal-loss" aria-label="Focal Loss">Focal Loss</a></li>
                <li>
                    <a href="#feature-pyramid-network" aria-label="Feature Pyramid Network">Feature Pyramid Network</a></li></ul>
                </li>
                <li>
                    <a href="#yolov3" aria-label="YOLOv3">YOLOv3</a></li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">¶</a></h2>
<p align="justify">
In the previous <a href="https://johanngerberding.github.io/johannsblog/Object-Detection-From-R-CNN-to-Mask-RCNN">post</a> we have reviewed region-based object detection algorithms (R-CNN models). In the following post I will dive a bit deeper into fast one-stage detection models like YOLO and RetinaNet which are more suited for certain applications with real-time requirements. The models I'm going to talk about here are a bit outdated and don't necessarily correspond to the state-of-the-art in this area anymore. Nevertheless, I find the general development in this area very interesting and the algorithms presented here form the basis for the current state-of-the-art. At that time, two-stage detectors were usually ahead of single-stage detectors in terms of accuracy, which is no longer the case today. In my next post I will go into more detail about state-of-the-art models such as <a href="https://arxiv.org/pdf/1911.09070.pdf">EfficientDet</a> and <a href="https://arxiv.org/pdf/2004.10934.pdf">YOLOv4</a>.
</p>
<h2 id="yolo">YOLO<a hidden class="anchor" aria-hidden="true" href="#yolo">¶</a></h2>
<p align="justify">
As mentioned before, two stage detection models like Faster R-CNN are region based and considered to slow for certain applications that require real-time capabilities, e.g. in the robotics area or autonomous driving. So let's start with YOLO ("You Only Look Once") which was one of the first approaches to building a fast real-time object detector. Instead of relying on region proposals the authors reframed the object detection as a single regression problem, predicting bounding boxes and class probabilities directly from the images (therefore the name).
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/yolo-network-architecture.png#center"
         alt="YOLO model architecture" width="100%"/> <figcaption>
            <p>Figure 1. YOLO network architecture</p>
        </figcaption>
</figure>

<p align="justify">
This makes the whole system (Figure 1) fairly simple (single ConvNet) and very fast (45 fps). Since the model uses features from the entire image to predict the boxes it reasons globally.
</p>
<h3 id="how-it-works">How it works<a hidden class="anchor" aria-hidden="true" href="#how-it-works">¶</a></h3>
<p align="justify">
The input image gets divided into an $S \times S$ grid, where each grid cell predicts $B$ bounding boxes and confidence scores ($S=7$,  $B=2$). If the center of an object falls into a grid cell than this grid cell is "responsible" for the detection. Each bounding box consists of 5 predictions: <i>x_center</i>, <i>y_center</i>, <i>width</i>, <i>height</i> and <i>confidence</i>. The x and y coordinates are relative to the bounds of a grid cell. The width and height are relative to the image. So all predicted values are between 0 and 1. In addition each grid cell also predicts $C$ class probabilities which are conditional on the grid cell containing an object (for PascalVOC: $C=20$). These values encode the probabilities of that class appearing in the box and how well the predicted box fits the object. One of the main limitations of this approach is the fact that each grid cell can only contain one object (max: 49 objects per image).
</p>
<h3 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">¶</a></h3>
<p align="justify">
Now let's talk about training the YOLO model. First the convolutional layers get pretrained on ImageNet for a week with an image input size of 224x224. Thereafter to finetune the network on the detection task, four convolutional layers and two fully connected layers get added and the image size is increased to 448x448. As activation LeakyReLU is applied. The loss function is Sum-Squared Error (SSE) consisting of two parts: localization and classification loss:
</p>
<p>$$
L_{loc} = \lambda_{coord} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} [(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^{2} + (\sqrt{w_{i}} - \sqrt{\hat{w}_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}]
$$</p>
<p>$$
L_{cls} = \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} (\mathbb{1}_{ij}^{obj} \lambda_{noobj}(1 - \mathbb{1}_{ij}^{obj})) (C_{i} - \hat{C}_{i})^{2} + \sum_{i=0}^{S^{2}} \sum_{C=C} \mathbb{1}_{i}^{obj} (p_{i}(c) - \hat{p}_{i}(c))^{2}
$$</p>
<p>$$
L = L_{cls} + L_{loc}
$$</p>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathbb{1}_{i}^{obj}$</td>
<td>indicator of whether the cell $i$ contains an object</td>
</tr>
<tr>
<td>$\mathbb{1}_{ij}^{obj}$</td>
<td>ground-truth class label, $u = 1, &hellip;, K$ (background $u = 0$)</td>
</tr>
<tr>
<td>$C_{i}$</td>
<td>confidence score of cell $i$, $Pr(contains object) * IoU (pred, truth)$</td>
</tr>
<tr>
<td>$\hat{C}_{i}$</td>
<td>predicted confidence score (box with higher IoU of the two predicted boxes)</td>
</tr>
<tr>
<td>$\mathcal{C}$</td>
<td>set of all classes (Pascal VOC: 20)</td>
</tr>
<tr>
<td>$p_{i}$</td>
<td>conditional probability of whether cell $i$ contains an object of class $c \in \mathcal{C}$</td>
</tr>
<tr>
<td>$\hat{p}_{i}$</td>
<td>predicted conditional class probabilities</td>
</tr>
<tr>
<td>$S^{2}$</td>
<td>grid size, here $S=7$</td>
</tr>
<tr>
<td>$B$</td>
<td>number of predicted bounding boxes per grid cell</td>
</tr>
</tbody>
</table>
<p align="justify">
Because of model instability due to the inbalance between cells containing or not containing objects the authors use two scale parameters to increase the loss from bounding box predictions ($\lambda_{coord} = 0.5$) and decrease the loss from confidence predictions for boxes that don't contain objects ($\lambda_{noobj} = 0.5$). The loss function only penalizes classification error if an object is present in that grid cell and it only penalizes bbox error if the cell is "responsible" for the ground truth box.
</p>
<p align="justify">
Some more training details: The authors trained YOLO on VOC 2007 and VOC 2012 with a batch size of 64, momentum of 0.9 and a weight decay of 0.0005. For regularization they rely on dropout and data augmentation.
</p>
<h3 id="shortcomings">Shortcomings<a hidden class="anchor" aria-hidden="true" href="#shortcomings">¶</a></h3>
<ul>
<li>strong spatial constraints since we have only one prediction per grid cell (7x7 -&gt; max. 49 object predictions); this is one of the reasons why the model struggles with crowds of small objects</li>
<li>struggles to generalize to objects in new or unusual aspect ratios or configurations (maybe this could be reduced with clever data augmentation or training on different image scales)</li>
<li>many incorrect localizations due to an inappropriate loss function and coarse features for bounding box prediction (multiple downsampling layers)</li>
</ul>
<h2 id="yolov2">YOLOv2<a hidden class="anchor" aria-hidden="true" href="#yolov2">¶</a></h2>
<p align="justify">
YOLOv2 is basically an improved version of YOLO, adding some tricks to overcome its shortcomings described before. Moreover the paper covers YOLO9000 which is built on top of YOLOv2 and trained with a joint dataset combining COCO and the top 9000 classes of ImageNet (combination of detection and classification). I will only cover YOLOv2 here, for those of you who are interested in YOLO9000 and the joint training procedure, should take a look a the paper.
</p>
<p align="justify">
As mentioned before, the central goal of YOLOv2 was to fix the problems of YOLO, primarily recall and localization shortcomings. The authors did this based on a variety of new ideas in the field (at that time) and they try to avoid increasing the model size at the same time to preserve the high speed:
</p>
<p><strong>Batch Normalization:</strong> This leads to significant improvements in convergence while eliminating the need for other forms of regularization like dropout (+2% mAP).</p>
<p><strong>High Resolution Classifier:</strong> Finetune the classification network at higher resolution (448x448) for 10 epochs on ImageNet before detection finetuning.</p>
<p><strong>Convolutional Anchor Box Detection:</strong> The fully connected layers from YOLO are removed and instead YOLOv2 incorporates anchor boxes (like Faster R-CNN) to predict the bounding boxes; this also decouples the class prediction from the spatial location by predicting class and objectness for every anchor box which leads to a slight decrease in accuracy (-0.3% mAP) but increases recall significantly (+7%) which gives the model more room to improve.</p>
<p><strong>Box Dimension Clustering:</strong> Instead of using hand picked anchor box sizes, YOLOv2 runs k-means clustering on the training data to determine good priors for anchor box dimensions; to maximize IoU scores, it relies on the following distance metric:</p>
<p>$$
d(x, c_{i}) = 1 - IoU(x, c_{i}), \quad i=i,&hellip;k
$$</p>
<p>where $x$ is a ground truth box candidate and $c_{i}$ is one of the centroids / the closest centroid.</p>
<p><strong>Direct Location Prediction:</strong> In Region Proposal Networks the box location prediction is unconstrained which means any anchor box can end up at any point in the image which can lead to an unstable training. YOLOv2 follows the approach of the original YOLO model by predicting location coordinates relative to the location of the grid cell (using a logistic activation). Given the anchor box width $p_{w}$ and height $p_{h}$ in the grid cell with the top left corner ($c_{x}, c_{y}$) the model predicts 5 values ($t_{x}, t_{y}, t_{w}, t_{h}, t_{o}$) which correspond to the following box values:</p>
<p>$$
b_{x} = \sigma (t_{x}) + c_{x} \
b_{y} = \sigma (t_{y}) + c_{y} \
b_{w} = p_{w}e^{t_{w}} \
b_{h} = p_{h}e^{t_{h}} \
Pr(obj) =IoU(b, obj) = \sigma (t_{o})
$$</p>
<p>This in combination with clustering priors improves mAP by up to 5%.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/yolov2-loc.png#center"
         alt="YOLOv2 bounding box prediction format" width="60%"/> <figcaption>
            <p>Figure 2. YOLOv2 bounding box prediction</p>
        </figcaption>
</figure>

<p><strong>Fine-grained Features:</strong> The grid size of the final feature map of YOLOv2 is increased from 7x7 in YOLO to 13x13. Moreover YOLOv2 incorporates a so called passthrough layer that brings features from an earlier layer at 26x26 resolution to the output layer. This process can be compared with identity mappings from ResNets to incorporate higher dimensional features (+1% mAP).</p>
<p><strong>Multi-scale Training:</strong> To increase the robustness of the model the authors trained it on images of different sizes. Every 10 batches the input size gets randomly sampled (between 320x320 and 608x608).</p>
<p align="justify">
To maintain the high inference speed, YOLOv2 is based on the <b>Darknet-19</b> model, consisting of 19 convolutional and 5 max-pooling layers. For detailed information on the architecture check out Table 6 in the <a href="https://arxiv.org/pdf/1612.08242.pdf">paper</a>.
</p>
<h2 id="retinanet">RetinaNet<a hidden class="anchor" aria-hidden="true" href="#retinanet">¶</a></h2>
<p align="justify">
Next up in our list of fast detection models is RetinaNet. The creators had the goal of closing the accuracy gap between one and two-stage detection approaches. To achieve this, RetinaNet relies on two crucial building blocks, <b>Feature Pyramid Networks</b> (FPN) as a backbone and a new loss function called <b>Focal Loss</b>.
</p>
<h3 id="focal-loss">Focal Loss<a hidden class="anchor" aria-hidden="true" href="#focal-loss">¶</a></h3>
<p align="justify">
The central cause for the accuracy gap between the two approaches lies in the extreme foreground-background class imbalance during training. In two-stage detectors this problem is addressed by narrowing down the number of candidate object locations (filtering out many background samples) and by using sampling heuristics like a fixed foreground-to-background ratio or online hard example mining. The proposed Focal Loss is designed to address this issue for one-stage detectors by focusing on hard negatives and down-weighting the easier predictions (obvious empty background). It is based on the normal cross entropy loss (for simplicity we use binary loss down here)
</p>
<p>$$CE(p,y) = -y \log p - (1-y) \log (1 - p) $$</p>
<p align="justify">
where $y={0,1}$ is a ground truth binary label, indicating whether a bounding box contains an object and $p \in [0,1]$ is the predicted probability that there is an object (also called objectness or confidence score). For notational convenience, let
</p>
<p>$$
p_{t} =
\begin{cases}
p       &amp; \quad \text{if } y=1\
1-p     &amp; \quad \text{otherwise}
\end{cases}
$$</p>
<p>which leads to</p>
<p>$$
CE(p_{t}) = - \log p_{t}
$$</p>
<p align="justify">
Easily classified negatives ($p_{t} \gg 0.5 ,y=0$) comprise the majority of the loss. You can balance the importance of the positive/negative examples by adding a balancing factor $\alpha$
</p>
<p>$$
CE(p_{t}) = - \alpha_{t} \log p_{t}
$$</p>
<p align="justify">
but this does not differentiate between easy or hard examples. To overcome this the Focal Loss adds a modulating factor $(1-p_{t})^{\gamma}$ with a tunable focusing parameter $\gamma \geq 0$:
</p>
<p>$$
FL(p_{t}) = - (1 - p_{t})^{\gamma} \log (p_{t})
$$</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/focal-loss.png#center"
         alt="Focal Loss with different gamma values" width="70%"/> <figcaption>
            <p>Figure 3. Focal Loss with different gamma values</p>
        </figcaption>
</figure>

<p align="justify">
For better control of the shape of the weighting function the authors used an $\alpha$-balanced version in practice, where $\alpha = 0.25$ and $\gamma = 2$ worked best in their experiments:
</p>
<p>$$
FL(p_{t}) = - \alpha (1 - p_{t})^{\gamma} \log (p_{t})
$$</p>
<h3 id="feature-pyramid-network">Feature Pyramid Network<a hidden class="anchor" aria-hidden="true" href="#feature-pyramid-network">¶</a></h3>
<p align="justify">
The FPN backbone for RetinaNet was constructed on top of ResNet. To really understand what that means you should take a look at the <a href="https://arxiv.org/pdf/1612.03144.pdf">paper</a>. Figure 4 down below shows the fundamental idea of FPN which is to leverage a ConvNets pyramidal feature hierarchy to build a feature pyramid with high level semantics throughout. It is general purpose and can be applied to many convolutional backbone architectures.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/featurized-image-pyramid.png#center"
         alt="Featurized Pyramid Network architecture" width="100%"/> <figcaption>
            <p>Figure 4. Featurized Pyramid Network architecture</p>
        </figcaption>
</figure>

<p align="justify">
The basic structure contains a sequence of pyramid levels each corresponding to one network stage. Often these stages contain multiple conv layers of the same size and stage sizes are scaled down by a factor of two. $C_{i}$ represents the different layers of those stages (for ResNet e.g. {$C_{2}, C_{3}, C_{4}, C_{5}$}). As you can see there are two different pathways which connect the conv layers:
</p>
<ol>
<li><strong>Bottom-up</strong> pathway: regular feedback path</li>
<li><strong>Top-down</strong> pathway: goes in the opposite direction, adding coarse but semantically stronger feature maps back into the previous levels of layer size by lateral connections (1x1 conv to match dimensions) and nearest neighbor upsampling; the combination of the two maps is done by element-wise addition</li>
</ol>
<p align="justify">
The final predictions ({$P_{i}$} where $i$ indicates the pyramid level and has resolution $2^{i}$ lower than the input) are generated out of every merged map by a 3x3 conv layer. RetinaNet utilizes feature pyramid levels $P_{3}$ to $P_{7}$ computed from the corresponding ResNet residual stage from $C_{3}$ to $C_{5}$. All pyramid levels have 256 channels (most of RetinaNet is similar to FPN with a few minor differences). The authors used translation-invariant anchor boxes as priors, similar to those used in RPN variant of FPN. To improve Average Precision the number of anchors was increased to $A=9$ (three aspect ratios {1:2, 1:1, 2:1} with three different sizes {$2^{0}, 2^{1/3}, 2^{2/3}$}). As seen before, for each anchor box the model predicts a class probability for each of $K$ classes with a classification subnet trained with Focal Loss. A box regression subnet outputs the offsets for the boxes to the nearest ground truth object. Both networks are independent Fully Convolutional Networks that don't share any parameters.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/retina-net.png#center"
         alt="RetinaNet architecture" width="100%"/> <figcaption>
            <p>Figure 5. RetinaNet architecture</p>
        </figcaption>
</figure>

<h2 id="yolov3">YOLOv3<a hidden class="anchor" aria-hidden="true" href="#yolov3">¶</a></h2>
<p align="justify">
YOLOv3 was created by applying changes to YOLOv2 inspired by, at that time, recent advances in the object detection world. It's a pretty short and rather unscientifically (I like it :D ) written. The following list summarizes the most important improvements:
</p>
<ul>
<li><strong>Logistic Regression for objectness scores</strong> instead of sum of squared errors</li>
<li><strong>Independent Logistic Classifiers</strong> for class prediction instead of softmax which increases the performance on non mutually exclusive multilabel datasets like Open Images</li>
<li><strong>Multi-scale predictions</strong> inspired by FPN (3 scales per stage)</li>
<li><strong>Darknet-53 as Feature Extractor</strong> which performs similar to ResNet-152 but is 2x faster</li>
</ul>
<p align="justify">
Overall YOLOv3 performs better and faster than SSD, worse then RetinaNet but is 3.8x faster and comparable to state-of-the-art methods on the $AP_{50}$ metric at that time. In the appendix Joseph (the author) adds a cool comment on his opinion about the COCO evaluation metrics. It's refreshing to see someone questioning stuff like this.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/object_detection_2/yolov3-res.png#center"
         alt="YOLOv3 performance" width="80%"/> <figcaption>
            <p>Figure 6. YOLOv3 performance</p>
        </figcaption>
</figure>

<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">¶</a></h2>
<p align="justify">
In this blog post, we went over four popular but now somewhat aging fast object recognition systems and you got a first introduction to the world of real-time object recognition. In the next post I would like to talk about some more recent models like EfficientDet and YOLOv4.
</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">¶</a></h2>
<p><a href="https://arxiv.org/pdf/1506.02640.pdf">[1]</a> Joseph Redmon, et al. “You only look once: Unified, real-time object detection.” CVPR 2016.</p>
<p><a href="https://arxiv.org/pdf/1612.08242.pdf">[2]</a> Joseph Redmon and Ali Farhadi. “YOLO9000: Better, Faster, Stronger.” CVPR 2017.</p>
<p><a href="https://arxiv.org/pdf/1804.02767.pdf">[3]</a> Joseph Redmon, Ali Farhadi. “YOLOv3: An incremental improvement.”.</p>
<p><a href="https://arxiv.org/pdf/1612.03144.pdf">[4]</a> Tsung-Yi Lin, et al. “Feature Pyramid Networks for Object Detection.” CVPR 2017.</p>
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">[5]</a> Tsung-Yi Lin, et al. “Focal Loss for Dense Object Detection.” IEEE transactions on pattern analysis and machine intelligence, 2018.</p>


  </div>

  <footer class="post-footer">
  </footer>
    <div class="comments-separator"></div><div class="comments">
  <div class="title">
    <span>Comments</span>
    <span class="counter"><span class="remark42__counter" data-url="http://localhost:1313/posts/2021-05-02-object-detection-faster-models/"></span></span>
  </div>
  <div id="remark42">
  </div>
</div>

<script>
  var remark_config = {
    host: 'https:\/\/remark42.johannsblog.com',
    site_id: 'johanngerberding.github.io',
    components: ['embed', 'counter'],
    max_shown_comments: 20,
    theme: 'light',
    simple_view: true,
    admonition: JSON.stringify(['Please subscribe by email to receive reply notifications.']),
  };
  if (isDarkTheme()) {
    remark_config.theme = 'dark'
  }

  (function() {
    
    const key = 'remark42'
    if (!toggleThemeCallbacks.hasOwnProperty(key)) {
      toggleThemeCallbacks[key] = (isDark) => {
        const remark42 = window.REMARK42
        if (!remark42 || !document.querySelector('#remark42')) {
          return;
        }
        if (isDark) {
          remark42.changeTheme('light');
        } else {
          remark42.changeTheme('dark');
        }
      }
    }

    
    const remark42 = window.REMARK42
    if (remark42) {
      remark42.destroy()
      remark42.createInstance(remark_config)
    } else {
      for (const component of remark_config.components) {
        var d = document, s = d.createElement('script');
        s.src = `${remark_config.host}/web/${component}.mjs`;
        s.type = 'module';
        s.defer = true;
        
        s.setAttribute('data-no-instant', '')
        d.head.appendChild(s);
      }
    }
  })();
</script>

</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2024 <a href="http://localhost:1313/">Johanns Blog</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

</body>

</html>
