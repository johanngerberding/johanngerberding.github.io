<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Retrieval-Augmented Text Generation (RAG) | Johanns Blog</title>
<meta name="keywords" content="rag, transformer, llm, vector-database, retrieval">
<meta name="description" content="A short intro to Retrieval-Augmented Text Generation, what is it, why is it useful and how does it work.">
<meta name="author" content="Johann Gerberding">
<link rel="canonical" href="http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.a0264e6a2df146fcf73b7783950d40c4be7e8efa44c2889964f6ec37304880d9.css" integrity="sha256-oCZOai3xRvz3O3eDlQ1AxL5&#43;jvpEwoiZZPbsNzBIgNk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/">

<meta name="twitter:title" content="Retrieval-Augmented Text Generation (RAG) | Johanns Blog" />
<meta name="twitter:description" content="A short intro to Retrieval-Augmented Text Generation, what is it, why is it useful and how does it work." />
<meta property="og:title" content="Retrieval-Augmented Text Generation (RAG) | Johanns Blog" />
<meta property="og:description" content="A short intro to Retrieval-Augmented Text Generation, what is it, why is it useful and how does it work." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2024-04-23T10:00:00&#43;02:00" />
  <meta property="article:modified_time" content="2024-04-23T10:00:00&#43;02:00" /><meta property="og:site_name" content="Johann&#39;s Blog" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Retrieval-Augmented Text Generation (RAG)",
      "item": "http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Retrieval-Augmented Text Generation (RAG) | Johanns Blog",
  "name": "Retrieval-Augmented Text Generation (RAG)",
  "description": "A short intro to Retrieval-Augmented Text Generation, what is it, why is it useful and how does it work.",
  "keywords": [
    "rag", "transformer", "llm", "vector-database", "retrieval"
  ],
  "wordCount" : "3908",
  "inLanguage": "en",
  "datePublished": "2024-04-23T10:00:00+02:00",
  "dateModified": "2024-04-23T10:00:00+02:00",
  "author":{
    "@type": "Person",
    "name": "Johann Gerberding"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Johanns Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" dark type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'dark';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Johanns Blog (Alt + H)">Johanns Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="posts" class="active"
                ><i class='fa fa-heart'></i>posts
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags"
                ><i class='fa fa-heart'></i>tags
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/reading/" title="reading"
                ><i class='fa fa-heart'></i>reading
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about-me/" title="about"
                ><i class='fa fa-heart'></i>about
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Retrieval-Augmented Text Generation (RAG)<sup><span class="entry-isdraft">&nbsp;&nbsp;[draft]</span></sup></h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>April 23, 2024</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select: text;"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z" style="user-select: text;"></path><line x1="7" y1="7" x2="7" y2="7" style="user-select: text;"></line></svg>
  <span class="post-tags"><a href="http://localhost:1313/tags/rag/">Rag</a><a href="http://localhost:1313/tags/transformer/">Transformer</a><a href="http://localhost:1313/tags/llm/">Llm</a><a href="http://localhost:1313/tags/vector-database/">Vector-Database</a><a href="http://localhost:1313/tags/retrieval/">Retrieval</a></span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select: text;"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" style="user-select: text;"></path><polyline points="14 2 14 8 20 8" style="user-select: text;"></polyline><line x1="16" y1="13" x2="8" y2="13" style="user-select: text;"></line><line x1="16" y1="17" x2="8" y2="17" style="user-select: text;"></line><polyline points="10 9 9 9 8 9" style="user-select: text;"></polyline></svg>
  <span>3908 words</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>19 min</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#rag-framework" aria-label="RAG Framework">RAG Framework</a><ul>
                        
                <li>
                    <a href="#pre-retrieval" aria-label="Pre-Retrieval">Pre-Retrieval</a><ul>
                        
                <li>
                    <a href="#data-preparation--modification" aria-label="Data Preparation &amp;amp; Modification">Data Preparation &amp; Modification</a></li>
                <li>
                    <a href="#indexing" aria-label="Indexing">Indexing</a></li>
                <li>
                    <a href="#query-manipulation" aria-label="Query Manipulation">Query Manipulation</a></li></ul>
                </li>
                <li>
                    <a href="#retrieval" aria-label="Retrieval">Retrieval</a></li>
                <li>
                    <a href="#post-retrieval" aria-label="Post-Retrieval">Post-Retrieval</a><ul>
                        
                <li>
                    <a href="#re-ranking" aria-label="Re-Ranking">Re-Ranking</a></li>
                <li>
                    <a href="#filtering" aria-label="Filtering">Filtering</a></li></ul>
                </li>
                <li>
                    <a href="#generation" aria-label="Generation">Generation</a></li></ul>
                </li>
                <li>
                    <a href="#advanced-rag-pipelines" aria-label="Advanced RAG Pipelines">Advanced RAG Pipelines</a><ul>
                        
                <li>
                    <a href="#self-rag" aria-label="Self RAG">Self RAG</a></li>
                <li>
                    <a href="#corrective-rag" aria-label="Corrective RAG">Corrective RAG</a></li>
                <li>
                    <a href="#rag-fusion" aria-label="RAG Fusion">RAG Fusion</a></li>
                <li>
                    <a href="#self-reasoning" aria-label="Self-Reasoning">Self-Reasoning</a></li></ul>
                </li>
                <li>
                    <a href="#evaluation" aria-label="Evaluation">Evaluation</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">¶</a></h2>
<p align="justify">
In 2020, Lewis et al. introduced a novel approach to enhance language model outputs through the use of Retrieval-Augmented Generation (RAG). Since its inception, RAG has evolved significantly, providing a robust solution to some of the persistent challenges faced by Large Language Models (LLMs).
</p>
<p align="justify">
LLMs, while transformative, are not without their issues. They often produce "hallucinations" or convincing yet inaccurate answers, due to their sole reliance on the fixed dataset on which they were trained on. This limitation is compounded by their inability to incorporate information that emerges after their last training update, rendering them outdated as the world evolves. Additionally, their performance can be suboptimal in specialized areas because their training prioritizes breadth over depth to ensure broad accessibility and applicability.
</p>
<p align="justify">
RAG addresses these challenges by dynamically supplementing model responses with external data fetched in real time in response to queries. This approach not only keeps the information current but also enhances the accuracy and relevancy of the responses.
</p>
<p align="justify">
For a RAG system to deliver high-quality outputs, two core components are crucial. First, it must effectively retrieve the most relevant information from external sources. This involves a sophisticated understanding of the query context and the ability to sift through vast amounts of data quickly and efficiently. Second, it needs to generate accurate responses that integrate the retrieved information, ensuring that the final output is both coherent and contextually appropriate.
</p>
<p align="justify">
Building on these technologies, startups <a href="https://www.trychroma.com/">like Chroma</a>, <a href="https://weaviate.io/">Weaviate</a>, and <a href="https://www.pinecone.io/">Pinecone</a> have expanded upon RAG's foundational concepts by incorporating robust text storage solutions and advanced tooling into their platforms. These enhancements facilitate more efficient data handling and retrieval processes, which are critical for delivering precise and contextually relevant responses.
</p>
<p align="justify">
In the upcoming sections, we will dig deeper into the world of RAG by providing a comprehensive overview of the framework and highlighting the key components essential for an effective RAG system. Additionally, we will explore practical strategies that can enhance the quality and performance of your RAG system. While RAG offers significant benefits, it is not without its challenges. Therefore, we will also examine some of the common obstacles encountered in building RAG systems and discuss the current limitations that engineers need to be aware of as they build these kind of systems.
</p>
<h2 id="rag-framework">RAG Framework<a hidden class="anchor" aria-hidden="true" href="#rag-framework">¶</a></h2>
<p align="justify">
The basic workflow of a RAG system consists of three fundamental steps: indexing, retrieval, and generation. This sequence begins with indexing, where data is organized to facilitate quick and efficient access. The retrieval step can be further divided into three subparts: pre-retrieval, retrieval, and post-retrieval. Pre-retrieval involves preparing and setting up the necessary parameters for the search. Retrieval is the actual process of fetching the most relevant information based on the query. Post-retrieval includes the refinement and selection of the fetched data to ensure its relevance and accuracy. Finally, in the generation phase, the system synthesizes this information into coherent and contextually appropriate responses. These foundational steps remain at the core of every advanced RAG workflow, serving as the building blocks for more complex systems.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_framework.png#center"
         alt="Abstract Concept Graphic Retrieval-Augmented Generation Elements" width="100%"/> <figcaption>
            <p>Figure 1. RAG Framework Paradigm [1]</p>
        </figcaption>
</figure>

<p align="justify">
RAG presents a cost-effective alternative to the traditional, resource-intensive training and fine-tuning processes required for LLMs. Unlike standard LLMs that need model updates to incorporate new information, RAG dynamically integrates fresh data by retrieving relevant external content and concatenating it to the search query. This ensures flexibility and scalability to make it particularly valuable across a variety of use cases, like self-service applications where solutions get retrieved from an internal knowledge base and integrated into a helpful answer. Down below you can see the basic workflow that most RAG systems follow. 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_workflow.png#center"
         alt="Basic Retrieval-Augmented Generation Workflow" width="100%"/> <figcaption>
            <p>Figure 2. Basic RAG Workflow []</p>
        </figcaption>
</figure>

<p align="justify"> 
You start by generating document vectors using an embedding model for every document or text you want to include in your knowledge database. Embedding models (e.g. BERT, OpenAI text-embedding-ada-002) turn text into a vector representation, which is essential for the retrieval process by enabling similarity search. The documents you would want to retrieve can range from specific company guidelines or documents, a website to solution descriptions to known problems. Popular choices for such vector stores are <a href="https://github.com/facebookresearch/faiss">FAISS</a>, <a href="https://www.trychroma.com/">Chroma</a> or <a href="https://lancedb.com/">Lance</a>.  
</p>
<p align="justify"> 
After you have generated the vector store you are good to go. First your query gets embedded into a vector, like we have described above, which is then used in a similarity search over your vector store to determine the most relevant documents. These documents get retrieved and concatenated to your query before its fed into the LLM. The LLM then produces based on the query and context an answer. This describes the very basic and naive workflow. In practice there are a lot of other things involved, like prompt engineering, a limited context window, limited input tokens of the embedding model, preprocessing of the query and documents, postprocessing of the answer etc. We will touch most of these things in more detail in the next sections. 
</p>
<h3 id="pre-retrieval">Pre-Retrieval<a hidden class="anchor" aria-hidden="true" href="#pre-retrieval">¶</a></h3>
<p>The pre-retrieval phase is all about the data or the knowledge base. The goal is to ensure efficient and correct information retrieval.</p>
<h4 id="data-preparation--modification">Data Preparation &amp; Modification<a hidden class="anchor" aria-hidden="true" href="#data-preparation--modification">¶</a></h4>
<p align="justify">
If you have worked on an applied ML project with real customers than you are aware of the quality of company data. It would be pretty naive to assume all the information you want to have in your knowledge base is in an easy to read and parse format. Let's be real, most of the company knowledge is in Excel sheets and PowerPoint presentations or in pdf scans that have no pretty text layers. Most of the current RAG pipelines just work on text data, so you have to think about ways to transform everything into text. In the long run, I think you will have multimodal pipelines that can also work with images for example so you can embed e.g. a presentation as a sequence of images. Most of the data preparation phase is about OCR (which still is far from being solved), data cleaning and being creative about how to incorporate all the knowledge you have. You can use frameworks like <a href="https://github.com/Unstructured-IO/unstructured">unstructured</a> for parsing your data. This is really helpful for quickly getting a prototype up and running. But if you have ever seen business Excel sheets you might know why I think that those tools are definitely not good enough and we will need something that works on images.
</p>
<p align="justify">
Data Modification is another important component of the pre-retrieval phase for enhancing retrieval efficiency. It includes e.g. removing irrelevant or redundant information from your text to improve the quality of retrieval results or enriching the data with additional information such as metadata to boost relevance and diversity of the retrieved content. You could e.g. use visual models to describe images and diagrams in text form or use a LLM to extract metadata from documents.
</p>
<h4 id="indexing">Indexing<a hidden class="anchor" aria-hidden="true" href="#indexing">¶</a></h4>
<p align="justify">
After our data is clean and pretty we follow up with indexing which is dependent on the task and data type. In the classical workflow, documents/texts are split into chunks and then each chunk is embedded and indexed. The embeddings are created through the use of an embedding model like the one from <a href="https://docs.mistral.ai/capabilities/embeddings/">Mistral</a>. Such an embedding model turns your text into a feature vector that captures the semantic meaning of chunks. Unfortunately most of these embedding models are closed sourced but feel free to build your own.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/indexing.png#center"
         alt="Basic Chunking/Indexing Workflow" width="80%"/> <figcaption>
            <p>Figure 3. Basic Chunking/Indexing Workflow</p>
        </figcaption>
</figure>

<p align="justify">
As you might already know, LLMs have limited capacity to process text or more specifically tokens, which is often described as context length. E.g. GPT3.5 has a context length of 4,096 tokens which it can process at once. This roughly about 6 pages of english text after tokenization. By chunking our documents we first of all have to make sure, that the chunk size is smaller than the maximum context length of our LLM of choice. Nowadays there exist models like Llama3.1 that have extensive context lengths like 128K tokens. Some people might argue that RAG will become irrelevant in the long term because we can shove our whole knowledge base in-context but that's for the future. In most cases we just need small passages of text from a document and we want to combine it with information from other documents as well to answer our question. Determining the chunk size is a critical decision that not only influences the accuracy but also the efficiency (response generation time) of the retrieval and the overall RAG system. The goal is to choose the correct chunks of optimal lengths so that you can give the LLM as much relevant information as possible.</p>
<p align="justify">
There are a bunch of different possibilities of how to chunk you documents, e.g. page-level, sentence-level or you try to split by sections. You can imagine, based on the use case and data type, different strategies could be helpful here. There is no one size fits all approach and most of the time you have to do a bit of experimenting. E.g. Sentence-level indexing is beneficial for question-answering systems to precisely locate answers, while document-level indexing is more appropriate for summarizing documents to understand their main concepts and ideas. In some cases the retrieved data doesn't even have to be the same we used while indexing. For example you could use a LLM to generate questions that every chunk answers and then you retrieve based on similarity of questions. Or you could generate summaries for every chunk and index those which can reduce redundant information and irrelevant details (Figure 4).</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/advanced_indexing.png#center"
         alt="Advanced Chunking/Indexing Workflow" width="90%"/> <figcaption>
            <p>Figure 4. Advanced Chunking/Indexing Workflow</p>
        </figcaption>
</figure>

<p align="justify">
Both strategies shown above are just two examples of how to improve the chunk indexing and you can be creative here, based on you use case. Another interesting way of indexing is described in MemWalker paper <a href="#references">[6]</a>, where the documents are split into segments which are recursively summarized and combined to form a memory tree. To generate answers the tree is navigated down to the most important segments. The method outperforms various long context LLMs.
</p>
<h4 id="query-manipulation">Query Manipulation<a hidden class="anchor" aria-hidden="true" href="#query-manipulation">¶</a></h4>
<p align="justify">
After the indexing is done the last possibility to improve you retrieval pipeline is through query manipulation to adjust the user queries for a better match with the indexed data. Examples of this include:
</p>
<ul>
<li><b>Query Reformulation/Refinement</b>: Rewrite the query to align more closely with the user's 'real' intention (this is hard to do) or create multiple queries with the same intention.</li>
<li><b>Query Expansion</b>: Extend the query to capture more relevant results through e.g. including synonyms or related terms or identifying people/places/companies and add information regarding those.</li>
<li><b>Query Normalization</b>: Resolve differences in spelling or terminology for more consistent query matching.</li>
</ul>
<p align="justify">
This is another part of the pipeline where you can be creative and experiment with different approaches, combine them or modify them for your specific use case. Here is a short list of papers that implemented those strategies or combinations of them: Rewrite-Retrieve-Read <a href="#references">[11]</a>, FiD (Fusion-in-Decoder) <a href="#references">[7]</a>, CoK (Chain-of-Knowledge) <a href="#references">[8]</a>, Query2Doc <a href="#references">[10]</a>, Step-Back <a href="#references">[9]</a> or FLARE (Forward-Looking Active Retrieval) <a href="#references">[12]</a>.
</p>
<h3 id="retrieval">Retrieval<a hidden class="anchor" aria-hidden="true" href="#retrieval">¶</a></h3>
<p align="justify">
The retrieval process is a combination of search and ranking algorithms. The overall goal is to select and prioritize documents from a knowledge base to enhance the quality of the generation model outputs. The first thing you have to think about here is the type of search strategy you want to use. There are three main choices you can select from, vector search, keyword search or a hybrid approach. 
<p align="justify">
Since you have embedded all your knowledge into feature vectors it's kinda easy to just use cosine similarity and k-Nearest-Neighbors to identify the 'k' vectors that are most similar to the query. kNN is a very popular choice due to its simplicity and explainability. One shortcoming of this strategy is the computational intensity of this approach when you have a very large knowledge base. Instead of cosine similarity you can also use euclidean distance or the dot prodoct which can lead to better search results in some cases.
</p>
- BM25 is the most popular keyword retrieval function that is based on bag-of-words
- extensions: BM25F which adds different weights to fields of a document and BM25+ which addresses a problem with the unfair scoring of longer documents
<p align="justify">
</p>
<ul>
<li>what do you do when you have your search results?</li>
<li>reciprocal rank fusion for hybrid search?</li>
</ul>
<p align="justify">
</p>
<h3 id="post-retrieval">Post-Retrieval<a hidden class="anchor" aria-hidden="true" href="#post-retrieval">¶</a></h3>
<p align="justify">
</p>
<h4 id="re-ranking">Re-Ranking<a hidden class="anchor" aria-hidden="true" href="#re-ranking">¶</a></h4>
<p align="justify">
</p>
<h4 id="filtering">Filtering<a hidden class="anchor" aria-hidden="true" href="#filtering">¶</a></h4>
<p align="justify">
</p>
<h3 id="generation">Generation<a hidden class="anchor" aria-hidden="true" href="#generation">¶</a></h3>
<p align="justify">
- task: generate text that is relevant to the query and reflective of the information found in the retrieved documents
- how: concat the query with the found information and input that into a LLM
- challenge: ensuring the outputs alignment and accuracy with the retrieved contents isn't straightforward 
- the generated output should accurately convey the information from the retrieved documents and align with the query's intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data 
- another challenge: what if there is no relevant information in your knowledge base for a certain query, how to detect such instances and how to behave? 
- retrieval is kinda naive, how would you filter irrelevant documents? based on some distance metric, but how to define that distance? 
</p>
<h2 id="advanced-rag-pipelines">Advanced RAG Pipelines<a hidden class="anchor" aria-hidden="true" href="#advanced-rag-pipelines">¶</a></h2>
<p align="justify">
The naive RAG implementation described before is rarely enough to satisfy production grade requirements. This has multiple reasons:
</p>  
<ul>
<li><b>Question ambiguity</b>: user questions are not well defined and may lead to irrelevant retrieval results </li>
<li><b>Low retrieval accuracy</b>: retrieved documents may not be equally relevant to the question </li>
<li><b>Limited knowledge</b>: the knowledge base may not include the information the user is looking for </li>
<li><b>Context window performance limitations</b>: trying to "over-retrieve" may hit on the capacity of the context window or otherwise produce a context window that is too big to return a result in a reasonable amount of time </li>
</ul>
<p align="justify">
Many new RAG patterns have emerged to address these limitations. In the following I will go over some of those techniques, but it is important to note that there is no silver bullet. Each one of these methods may still produce poor results in certain situations or isn't well fitted for your specific use case.
</p>
<h3 id="self-rag">Self RAG<a hidden class="anchor" aria-hidden="true" href="#self-rag">¶</a></h3>
<p align="justify">
The Self-Reflective RAG paper describes fine-tuned models that incorporate mechanisms for adaptive information retrieval and self critique. These models can dynamically determine when external information is needed, and can critically evaluate its generated responses for relevance and factual accuracy. Figure X down below shows the retrieval process using these new tokens.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/self_rag.png#center"
         alt="Flowchart of the Self RAG method" width="100%"/> <figcaption>
            <p>Figure X. Self RAG Process [3]</p>
        </figcaption>
</figure>

<p align="justify">
First the Language Model decides about the necessity of additional information to answer the prompt. In the second step multiple segments are generated concurrently and each of them is rated in terms of relevance to the prompt and usefulness of the retrieved information. Thereafter each segments is critiqued and the best one is chosen. Then the cycle can repeat, if the model decides to retrieve more information. The table down below shows the four new tokens that make this whole process work.
</p>
<table>
    <tr>
        <th>Name</th>
        <th>Description</th>
        <th>Output</th>
</tr>
    <tr>
        <td>Retrieval Token</td>
        <td>Retrieve additional information from knowledge base</td>
        <td>yes, no</td>
    </tr>
    <tr>
        <td>Relevance Token</td>
        <td>Retrieved information is relevant to the prompt</td>
        <td>relevant, irrelevant</td>
    </tr>
    <tr>
        <td>Support Token</td>
        <td>Generated information is supported by the provided context</td>
        <td>fully supported, partially supported, no support</td>
    </tr>
    <tr>
        <td>Critique Token</td>
        <td>Useful response to the prompt</td>
        <td>5, 4, 3, 2, 1</td>
    </tr>
</table> 
<p align="justify">
- Why is all this a good idea? (Benchmarks) 
- What are challenges?  
For more detailed information regarding training, how they inserted these special tokens and data collection I'll recommend checking out the <a href="https://arxiv.org/pdf/2310.11511">paper</a>.
</p>
<h3 id="corrective-rag">Corrective RAG<a hidden class="anchor" aria-hidden="true" href="#corrective-rag">¶</a></h3>
<p align="justify">
The Corrective Retrieval Augmented Generation method to improve the accuracy and robustness of LM by re-incorporating information from retrieved documents. This is accomplished by the introduction of three additional components. First a lightweight evaluator model that assesses the quality of retrieved documents and determines which to use, ignore or request more documents based on a score. The second new component is an incorporated web search to extend the information base and get more up-to-date information. The last new addition is a decompose-then-recompose algorithm for selectively focusing on key information and filtering out irrelevant information that also makes use of the evaluator model. The Figure X down below gives a nice overview of the CRAG method at inference.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/crag.png#center"
         alt="An overview of Corrective Retrieval Augmented Generation" width="100%"/> <figcaption>
            <p>Figure X. An overview of CRAG at inference [4]</p>
        </figcaption>
</figure>

<p align="justify">
The Knowledge Refinement process is quite straightforward and starts by segmenting each retrieved document into fine-grained knowledge strips through heuristic rules. Then the retrieval evaluator is used to calculate a relevance score of each knowledge strip. Based on these scores the strips are filtered and recomposed via concatenation in order.
</p>
<h3 id="rag-fusion">RAG Fusion<a hidden class="anchor" aria-hidden="true" href="#rag-fusion">¶</a></h3>
<ul>
<li>research from a guy from Infineon</li>
</ul>
<p align="justify">
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_fusion.png#center"
         alt="RAG Fusion" width="100%"/> <figcaption>
            <p>Figure X. RAG Fusion [24]</p>
        </figcaption>
</figure>

<h3 id="self-reasoning">Self-Reasoning<a hidden class="anchor" aria-hidden="true" href="#self-reasoning">¶</a></h3>
<p align="justify">
The method described in this paper focuses on the problem of irrelevant document retrieval which may result in unhelpful response generation or even performance deterioration. It is similar to the Self-RAG without the need of extra models and datasets. The authors propose an end-to-end self-reasoning framework to improve on these problems and especially improve reliability and traceability. 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/self-reasoning.png#center"
         alt="Self-Reasoning Framework to Improve RAG" width="100%"/> <figcaption>
            <p>Figure X. Self-Reasoning Framework to Improve RAG [5]</p>
        </figcaption>
</figure>

<p align="justify">
As shown above the framework consists of three processes, the Relevance-Aware Process, the Evidence-Aware Selective Process and the Trajectory Analysis Process.
</p>
<p align="justify">
<b>Relevance-Aware Process</b>: Instruct the model to judge the relevance between the retrieved documents $\mathcal{D}$ and the given question $\mathcal{q}$. In addition the model is requested to generate reasons explaining the relevance of each document. The self-reasoning trajectories are defined as $\mathcal{\tau_{r}}$. If none of the documents is classified as relevant, the answer should be generated by the basic LLM. 
</p>
<p align="justify">
<b>Evidence-Aware Selective Process</b>: Instruct the model to choose relevant documents and select snippets of key sentences for the selected documents. Then the model has to generate the reason why the selected snippets can answer the question, so that you end up with a list containing the cited content and the reason for the cite. The self-reasoning trajectories generated in this process are defined as $\mathcal{\tau_{e}}$.</p>
<p align="justify">
<b>Trajectory Analysis Process</b>: Here the self-reasoning trajectories ($\mathcal{\tau_{r}}$ & $\mathcal{\tau_{e}}$) are consolidated to form a chain of reasoning snippets. With these snippets as context the LLM is instructed to output content with two fields: analysis (long-form answer) and answer (short-form). 
</p>
<p align="justify">
Regarding the training process the authors propose a stage-wise training because of problems with the generation of long reasoning trajectories. In the first stage only the first two stage trajectories are masked, in the second stage only the trajectories from the third process are masked. Finally all trajectories are concatenated and trained end-to-end. For more details on this procedure, check out the paper.</p>
<h2 id="evaluation">Evaluation<a hidden class="anchor" aria-hidden="true" href="#evaluation">¶</a></h2>
<p align="justify">
Now you have built you fancy RAG engine, but how do you determine if this thing is any good? Is it even better as a vanilla LLM or is it even worse? Since RAG systems are comprised of multiple components and can get kinda complicated as we have seen above, you can evaluate single components individually or just the performance of the whole thing on the downstream task. Because of the AI hype and the push of LLMs into prodcution systems, this has become an important area of research. 
</p>
<p>Datasets for measuring performance on downstream tasks like you would assess a regular LLM using established metrics like F1 or Exact Match (EM):</p>
<ul>
<li>TriviaQA <a href="#references">[17]</a></li>
<li>HotpotQA <a href="#references">[18]</a></li>
<li>FEVER <a href="#references">[19]</a></li>
<li>Natural Questions <a href="#references">[20]</a></li>    
<li>Wizard of Wikipedia <a href="#references">[21]</a></li>
<li>T-REX <a href="#references">[22]</a></li>
</ul>
<p align="justify">
For the further development of the methods we need a more finegrained evaluation for identifying the components of the pipeline that are the weak points and we need to be able to analyze those weaknesses in more detail. Various frameworks were introduced in the recent past to evaluate RAG pipelines across multiple dimensions, including the relevance of the retrieved documents, the quality of the generated text and the resilience to misinformation. The following table from <a href="#references">[1]</a> shows more details about the different frameworks, their methods and metrics as well as the datasets they use. 
</p>
<table>
<thead>
<tr>
<th>Framework</th>
<th>Aspects</th>
<th>Methods</th>
<th>Metrics</th>
<th>Datasets</th>
</tr>
</thead>
<tbody>
<tr>
<td>RAGAS <a href="#references">[13]</a></td>
<td>Quality of RAG Systems</td>
<td>Context Relevance, Answer Relevance, Faithfulness</td>
<td>Extracted Sentences / Total Sentences, Average Cosine Similarity, Supported Statements / Total Statements</td>
<td>WikiEval</td>
</tr>
<tr>
<td>ARES <a href="#references">[14]</a></td>
<td>Improving RAGAS</td>
<td>Context Relevance, Answer Relevance, Answer Faithfulness</td>
<td>Confidence Intervals</td>
<td>KILT, SuperGLUE</td>
</tr>
<tr>
<td>RECALL <a href="#$references">[15]</a></td>
<td>Counterfactual Robustness</td>
<td>Response Quality, Robustness</td>
<td>Accuracy (QA), BLEU, ROGUE-L</td>
<td>EventKG, UJ</td>
</tr>
<tr>
<td>RGB <a href="#references">[16]</a></td>
<td>Impact of RAG on LLMs</td>
<td>Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness</td>
<td>Accuracy, Rejection Rate, Error Detection Rate, Error Correction Rate</td>
<td>Synthetic Dataset including English and Chinese</td>
</tr>
</tbody>
</table>
<p align="justify">
Let's take a closer look at the evaluation of the two components:
</p>
<p align="justify">
<b>Evaluate Retrieval</b>: The quality of the search results is most of the time evaluated based on standard metrics like Mean Average Precision (MAP), Precision, Reciprocal Rank, Recall and Normalized Discounted Cumulative Gain (NDCG) which primarily assess the relevance of retrieved documents regarding a given query. For some explanations on those take a look <a href="https://www.deepset.ai/blog/rag-evaluation-retrieval">here</a>. The Rejection Rate measures a systems ability to decline answering when no relevant information is found. In addition the Error Detection Rate assesses the models capability to identify and disregard incorrect or misleading information from retrieved documents. Another important metric is the Context Relevance that ensures the perinence of the retrieved documents so that the information used to generate responses is directly related to the querys context. Faithfulness measures the accuracy with which the generated content reflects the information in the retrieved documents.
</p>
<p align="justify">
<b>Evaluate Generation</b>: This part is more common and there are a lot of different standard metrics based on the downstream task. Those metrics assess linguistic quality and coherence (BLEU score, ROUGE-L), accuracy and the extent to which the generated text reflects the ground truth data (EM, F1 score). Beyond the standard metrics, the evaluation can incorporate task-specific criteria and novel metrics tailored to particular applications. In dialog generation perplexity and entropy are used to evaluate response diversity and naturalness. Based on your specific use case, you can get creative here.
</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">¶</a></h2>
<p><a name="references"></a></p>
<p><a href="https://arxiv.org/pdf/2404.10981">[1]</a> Y. Huang &amp; J.X. Huang &ldquo;A Survey on Retrieval-Augmented Text Generation for Large Language Models&rdquo; (2024).</p>
<p><a href="https://www.pinecone.io/learn/advanced-rag-techniques/">[2]</a> Roie Schwaber-Cohen &ldquo;Advanced RAG Techniques&rdquo; (2024).</p>
<p><a href="https://arxiv.org/pdf/2310.11511">[3]</a> Asai et al. &ldquo;Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2401.15884">[4]</a> Yan et al. &ldquo;Corrective Retrieval Augmented Generation&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2407.19813">[5]</a> Xia et al. &ldquo;Improving Retrieval Augmented Language Model with Self-Reasoning&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2310.05029">[6]</a> Chen et al. &ldquo;Walking Down the Memory Maze: Beyond Context Limit Through Interactive Reading&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2007.01282">[7]</a> G. Izacard &amp; E. Grave &ldquo;Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering&rdquo; (2020)</p>
<p><a href="https://arxiv.org/pdf/2305.13269">[8]</a> Xingxuan et al. &ldquo;Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2310.06117">[9]</a> Zheng et al. &ldquo;Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2303.07678">[10]</a> Wang et al. &ldquo;Query2doc: Query Expansion with Large Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.14283">[11]</a> Ma et al. &ldquo;Query Rewriting for Retrieval-Augmented Large Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.06983">[12]</a> Jiang et al. &ldquo;Active Retrieval Augmented Generation&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2309.15217">[13]</a> Shahul et al. &ldquo;RAGAS: Automated Evaluation of Retrieval Augmented Generation&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2311.09476">[14]</a> Saad-Falcon et al. &ldquo;ARES: An Automated Evaluation Framework for Retrieval Augmented Generation Systems&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2311.08147">[15]</a> Liu et al. &ldquo;RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2309.01431">[16]</a> Chen et al. &ldquo;Benchmarking Large Language Models in Retrieval-Augmented Generation&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/1705.03551">[17]</a> Joshi el al. &ldquo;TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension&rdquo; (2017)</p>
<p><a href="https://arxiv.org/pdf/1809.09600">[18]</a> Yang et al. &ldquo;HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering&rdquo; (2018)</p>
<p><a href="https://arxiv.org/pdf/1803.05355">[19]</a> Thorne et al. &ldquo;FEVER: A large-scale dataset for Fact Extraction and VERification&rdquo; (2018)</p>
<p><a href="https://aclanthology.org/Q19-1026.pdf">[20]</a> Kwiatowski el al. &ldquo;Natural Questions: A Benchmark for Question Answering Research&rdquo; (2019)</p>
<p><a href="https://arxiv.org/pdf/1811.01241">[21]</a> Dinan et al. &ldquo;Wizard of Wikipedia: Knowledge Powered Conversational Agents&rdquo; (2019)</p>
<p><a href="https://aclanthology.org/L18-1544.pdf">[22]</a> ElSahar et al. &ldquo;T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples&rdquo; (2018)</p>
<p><a href="https://arxiv.org/pdf/2402.03367">[23]</a> Z. Rackauckas &ldquo;RAG-Fusion: a New Take on Retrieval Augmented Generation&rdquo; (2024)</p>


  </div>

  <footer class="post-footer">
  </footer>
    <div class="comments-separator"></div><div class="comments">
  <div class="title">
    <span>Comments</span>
    <span class="counter"><span class="remark42__counter" data-url="http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/"></span></span>
  </div>
  <div id="remark42">
  </div>
</div>

<script>
  var remark_config = {
    host: 'https:\/\/remark42.johannsblog.com',
    site_id: 'johanngerberding.github.io',
    components: ['embed', 'counter'],
    max_shown_comments: 20,
    theme: 'light',
    simple_view: true,
    admonition: JSON.stringify(['Please subscribe by email to receive reply notifications.']),
  };
  if (isDarkTheme()) {
    remark_config.theme = 'dark'
  }

  (function() {
    
    const key = 'remark42'
    if (!toggleThemeCallbacks.hasOwnProperty(key)) {
      toggleThemeCallbacks[key] = (isDark) => {
        const remark42 = window.REMARK42
        if (!remark42 || !document.querySelector('#remark42')) {
          return;
        }
        if (isDark) {
          remark42.changeTheme('light');
        } else {
          remark42.changeTheme('dark');
        }
      }
    }

    
    const remark42 = window.REMARK42
    if (remark42) {
      remark42.destroy()
      remark42.createInstance(remark_config)
    } else {
      for (const component of remark_config.components) {
        var d = document, s = d.createElement('script');
        s.src = `${remark_config.host}/web/${component}.mjs`;
        s.type = 'module';
        s.defer = true;
        
        s.setAttribute('data-no-instant', '')
        d.head.appendChild(s);
      }
    }
  })();
</script>

</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2024 <a href="http://localhost:1313/">Johanns Blog</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

</body>

</html>
