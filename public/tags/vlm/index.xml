<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Vlm on Johanns Blog</title>
    <link>http://localhost:1313/tags/vlm/</link>
    <description>Recent content in Vlm on Johanns Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Aug 2024 10:00:00 +0200</lastBuildDate><atom:link href="http://localhost:1313/tags/vlm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Vision Language Models</title>
      <link>http://localhost:1313/posts/2024-08-23-vision-language-models/</link>
      <pubDate>Fri, 23 Aug 2024 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2024-08-23-vision-language-models/</guid>
      <description>LLMs are boring but VLMs are awesome, let&amp;rsquo;s see why.</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
Vision Language Models (VLM) are multimodal models that can learn from text and images and generate text as an output (some are able to generate images too). Typical use cases for this range from image chatting, image recognition, visual question answering to image captioning, document understanding and more. In addition to that some models are also able to perform object detection, segmentation or reasoning about relative positions of objects (which is kinda fire).
</p>
<p align="justify">
The focus of this post lies on open source models just because we have information about their architecture, datasets and training processes. VLMs currently are far from being perfect, there exist a lot of open questions, challenges when building them and problems that have to be addressed e.g.:
</p>
<ul>
<li>bad understanding of spatial relationships</li>
<li>bad at counting (without complicated engineering overhead that relies on additional data annotation or other hacks)</li>
<li>lack understanding of attributes and ordering</li>
<li>ignorance of parts of the input prompt (need for a lot of prompt engineering to produce the results you want)</li>
<li>hallucinations (like in LLMs)</li>
</ul> 
<p align="justify">
In the following I will categorize them by their training paradigm like in [2] and will go over some prominent examples. There exist way to many models to cover them all so if this overview here isn't enough and you want more information check out <a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&sort=trending">huggingface models</a>, this cool <a href="https://huggingface.co/collections/merve/vision-language-models-papers-66264531f7152ac0ec80ceca">paper collection</a> or <a href="https://paperswithcode.com/">paperswithcode</a>. 
</p>
<h2 id="families-of-vlms">Families of VLMs</h2>
<p align="justify">
One way to categorize VLMs is based on the training paradigm like in [2]:
</p>
<ul>
<li><b>contrastive</b>: Leverage pairs of positive and negative examples.</li>
<li><b>masking</b>: Leverage reconstruction of masked image patches given some unmasked text.</li>
<li><b>pretrained backbones</b>: Combine a pretrained LLM with a pretrained image encoder and then learn a mapping between those two.</li>
<li><b>generative</b>: Generate captions and images.</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/families_of_vlms.png#center"
         alt="Families of Vision Language Models" width="90%"/> <figcaption>
            <p>Figure 1. Families of VLMs [2]</p>
        </figcaption>
</figure>

<p align="justify">
The paradigms are not mutually exclusive and many approaches we explore in this post rely on a mix of those training strategies. In the following we will describe some approaches for each paradigm.
</p>
<h3 id="contrastive-based-methods">Contrastive-based Methods</h3>
<p align="justify">
In this section I am presenting two contrastive-based VLMs, the very popular CLIP model from OpenAI and one of the successor models from Meta called Llip.
</p>
<h4 id="clip">CLIP</h4>
<p align="justify">
CLIP (<b>C</b>ontrastive <b>L</b>anguage <b>I</b>mage <b>P</b>re-training) was one of those models created by OpenAI which were really open (you know back in the days OpenAI was really open and cool). The pre-training task here was not to predict the exact caption for each image but to predict which whole caption to pair with a certain image. This switch from a predictive objective (so the classic ML approach with labels) to a contrastive one lead to a 4x efficiency improvement. To train this model the authors leveraged captions from the internet and collected a huge dataset of 400 million image-text-pairs which they called WIT for WebImageText. Another advantage of prediction captions instead of e.g. classes was a better and more flexible zero-shot transfer capability of the model. The figure down below gives a good overview of the approach.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/clip.png#center"
         alt="CLIP approach overview" width="100%"/> <figcaption>
            <p>Figure 2. CLIP approach overview [3]</p>
        </figcaption>
</figure>

<p align="justify">
The CLIP model is trained using a batch of $N$ image-text pairs. The training objective is to predict which of the $NÃ—N$ possible image-text pairings within the batch actually occurred. To achieve this, CLIP learns a multimodal embedding space by jointly training an image encoder and a text encoder. The goal is to maximize the cosine similarity between the image and text embeddings of the $N$ correct (real) pairs in the batch while minimizing the cosine similarity for the incorrect pairings. For optimization, a symmetric cross-entropy loss, also known as InfoNCE loss, is applied to the similarity scores. The following pseudocode outlines this procedure.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/clip_code.png#center"
         alt="CLIP training pseudocode" width="60%"/> <figcaption>
            <p>Figure 3. CLIP training pseudocode [3]</p>
        </figcaption>
</figure>

<p align="justify">
As the image encoder, the authors trained 5 ResNet and 3 ViT versions and found the ViT-L/14@336px version to perform the best. The text encoder is a 63M parameter Transformer (12 layers) with a vocab size of ~49K. The model showed strong zero-shot performance on ImageNet classification task (same as the original ResNet50). Some of the limitations of CLIP were e.g. that the zero-shot performance on more finegrained vision tasks was quite bad (like differenciating between different car models), on some tasks it was random (like counting objects) and it was not as good as you would expect on simple out of distribution tasks like MNIST (just 88% accuracy).  
</p>
<h4 id="llip">Llip</h4>
<p align="justify">
One of the problems with CLIP was, that there are a thousand ways to caption an image, based on the fact that the caption could describe only specific regions of an image or specific objects. To better model the visual richness of an image, a training objective of a vision language model should aim to capture all the possible text descriptions. This is what the authors of Llip, <b>L</b>atent <b>L</b>anguage <b>I</b>mage <b>P</b>retraining, try to do. To enable the prediction of different representations from a fixed image, they implemented the image-to-text representation function as a one-to-many mapping. This is achieved by augmenting the visual encoder with a latent variable that captures context information. The contextual latent is inferred from the caption and used to modulate the representation. The visual encoder is implemented as a Vision Transformer that outputs $K$ learnable mixture tokens in addition to the visual tokens. These mixture tokens should capture different visual aspects of an image. Figure 4 down below shows this simple modification of CLIP.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/clip_vs_llip.png#center"
         alt="CLIP vs. Llip" width="80%"/> <figcaption>
            <p>Figure 4. CLIP vs. Llip [4]</p>
        </figcaption>
</figure>

<p align="justify">
The authors added a cross-attention mechanism to infer the mixture token weights as a function of the text caption. The weighted mixture defines the contextual representation that is contrasted with text representations. This leads to significant improvement of the visual representation quality as well as a more rich visual representation.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/llip_cross_attention.png#center"
         alt="Llip Cross Attention mechanism" width="90%"/> <figcaption>
            <p>Figure 5. Llip cross-attention mechanism [4]</p>
        </figcaption>
</figure>

<p align="justify">
On zero-shot transfer classification, Llip consistently outperforms CLIP pretraining for architecture of similar size on a large set of benchmarks. Especially on zero-shot image-text and text-image retrieval, Llip consistently outperforms CLIP pretraining on COCO by 6.0% image-to-text retrieval.
</p>
<h3 id="vlms-with-masking-objectives">VLMs with Masking Objectives</h3>
<p align="justify">
Masking is a commonly used technique in deep learning research. It can be viewed as a specific form of denoising autoencoder in which the noise has a spatial structure. In 2019 the authors of the BERT paper used Masked-Language-Modeling (MLM) to predict missing text tokens in a sentence. More recently the same concept (Masked-Image-Modeling) was used in the vision space to learn strong visual representations like in I-JEPA. In the following we are going through two approaches that combined those techniques to train a VLM, FLAVA [5] and MaskVLM [6].
</p>
<h4 id="flava">FLAVA</h4>
<p align="justify">
Contrastive methods like CLIP aren't easy usable on multimodal problems that require dealing with both modalities at the same time. Many of the more recent models that rely on early fusion and shared self-attention across modalities often perform very bad on vision-only or language-only tasks. The goal of the authors was to create a single "foundation" model that is good at vision tasks, language tasks and cross- and multi-modal tasks. FLAVA consists of three models, an image encoder, a text encoder and a multimodal encoder that takes as input the encoded image and text and integrates their represenations for multimodal reasoning.
</p>
<p align="justify">
The image encoder is a ViT-B/16 model with a fixed image size. It outputs is a list of hidden state vectors $\{h_{I}\}$, each corresponding to an image patch, and a classification token $h_{CLS,I}$. The text encoder has the same architecture as the vision part and outputs a hidden state vector $\{h_{T}\}$ and a text classification token $h_{CLS,T}$. The multimodal encoder also is a transformer model that fuses image and text hidden states. Over each of the hidden state vectors generated by the image and text encoder two learned linear projections are applied and an additional $[CLS_M]$ token is added before feeding this into the multimodal encoder. Like the text and image encoders, the multimodal encoder also outputs a list of hidden state vectors $\{h_{M}\}$ and a vector $h_{CLS,M}$ for the classification token. 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/flava_overview.png#center"
         alt="Overview of the FLAVA model architecture" width="100%"/> <figcaption>
            <p>Figure 6. Overview of the FLAVA model architecture [5]</p>
        </figcaption>
</figure>

<p align="justify">
The training process consists of a joint pretraining on both unimodal and multimodal data. First the authors pretrained the text encoder, then the image encoder and then they used both pretrained encoders to train the multimodal encoder. 
For the unimodal pretraining they used established loss components. For the image encoder they used a masked image modeling (MIM) objective like in BEiT. First they tokenize the image patches with a dVAE tokenizer, then they used a classifier on top of the image encoder outputs to predict the missing tokens. Masked language modeling (MLM) was used for the text encoder as an objective with 15% of text tokens masked. For the multimodal pretraining three loss components were used:
</p>
<ul>
<li><b>Global contrastive</b> (GC) loss, like in CLIP</li>
<li><b>Masked Multimodal Modeling</b> (MMM) loss, masking of both the image patches and text tokens</li>
<li><b>Image Text Matching</b> (ITM) loss, by applying a classifier on top of the multimodal encoder to decide if the input image and text match to each other</li>
</ul>
<p align="justify">
One of the cool things of this paper is, that the authors only used public unimodal and multimodal datasets for training, 70 million image-text pairs in total (but the avg. text length was just 12 words). The validated FLAVA on 35 tasks across vision, NLP and multimodal domains and performed better or competitive on all of those tasks with the state of the art models at that time which were mostly trained on much larger and probably cleaner datasets. 
</p>
<h4 id="maskvlm">MaskVLM</h4>
<p align="justify">
Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, the authors propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. This works especially well in scenarios with limited data. Figure 7 illustrates the difference between this new paradigm and the classic MIM and MLM based approaches.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/maskvlm_idea.png#center"
         alt="Left MIM and MLM and right the MaskVLM idea" width="85%"/> <figcaption>
            <p>Figure 7. Left: MIM &amp; MLM; Right: Masked Vision Language Modeling [6]</p>
        </figcaption>
</figure>

<p align="justify">
There are two main types of pre-training objectives in this model. The first is masked vision and language modeling. Here, transformer-based models are used as image and text encoders to extract features from both modalities. These features are then processed by image and text cross-modality encoders, which consist of three cross-attention blocks. These blocks allow the text and image features to interact, enhancing the representation of each by leveraging information from the other. The second objective is multimodal alignment, which includes image-text contrastive learning (ITC) and image-text matching (ITM). These methods align the representations of images and text to ensure they correspond accurately. For more detailed information, you can refer to the original paper.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/maskvlm_architecture.png#center"
         alt="MaskVLM model architecture" width="90%"/> <figcaption>
            <p>Figure 8. Overview of the MaskVLM model architecture [6]</p>
        </figcaption>
</figure>

<p align="justify">
MaskVLM is very data efficient, it especially shines in limited data scenarios where only âˆ¼40% of data used by the state-of-the-art models is sufficient to match their performance.
</p>
<h3 id="generative-based-vlms">Generative-based VLMs</h3>
<p align="justify">
In contrast to the paradigms above, that mostly operate on latent representations we will now look at generative VLM that are trained to generate text and images. We are looking at two methods in more detail, <b>CoCa</b> which learns to generate text and <b>Chameleon</b> which is a multimodal generative model that can generate text and images. Before we delve deeper I will list some of the advantages of generative classifiers and why this training paradigm can be a good idea: 
</p>
<ul>
<li>more effective robustness which means better out-of-distribution performance</li>
<li>better on compositional reasoning tasks than discriminative methods like CLIP</li> 
<li>more shape bias and better alignment with human judgement</li>
<li>can be jointly adapted with discriminative models at test time using only unlabeled test samples which improves classification, segmentation and depth prediction performance</li>
</ul>
<h4 id="coca">CoCa</h4>
<ul>
<li><b>Co</b>ntrastive <b>Ca</b>ptioner (CoCa) is an image-text encoder-decoder foundation model pretrained jointly with a contrastive and a captioning loss, which makes it a combination of contrastive approaches like CLIP and generative methods.</li>
<li>decoder is decoupled into two parts, a unimodal decoder and a multimodal decoder</li>
<li>omit cross-attention in unimodal decoder layers to encode text-only representations, and cascade multimodal decoder layers cross-attending to image encoder outputs to learn multimodal image-text representation</li>
</ul>
<p align="justify">
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/coca_architecture.png#center"
         alt="CoCa model architecture, training objectives and pseudocode" width="100%"/> <figcaption>
            <p>Figure 9. Overview of the CoCa model architecture and training objectives [10]</p>
        </figcaption>
</figure>

<p align="justify">
- quick transfer to downstream tasks with zero-shot transfer or minimal task adaptation
</p>
<h4 id="cm3leon">CM3leon</h4>
<p align="justify">
- retrieval augmented, token based, decoder-only multimodal model capable of generating text and images
- uses the CM3 model architecture as basis and benefits a lot from scaling and instruction tuning
- CM3 uses a VQGAN to turn images into 1024 tokens 
- trained with a recipe adapted from text-only language models, including a large scale retrieval-augmented pretraining stage and a second multi-task supervised finetuning stage
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/ra-cm3_architecture.png#center"
         alt="RA-CM3 model architecture" width="100%"/> <figcaption>
            <p>Figure 10. Overview of the RA-CM3 model architecture and training pipeline [12]</p>
        </figcaption>
</figure>

<p align="justify">
</p>
<h3 id="vlms-from-pretrained-backbones">VLMs from Pretrained Backbones</h3>
<p align="justify">
- costly to train from scratch because you need hundreds or thousands of GPUs while having to use millions of image-text pairs 
- to avoid these high costs there is a lot of research in the area of leveraging existing unimodal models 
- just learn to map between the text and image modalities which requires a low amount of compute resources
- in this section we are looking at some of the best open source vision language models out there: the Idefics series, InternVL1.5 and 2, Qwen2-VL
</p>
<h4 id="idefics">Idefics</h4>
<p>Idefics1:</p>
<ul>
<li>introduction of the OBELICS dataset which consists of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images and 115 billion text tokens</li>
<li>one of the advantages of OBELICS is the amount and detail of text per image, which is much bigger than in other image-text datasets</li>
<li>Figure X down below shows the whole dataset creation process</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/obelics.png#center"
         alt="Generation process of the OBELICS dataset" width="90%"/> <figcaption>
            <p>Figure 9. Overview of the OBELICS generation process [7]</p>
        </figcaption>
</figure>

<ul>
<li>with that dataset the authors created two vision language models called &ldquo;Idefics&rdquo;, a 9 and a 80 billion parameter model</li>
<li>it is based on the Flamingo architecture, comprised of two frozen unimodal backbones, Llama for the language encoder and OpenCLIP for the vision part</li>
<li>learnable cross-attention Transformer blocks and Perceiver blocks are added to connect both unimodal encoders</li>
<li>best results on a combination of the LAION and the OBELICS datasets</li>
</ul>
<p>Idefics2:</p>
<ul>
<li>
<p>investigate which choices matter when building VLMs</p>
</li>
<li>
<p>focus on two design choices</p>
<ul>
<li>model architecture, especially strategy of how to fuse image and text information</li>
<li>training procedure</li>
</ul>
</li>
<li>
<p>main findings:</p>
<ul>
<li>progress in vision-language models is largely driven by the progress of pretrained unimodal backbones (LLM is more important than Vision Encoder, but there are no really good vision encoders out there and I think InternVL will show that this finding is not true)</li>
<li>fully autoregressive architecture outperforms cross-attention, but it requires modifications to the optimization procedure to ensure a stable training</li>
<li>reducing the number of visual tokens with learned pooling significantly improves compute efficiency while also improving performance on downstream tasks (efficiency yes but performance is another thing which is hard to prove without a very good vision encoder)</li>
<li>splitting images into subimages allow trading compute efficiency for more performance during inference, especially noticeable for tasks that require text reading capabilities</li>
</ul>
</li>
<li>
<p>based on the findings described above the authors created Idefics2, a 8B parameter VLM that achieves state-of-the-art performance within its size category</p>
</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/idefics2.png#center"
         alt="Idefics2 model architecture" width="90%"/> <figcaption>
            <p>Figure 10. Idefics2 model architecture [8]</p>
        </figcaption>
</figure>

<ul>
<li>vision encoder = SigLIP-SO400M, LLM = Mistral-7B-v0.1</li>
<li>datasets for training: OBELICS, LAION COCO, PMD, OCR-IDL, PDFA</li>
</ul>
<p>Idefics3:</p>
<ul>
<li>test</li>
</ul>
<p align="justify">
</p>
<h4 id="internvl">InternVL</h4>
<p align="justify">
</p>
<h4 id="qwen2-vl">Qwen2-VL</h4>
<p align="justify">
</p>
<h4 id="minicpm-v">MiniCPM-V</h4>
<p>2.5 &amp; 2.6</p>
<p align="justify">
- remaining challenges prevent VLMs from being used in real world applications, the most significant one is the high cost of running those big models 
- most VLMs have to be deployed on high-performance cloud servers, which greatly limits their application scope (mobile, offline, privacy-protective)
- MiniCPM is a model family that tries to change that
- model have strong performance on general benchmarks and especially OCR capabilities, has multilingual support for more than 30 languages and you can run these models on mobile phones
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/minicpm.png#center"
         alt="MiniCPM-V-2.5 model architecture" width="100%"/> <figcaption>
            <p>Figure 11. MiniCPM-V-2.5 model architecture []</p>
        </figcaption>
</figure>

<p align="justify">
- three key modules: visual encoder, compression layer, LLM 
- input image is encoded by a SigLIP SoViT-400m/14, utilizing the adaptive visual encoding approach proposed by LLaVA-UHD
- the compression layer has a perceiver resampler structure with one layer cross-attention
- the compressed visual tokens along with the text input are fed into the LLM for conditional text generation
- image partitioning -> slice encoding (and resizing so that the slice size matches the ViT input size) -> token compression (1024 tokens per image slice, compressed with cross attention layer to 64/96 tokens)
- 3 phase training process: pretraining, supervised fine-tuning, RLAIF-V
- RLAIF-V is a framework for improving trustworthiness and reduce hallucinations 
</p>
<h2 id="training">Training</h2>
<p align="justify">
- scale is very important to model performance but a lot of companies and academic labs don't have the resources to aquire the amount of compute needed to train such models
- recently researchers found that a data curation pipeline makes it possible to beat the scaling law
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/training.png#center"
         alt="Important training decisions to make" width="100%"/> <figcaption>
            <p>Figure X. Important decisions to make when training VLMs [2]</p>
        </figcaption>
</figure>

<p align="justify">
- in the following we will explore the importance of data and recipes that are used to create high quality datasets for VLM training
- in addition we will go into grounding, alignment with human preferences and some recipes about choosing the right model for your use case
</p>
<h3 id="data">Data</h3>
<p align="justify">
The quality and organization of data play a critical role in the successful training of Vision-Language Models (VLMs). Due to the complex nature of these models, which need to understand both visual and textual inputs, effective data management through pruning, augmentation, and quality control is essential to achieving optimal performance. Data pruning is a crucial step in refining the datasets used to train VLMs. Pruning techniques fall into three main categories:
</p>
<ul>
<li><b>Heuristic-based pruning</b> involves the use of rules to eliminate low-quality image-text pairs. This can be done through unimodal filtering, where low-quality captions (e.g., lacking complexity or containing non-English text) or inappropriate images (based on size or aspect ratio) are removed. Multimodal filtering further refines data by using image classifiers to detect when objects in the image do not align with the associated text, or by filtering out examples where the majority of text in the caption appears directly within the image itself.</li>
<li><b>Bootstrapping methods</b> leverage pre-trained VLMs to rank image-text pairs based on their multimodal alignment, using metrics such as CLIPScore. This approach helps prioritize data that better represents meaningful relationships between the visual and textual components, further refining the dataset for training.</li>
<li><b>Diversity and balance techniques</b> aim to create datasets that not only contain high-quality pairs but also represent a wide variety of content. Ensuring diversity in objects, actions, and attributes across different modalities can prevent bias and improve the modelâ€™s generalization capability.</li>
</ul>
<p align="justify">
In addition to pruning, the generation of synthetic data has become an effective strategy for enhancing VLM performance. For example, LLMs can generate captions, which can then be used by text-to-image models to create corresponding images. This helps to supplement the training data, especially in areas where real-world examples may be limited or lacking. Data augmentation techniques, such as adding variations to existing image-text pairs (e.g., rotating or cropping images, rewording captions), further contribute to improving model robustness by exposing the VLM to a wider range of inputs.
</p>
<p align="justify">
Data quality is paramount when training VLMs or any other AI model, but assessing the quality of multimodal and interleaved data remains a challenge. The lack of a standardized method for evaluating such data complicates the task, as quality often depends on the subjective judgment of what constitutes a "good" image-text pair. This is an active area of research, as models continue to evolve and demand increasingly refined datasets for training. Despite the challenges, curated datasets such as OBELICS, which contain interleaved data from multiple modalities, have shown promising results. These datasets are carefully constructed to ensure a rich variety of content, making them valuable resources for VLM training.
</p>
<p align="justify">
In summary, data plays a foundational role in training vision-language models, and techniques such as data pruning, augmentation, and synthetic data generation are essential for improving VLM performance. However, assessing data quality remains an open challenge that continues to evolve alongside advancements in multimodal learning.
</p>
<h3 id="grounding">Grounding</h3>
<p align="justify">
- grounding describes the ability to correctly map text with visual clues 
</p>
<h3 id="alignment">Alignment</h3>
<p align="justify">
</p>
<h2 id="evaluation">Evaluation</h2>
<p align="justify">
</p>
<h3 id="benchmarks">Benchmarks</h3>
<p align="justify">
</p>
<h3 id="huggingface-vlm-leaderboard">HuggingFace VLM Leaderboard</h3>
<ul>
<li>let&rsquo;s take a look at the current Huggingface VLM Leaderboard</li>
</ul>
<p align="justify">
</p>
<h2 id="references">References</h2>
<p><a name="references"></a></p>
<p><a href="https://huggingface.co/blog/vlms">[1]</a> M. Noyan &amp; E. Beeching &ldquo;Vision Language Models Explained&rdquo; (2024).</p>
<p><a href="https://arxiv.org/pdf/2405.17247">[2]</a> Bordes et al. &ldquo;An Introduction to Vision-Language Modeling&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2103.00020">[3]</a> Radford et al. &ldquo;Learning Transferable Visual Models from Natural Language Supervision&rdquo; (2021)</p>
<p><a href="https://arxiv.org/pdf/2405.00740">[4]</a> Lavoie et al. &ldquo;Modeling Caption Diversity in Contrastive Vision-Language Pretraining&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2112.04482">[5]</a> Singh et al. &ldquo;FLAVA: A Foundational Language And Vision Alignment Model&rdquo; (2021)</p>
<p><a href="https://arxiv.org/pdf/2208.02131">[6]</a> Kwon et al. &ldquo;Masked Vision and Language Modeling for Multi-Modal Represenation Learning&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2306.16527">[7]</a> Laurencon et al. &ldquo;OBELICS: An Open Web-Scaled Filtered Dataset of Interleaved Image-Text Documents&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2405.02246">[8]</a> Laurencon et al. &ldquo;What matters when building vision-language models?&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2408.12637">[9]</a> Laurencon et al. &ldquo;Building and better understanding vision-language models: insights and future directions&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2205.01917">[10]</a> Yu et al. &ldquo;CoCa: Contrastive Captioners are Image-Text Foundation Models&rdquo; (2022)</p>
<p><a href="https://arxiv.org/pdf/2309.02591">[11]</a> Yu et al. &ldquo;Scaling Autoregressive Multi-Modal Models: Pre-Training and Instruction Tuning&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2211.12561">[12]</a> Yasunaga et al. &ldquo;Retrieval-Augmented Multimodal Language Modeling&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2201.07520">[13]</a> Aghajanyan et al. &ldquo;CM3: A Causal Masked Multimodal Model of The Internet&rdquo; (2022)</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
