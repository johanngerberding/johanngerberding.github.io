<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Reinforcement-Learning on Johanns Blog</title>
    <link>http://localhost:1313/tags/reinforcement-learning/</link>
    <description>Recent content in Reinforcement-Learning on Johanns Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Mar 2022 10:00:00 +0200</lastBuildDate><atom:link href="http://localhost:1313/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Peek into Deep Reinforcement Learning - Part II</title>
      <link>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/</link>
      <pubDate>Mon, 14 Mar 2022 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/</guid>
      <description>Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space.</description>
      <content:encoded><![CDATA[<p align="justify">
In this second part on Deep Reinforcement Learning we are going to explore some of the combined methods (look into <a href="https://johanngerberding.github.io/posts/A-Peek-into-Deep-Reinforcement-Learning-Part-1">Part I</a> if you don't know what I mean with combined methods). We will start with the so called <b>Advantage Actor-Critic</b> algorithm which combines some of the concepts we have learned about in the first part. Then we dive into the very popular <b>Proximal Policy Optimization</b> algorithm. Moreover I will give you an overview of the <b>Alpha(Go)Zero</b> algorithm from <a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">DeepMind</a>. In the third part of this series we are going to talk more about <a href="https://smartlabai.medium.com/a-brief-overview-of-imitation-learning-8a8a75c44a9c">Imitation Learning</a> and <b>Model-based RL</b>.
</p>
<br>
<h2 id="advantage-actor-critic-a2c">Advantage Actor-Critic (A2C)</h2>
<p align="justify">
As mentioned before, Advantage Actor-Critic (A2C) algorithms combine the ideas from policy gradient methods (e.g. <i>REINFORCE</i>) and a learned value function (e.g. <i>DQN</i>). Here we reinforce a policy with a learned reinforcing signal generated by a learned value function. A2C algorithms therefore consist of two jointly learned components:
</p>
<ul>
<li>an <b>actor</b> which learns a parameterized policy and</li>
<li>a <b>critic</b> which learns a value function to evaluate state-action pairs (it provides a reinforcing signal to the actor)</li>
</ul>
<p align="justify">
The motivation for this is that a learned reinforcing signal can be much more informative for a policy than the rewards available from an environment. Instead of learning $Q^{\pi}(s,a)$ or $V^{\pi}$, it is common to learn the so called <b>advantage function</b> $A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$ as the reinforcing signal. The key idea behind this is that it is better to select an action based on how it performs relative to the other actions available in a particular state, instead of using the absolute value (hence the name <i>advantage</i> actor-critic). The actors learn a parameterized policy $\pi_{\theta}$ using the policy gradient. This is similar to the REINFORCE algorithm, but instead of the Monte-Carlo estimate $R_{t}(\tau)$, the advantage is used:
</p>
<p>$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{t} \Big[ A_{t}^{\pi} \nabla_{\theta} \log \pi_{\theta} (a_{t} | s_{t})\Big]
$$</p>
<p align="justify">
The critic is responsible for learning how to evaluate state-action-pairs and using this to generate $A^{\pi}$. To estimate the advante function, we will go over two possible methods: <b>n-step returns</b> and <b>Generalized Advantage Estimation</b> (GAE). In general the advantage function measures the extent to which an action is better or worse than the policy's average action in a particular state:
</p>
<p>$$
A^{\pi}(s_{t},a_{t}) = Q^{\pi}(s_{t},a_{t}) - V^{\pi}(s_{t})
$$</p>
<p align="justify">
One benefit of using the advantage instead of $Q^{\pi}$ or $V^{\pi}$ is that it avoids penalizing an action for the policy currently being in a particularly bad state, like in the following example:
</p>
<p>$$
Q^{\pi}(s,a) = 110, \quad V^{\pi}(s) = 100, \quad A^{\pi}(s,a) = 10
$$</p>
<p>$$
Q^{\pi}(s,a) = -90, \quad V^{\pi}(s) = -100, \quad A^{\pi}(s,a) = 10
$$</p>
<p align="justify">
The advantage function is better able to capture the long-term effects of an action because it considers all future time steps while ignoring the effects of all the actions to date.
</p>
<br>
<h3 id="n-step-returns">n-step Returns</h3>
<p align="justify">
As seen before, to calculate $A^{\pi}$ we need estimates for both $Q^{\pi}(s,a)$ and $V^{\pi}(s)$. In the n-step Returns method we achieve this by learning $V^{\pi}(s)$ and estimating $Q^{\pi}(s,a)$ from it:
</p>
<p>$$
Q^{\pi}(s,a) = \mathbb{E}_{\tau \sim \pi} \Big[ r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} &hellip; + \gamma^{n} r_{t+n} \Big] + \gamma^{n+1} \hat{V}^{\pi}(s_{t+n+1})
$$</p>
<p align="justify">
The expectation part of the Q-value estimate show above is calculated based on a 3-step return, which means that we use our collected trajectory data to look three steps in the future and sum up the rewards multiplied by a discounting factor $\gamma^{t}$. This part of the equation is unbiased but has a high variance because it comes from only one trajectory. $n$ is a hyperparameter that needs to be tuned. The bigger the value of $n$, the higher the variance of the estimate. The return after the n-th step is calculated by the critic network. It has lower variance since it reflects an expectation over all of the trajectories seen so far, but it is biased because it is calculated using a function approximator. From this we now get a formula for estimating the advantage:
</p>
<p>$$
A_{NSTEP}^{\pi}(s_{t}, a_{t}) = Q^{\pi}(s_{t}, a_{t}) - V^{\pi}(s_{t})
$$</p>
<p>$$
A_{NSTEP}^{\pi}(s_{t}, a_{t}) \approx r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + &hellip; + \gamma^{n} r_{t+n} + \gamma^{n+1} \hat{V}^{\pi}(s_{t+n+1}) - \hat{V}^{\pi}(s_{t})
$$</p>
<br>
<h3 id="generalized-advantage-estimation-gae">Generalized Advantage Estimation (GAE)</h3>
<p align="justify">
Generalized Advantage Estimation was proposed as an improvement over the n-step returns estimate for the advantage function. It addresses the problem of having to explicitly choose the number of steps of returns $n$. The main idea is, instead of picking one value of $n$, we mix multiple values by calculating the advantage using a weighted average of individual advantages calculated with $n = 1, 2, 3, ..., k$. This significantly reduces the variance of the estimator while keeping the bias introduced as low as possible.
</p>
<p>$$
A_{GAE}^{\pi}(s_{t}, a_{t}) = \sum^{\infty}_{l=0}(\gamma \lambda) \delta_{t+l}
$$</p>
<p>$$
\text{where }  \delta_{t} = r_{t} + \gamma V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})
$$</p>
<p align="justify">
GAE is taking a weighted average over a number of advantage estimators with different bias and variance. It weights the high-bias, low-variance 1-step advantage the most but also includes contributions from lower-bias, higher-variance estimators using $2,3,...,n$ steps. The contributions decay at an exponential rate as the number of steps increases and the decay rate gets controlled by the coefficient $\lambda$ (the larger $\lambda$, the higher the variance). In contrast to $n$, $\lambda$ represents a softer choice than $n$, which means smaller values of $\lambda$ will more heavily weight the V-function estimate, whilst larger values will weight the actual rewards more.
</p>
<br>
<h3 id="algorithm--network-architecture">Algorithm &amp; Network Architecture</h3>
<p align="justify">
But how do we learn the value function $V^{\pi}$? We have different possibilities here, one of them is <i>Temporal Difference Learning</i>, similar to DQN. First we parameterize $V^{\pi}$ with $\theta$, then we generate $V_{tar}^{\pi}$ for each of the experiences an agent gathers. Finally we minimize the distance between $\hat{V}^{\pi}(s; \theta)$ and $V^{\pi}_{tar}$ using a simple regression loss such as MSE. You can use different methods to generate $V_{tar}^{\pi}$:
</p>
<ul>
<li>n-step estimate:</li>
</ul>
<p>$$
V_{tar}^{\pi}(s) = r + \hat{V}^{\pi}(s&rsquo;; \theta)
$$</p>
<p>$$
V_{tar}^{\pi}(s_{t}) = r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} + &hellip; + \gamma^{n} r_{t+n} + \gamma^{n+1} \hat{V}^{\pi}(s_{t+n+1})
$$</p>
<ul>
<li>Monte-Carlo estimate:</li>
</ul>
<p>$$
V_{tar}^{\pi}(s_{t}) = \sum_{t&rsquo;=t}^{T}\gamma^{t&rsquo;-t} r_{t&rsquo;}
$$</p>
<ul>
<li>GAE:</li>
</ul>
<p>$$
V_{tar}^{\pi}(s_{t}) = A_{GAE}^{\pi}(s_{t}, a_{t}) + \hat{V}^{\pi}(s_{t})
$$</p>
<p align="justify">
The choice is often related to the method used to estimate the advantage. It is also possible to use a different, more complicated optimization procedure (trust region method) when learning the value function $\hat{V}^{\pi}$ which you can find in the original <a href="https://arxiv.org/abs/1506.02438">GAE paper</a>.
</p>
<p align="justify">
You can run the actor-critic algorithm online as well as batched, down below you see the simplified steps of an online version:
</p>
<ol>
<li>take action $\mathbf{a} \sim \pi_{\theta}(\mathbf{a}|\mathbf{s})$, get $(\mathbf{s}, \mathbf{a}, \mathbf{s&rsquo;}, r)$</li>
<li>update $\hat{V}_{\phi}^{\pi}$ using target $r + \gamma \hat{V}_{\phi}^{\pi}(\mathbf{s&rsquo;})$</li>
<li>evaluate $\hat{A}^{\pi}(\mathbf{s}, \mathbf{a}) = r(\mathbf{s}, \mathbf{a}) + \gamma \hat{V}_{\phi}^{\pi}(\mathbf{s&rsquo;}) - \hat{V}_{\phi}^{\pi}(\mathbf{s})$</li>
<li>$\nabla_{\theta} J(\theta) \approx \nabla_{\theta} \log \pi_{\theta}(\mathbf{a}|\mathbf{s})\hat{A}^{\pi}(\mathbf{s}, \mathbf{a})$</li>
<li>$\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)$</li>
<li><strong>repeat</strong></li>
</ol>
<p align="justify">
To learn the parameterized functions for the actor and the critic, you can use two separate neural networks. But it is also possible and conceptually appealing to use a network structure with shared parameters, because learning $\pi$ and $V^{\pi}(s)$ for the same task are related and they share the same input. Moreover your total number of learnable parameters is reduced which makes it overall more sample efficient. The sharing of the lower layers could be interpreted as learning a common representation of the state space. Upper layers are separate because the networks have different tasks, so their structure and size can be different. But sharing parameters has also some downsides:
</p>
<ul>
<li>learning becomes more unstable because the two gradients can have different scales</li>
<li>we need to balance the two which is typically accomplished by adding a scalar weight to one of the losses for scaling</li>
<li>this adds another hyperparameter to tune</li>
</ul>
<br>
<h2 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h2>
<p align="justify">
Two of the main problems with policy gradient algorithms like REINFORCE are:
</p>
<ul>
<li>susceptibility to performance collapse (agent suddenly performs bad)</li>
<li>sample-inefficiency (on policy - no data reuse)</li>
</ul>
<p align="justify">
The PPO paper addresses both of these issues by introducing a <b>surrugate objective</b> which avoids performance collapse by guarantueeing monotonic policy improvement and enables off-policy data reuse. This leads often to more stable and sample efficient training. PPO methods have some benefits of <a href="https://arxiv.org/abs/1502.05477">trust region policy optimization</a> (TRPO) but are much simpler to implement, more general and have better sample efficiency. PPO can be used to extend f.e. REINFORCE of Actor-Critic methods by replacing their original objective $J(\pi_{\theta})$ with the surrogate objective. It is currently one of the most popular policy gradient algorithms. The paper presents two different variants of the objective function which we are diving into both.
</p>
<h3 id="adaptive-kl-penalty">Adaptive KL Penalty</h3>
<p align="justify">
This PPO version incorporates the <i>Kullback-Leiber Divergence</i> (KL) as a penalty into the surrogate objective. The KL is a measure for information loss between two distributions, you can find a nice explanation <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">here</a>. The objective function looks like the following:
</p>
<p>$$
J^{KPLEN}(\theta) = \max_{\substack{\theta}} \mathbb{E}_{t}\Big[r_{t}(\theta)A_{t}-\beta KL \big(\pi_{\theta}(a_{t}|s_{t}) \parallel \pi_{\theta_{old}}(a_{t}|s_{t})\big)\Big]
$$</p>
<p>$$
\text{with } r(\theta) = \frac{\pi(a_{t}|s_{t})}{\pi_{old}(a_{t}|s_{t})}
$$</p>
<p align="justify">
$\beta$ is an adaptive coefficient which controls the size of the KL penalty (the larger $\beta$, the larger the difference between $\pi$ and $\pi_{old}$). Because $\beta$ is a new hyperparameter that has to work for different problems in different scenarios, it is not a constant value. Instead $\beta$ gets recalculated after every policy update based on a heuristic which allows it to adapt over time:
</p>
<ul>
<li>set target value $\delta_{tar}$ for the expectation of KL, init $\beta$ randomly</li>
<li>use multiple epochs of minibatch SGD, optimize the KL-penalized surrogate objective</li>
<li>compute $\delta = \mathbb{E}_{t}\Big[ KL \big(\pi_{\theta}(a_{t}|s_{t}) \parallel \pi_{\theta_{old}}(a_{t}|s_{t})\big) \Big]$
<ul>
<li>if $\delta &lt; \delta_{tar}/1.5$ then $\beta \leftarrow \beta/2$</li>
<li>else if $\delta &gt; \delta_{tar} \times 1.5$ then $\beta \leftarrow \beta \times 2$</li>
<li>else pass</li>
</ul>
</li>
</ul>
<p align="justify">
This approach is simple to implement but you still have to choose a target value for $\delta_{tar}$. In general it is computationally expensiv e to calculate the KL, which is one reason why the second form of the PPO is often preferred.
</p>
<h3 id="clipped-surrogate-objective">Clipped Surrogate Objective</h3>
<p align="justify">
The clipped surrogate objective is much simpler and omits the calculation of the KL entirely:
</p>
<p>$$
J^{CLIP}(\theta) = \mathbb{E}_{t} \Big[ \min \big( r_{t}(\theta) A_{t}, clip(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon)A_{t} \big) \Big]
$$</p>
<p align="justify">
$\epsilon$ is a hyperparameter which defines the clipping neighbourhood and can be decayed during training. This objective function prevents parameter updates which could cause large and risky changes to the policy $\pi_{\theta}$. As mentioned before, you can use both of the described objectives with e.g. Actor-Critic methods or REINFORCE. For details take a look at the <a href="https://arxiv.org/abs/1707.06347">paper</a> itself.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/PPO_ac_algorithm.png#center"
         alt="PPO algorithm Actor-Critic style" width="90%"/> <figcaption>
            <p>Figure 1. Algorithm - PPO Actor-Critic [2]</p>
        </figcaption>
</figure>

<p align="justify">
Down below you can find a few results from the paper on several MuJoCo environments after one million steps of training. At that times it outperformed the previous methods on almost all the continuous control environments.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/PPO_results.png#center"
         alt="MuJoCo environment results" width="100%"/> <figcaption>
            <p>Figure 2. MuJoCo environment results [2]</p>
        </figcaption>
</figure>

<br>
<h2 id="alphagozero">Alpha(Go)Zero</h2>
<p align="justify">
<i>AlphaGo Zero</i> is the successor of DeepMinds <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> which was the first program to defeat a world champion in the game of Go. It differs from AlphaGo in several important aspects. It was trained solely by self-play without any use of expert data or human supervision, but knowing the rules of the game (model-based). It uses only a single neural network and defeated AlphaGo by 100 games to 0. With <i>AlphaZero</i>, DeepMind generalized and extended the method to the games Chess and Shogi.
</p>
<p align="justify">
<i>AlphaGo Zero</i> combines a neural network and <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a> (MCTS) in an elegant policy iteration framework to achieve stable learning and rapid improvement. Lets start by taking a closer look at the neural network.
</p>
<h3 id="neural-network">Neural Network</h3>
<p align="justify">
The authors used a deep Convolutional Neural Network $f_{\theta}$ which is parameterized by $\theta$ and takes as input the state $\mathbf{s}$ of the board and outputs both, move probabilities and a value $(\mathbf{p}, v) = f_{\theta}(s)$. It combines the roles of the policy network and value network into a single architecture, similar to the shared network which can be used in Actor-Critic methods. The vector of the move probabilities $\mathbf{p}$ represents the probability of selecting each move $a$, $\mathbf{p}_{a} = Pr(a|s)$. The value $v$ is a scalar evaluation, estimating the probability of the current player winning from position $\mathbf{s}$. The network is trained at the end of each game of self-play based on the data of each time step $t$ which is stored in the form $(s_{t}, \pi_{t}, z_{t})$. $\pi_{t}$ represents the search probabilities generated by MCTS and $z_{t} \in {-1, 1}$ is the game winner from the perspective of the current player (gets added at the end of the game to each timestep). The loss function $l$ sumsof the mean-squared error and cross-entropy loss:
</p>
<p>$$
l = (z - v)^{2} - \pi^{T} \log \mathbf{p} + c || \theta ||^{2}
$$</p>
<p align="justify">
where $c$ is a hyperparameter controlling the level of L2 weight regularization. Figure 3. down below illustrates this self-play RL procedure in <i>AlphaGo Zero</i>.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/AlphaGoZero_self_play.png#center"
         alt="AlphaGo Zero self-play and neural network training" width="70%"/> <figcaption>
            <p>Figure 3. AlphaGo Zero - Self-play and neural network training [7]</p>
        </figcaption>
</figure>

<h3 id="monte-carlo-tree-search">Monte Carlo Tree Search</h3>
<p align="justify">
The neural network we just talked about is trained from games of self-play by a novel reinforcement learning algorithm. In each position $s$, an MCTS is executed, guided by the neural network $f_{\theta}$ and it outputs search probabilities $\pi$ of playing each move. Based on these you can usually select a much stronger move than based on the raw move probabilities $\mathbf{p}$ output by the neural network $f_{\theta}(s)$ which makes MCTS to kind of a policy improvement operator. It uses the neural network $f_{\theta}$ for its simulations. Each edge $(s,a)$ in the search tree stores a prior probability $P(s,a)$, a visit count $N(s,a)$ and an action value $Q(s,a)$. Each simulation run starts from the root state and iteratively selects moves that maximize an upper confidence bound
</p>
<p>$$
U(s,a) = Q(s,a) + c * P(s,a) * \frac{\sum_{b}N(s,b)}{1 + N(s,a)}
$$</p>
<p align="justify">
where c is a hyperparameter which controls the degree of exploration. We initialize our empty search tree with $\mathbf{s}$ as the root. A single simulation run proceeds as follows. First we compute the action $a$ that maximizes the upper confidence bound $U(s,a)$. We play this action and if the resulting state $\mathbf{s'}$ exists in our tree, we recursively search on $\mathbf{s'}$. If it doesn't exist, it gets added to the tree and $Q(s',a)$ and $N(s',a)$ are initialized to 0 for all $a$. The neural network is used to initialize $P(s', \cdot) = \mathbf{p}$ and $v(s') = v_{\theta}(s')$. Instead of performing a rollout, $v(s')$ gets propagated up along the path seen in the current simulation and update all $Q(s,a)$ values. If on the other hand a terminal state gets encountered, the reward gets propagated.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/AlphaGoZero_mcts.png#center"
         alt="AlphaGo Zero Monte Carlo Tree Search Process" width="90%"/> <figcaption>
            <p>Figure 4. AlphaGo Zero - MCTS Search Process [7]</p>
        </figcaption>
</figure>

<p align="justify">
DeepMind trained the AlphaGo Zero system for approximately three days, generating 4.9 million games of self-play, using 1,600 simulations for each MCTS. The neural net they used consisted of 20 residual blocks (they also trained a bigger network for longer).
</p>
<br>
<h2 id="summary">Summary</h2>
<p align="justify">
In this post we looked at three combined RL methods, A2C, PPO and AlphaGoZero. As Alpha(Go)Zero is a more specialized method for games, PPO and A2C are more generally applicable. I hope I was able to give you an understandable insight into the three algorithms and their most important components. In the next post of this series I am going to dive a bit deeper into model-based RL and imitation learning. Until then, stay tuned and healthy!
</p>
<h2 id="references">References</h2>
<p><a href="https://arxiv.org/abs/1602.01783">[1]</a> Mnih et al. &ldquo;Asynchronous Methods for Deep Reinforcement Learning&rdquo; (2016).</p>
<p><a href="https://arxiv.org/abs/1707.06347">[2]</a> Schulman et al. &ldquo;Proximal Policy Optimization Algorithms&rdquo; (2017).</p>
<p><a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">[3]</a> Kullback-Leiber Divergence Explained (2017).</p>
<p><a href="https://arxiv.org/abs/1502.05477">[4]</a> Schulman et al. &ldquo;Trust Region Policy Optimization&rdquo; (2015).</p>
<p><a href="https://arxiv.org/abs/1712.01815">[5]</a> Silver et al. &ldquo;Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm&rdquo; (2017).</p>
<p><a href="https://arxiv.org/abs/1506.02438">[6]</a> Schulman et al. &ldquo;High-Dimensional Continuous Control Using Generalized Advantage Estimation&rdquo; (2015).</p>
<p><a href="https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf">[7]</a> Silver et al. &ldquo;Mastering the game of Go without human knowledge&rdquo; (2017).</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>A Peek into Deep Reinforcement Learning - Part I</title>
      <link>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-1/</link>
      <pubDate>Wed, 26 Jan 2022 10:00:00 +0200</pubDate>
      
      <guid>http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-1/</guid>
      <description>Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms.</description>
      <content:encoded><![CDATA[<p align="justify">
I guess many of you interested in the field of Machine Learning have heard about <a href="https://deepmind.com/">DeepMind</a> creating a system defeating the best professional human player in the Game of Go, called <i>AlphaGo</i>. I personally have never played Go before, so at first I wasn't aware of its complexity. Two years later they presented AlphaGo's successor called <i>AlphaZero</i>, which learned from scratch to master not only Go but also Chess and Shogi and defeated <i>AlphaGo</i> 100-0 without the use of domain knowledge or any human data. In December 2020 they presented the next evolution of this algorithm, called MuZero, which was able to master Go, Chess, Shogi and nearly all Atari games without knowing the rules of the game or the dynamics of the environment. After reading all of this it got my hooked and I wanted to know more about the algorithms and theory behind this magic - <b>Reinforcement Learning (RL)</b>. In fact RL is around for quite some time now but in the last couple of years it really took off. Despite the truly impressive results of <a href="https://deepmind.com/">DeepMind</a>, however, after a short research I also quickly realized that there are still relatively few viable real-world applications of reinforcement learning. However, I hope that this will change in the near future. To be honest, I am most excited about the applications in the field of robotics (e.g. <a href="https://openai.com/blog/solving-rubiks-cube/">Solving Rubik's Cube with Robotic Hand</a>, <a href="https://ai.facebook.com/blog/ai-now-enables-robots-to-adapt-rapidly-to-changing-real-world-conditions/">Robotic Motor Adaptation</a>) and autonomous driving. This prompted me to learn more about the field of Deep Reinforcement Learning and share my learnings with you in this blogpost.
</p>
<h2 id="introduction-into-reinfocement-learning">Introduction into Reinfocement Learning</h2>
<p align="justify">
Before we dive into the algorithms and all that cool stuff I want to start with a short introduction into the most essential concepts and the terminology of RL. After that I give an overview of the framework for modeling those kinds of problems, called <b>Markov Decision Processes (MDP)</b> and I will present you a way to categorize deep RL algorithms.
</p>
<h3 id="concepts-and-terminology">Concepts and Terminology</h3>
<p align="justify">
RL in general is concerned with solving sequential decision-making problems (e.g. playing video games, driving, robotic control, optimizing inventory) and such problems can be expressed as a system consisting of an <b>agent</b> which acts in an <b>environment</b>. These two are the core components of RL. The environment produces information which describes the <b>state</b> of the system and it can be considered to be anything that is not the agent. So, what is an agent then? An agent "lives" in and interacts with an environment by observing the state (or at least a part of it) and uses this information to select between actions to take. The environment accepts these actions and transitions into the next state and after that returns the next state and a <b>reward</b> to the agent. A reward signal is a single scalar the environment sents to the agent which defines the goal of the RL problem we want to solve. This whole cycle I have described so far is called one time-step and it repeats until/if the environment terminates (Figure 1.).
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/reinforcement_learning_loop.png#center"
         alt="Reinforcement Learning Control-Loop" width="70%"/> <figcaption>
            <p>Figure 1. Reinforcement Learning Control-Loop [1]</p>
        </figcaption>
</figure>

<p align="justify">
The action selection function the agent uses is called a <b>policy</b>, which maps states to actions. Every action will change the environment and affect what an agent observes and does next. To determine which actions to take in different situations every RL problems needs to have an objective or goal which is described by the sum of rewards received over time. The goal is to maximize the objective by selecting good actions which the agent learns by interacting with the environment in a process of trial-and-error combined with using the reward signals it receives to reinforce good actions. The exchange signal is often called <b>experience</b> and described as tuple of $(s_{t}, a_{t}, r_{t})$. Moreover we have to differentiate between <i>finite</i> and <i>infinite</i> environments. In finite environments $t=0,1,...,T$ is called one <b>episode</b> and a sequence of experiences over an episode $\tau = (s_{0}, a_{0}, r_{0}), (s_{1}, a_{1}, r_{1}), ...$ is called a <b>trajectory</b>. An agent typically needs many episodes to learn a good policy, ranging from hundreds to millions depending on the complexity of the problem. Now lets describe the states, actions and rewards a bit more formally:
</p>
<ul>
<li>$s_{t} \in \mathcal{S}$ is the state, $\mathcal{S}$ is the state space</li>
<li>$a_{t} \in \mathcal{A}$ is the action, $\mathcal{A}$ is the action space</li>
<li>$r_{t} = \mathcal{R}(s_{t}, a_{t}, s_{t+1})$ is the reward, $\mathcal{R}$ is the reward function</li>
</ul>
<p align="justify">
Next we are diving into the framework for modeling those interactions between the agent and the environment called Markov Decision Processes.
</p>
<h3 id="rl-as-an-markov-decision-process">RL as an Markov Decision Process</h3>
<p align="justify">
MDP in general is a mathematical framework for modeling sequential decision making and in RL the transitions of an environment between states is described as an MDP. The <b>transition function</b> has to meet the <b>Markov property</b> which assumes that the next state $s_{t+1}$ only depends on the previous state $s_{t}$ and action $a_{t}$ instead of the whole history of states and actions. When we talk about a state here it is also important to distinguish between the <b>observed state</b> $s_{t}$ from the agent and the environments <b>internal state</b> $s_{t}^{int}$ used by the transition function. In an MDP $s_{t} = s_{t}^{int}$ but in many interesting real-world problems the agent has only limited information and $s_{t} \neq s_{t}^{int}$. In those cases the environment is described as a <b>partially oberservable</b> MDP, in short <b>POMDP</b>.
</p>
<p>All we need for the formal MDP description of a RL problem is a 4-tuple $\mathcal{S}$, $\mathcal{A}$, $P(.)$ and $\mathcal{R}(.)$:</p>
<ul>
<li>$\mathcal{S}$ is the set of states</li>
<li>$\mathcal{A}$ is the set of actions</li>
<li>$P(s_{t+1} \mid s_{t}, a_{t})$ is the state transition function of the environment</li>
<li>$\mathcal{R}(s_{t}, a_{t}, s_{t+1})$ is the reward function of the environment</li>
</ul>
<p align="justify">
It is important to note that agents to have access to the transition function of the reward function, they only get information about these functions through the $(s_{t}, a_{t}, r_{t})$ tuples. The objective of an agent can be formalized by the <b>return</b> $R(\tau)$ using a trajectory from an episode
</p>
<p>$$
R(\tau) = r_{0} + \gamma r_{1} + \gamma^{2} r_{2} + &hellip; + \gamma^{T} r_{T} = \sum_{t=0}^{T} \gamma^{t} r_{t}
$$</p>
<p align="justify">
$\gamma$ describes a <i>discount factor</i> which changes the way future rewards are valued. The objective $J(\tau)$ is simply the expectation of the returns over many trajectories 
</p>
<p>$$
J(\tau) = \mathbb{E}_{\tau \sim \pi} \big[R(\tau)\big] = \mathbb{E}_{\tau} \Big[\sum_{t=0}^{T} \gamma^{t} r_{t} \Big]
$$</p>
<p align="justify">
For problems with infinite time horizons it is important to set $\gamma < 1$ to prevent the objective from becoming unbounded.
</p>
<h3 id="learnable-functions-in-rl">Learnable Functions in RL</h3>
<p align="justify">
In RL there exist three primary functions which can be learned. One of them is the <b>policy</b> $\pi$ which maps states to actions: $a \sim \pi(s)$. This policy can be either deterministic or stochastic.
</p>
<p align="justify">
The second one is called a <b>value function</b>, $V^{\pi}$ or $Q^{\pi}(s,a)$, which estimates the expected return $\mathbb{E}_{\tau}[R(\tau)]$. Value functions provide information about the objective and thereby help an agent to understand how good the states and available actions are in terms of future rewards. As mentioned before, there exist two different versions of value functions:
</p>
<p>$$
V^{\pi}(s) = \mathbb{E}_{s_{0} = s, \tau \sim \pi} \Big[\sum_{t=0}^{T} \gamma^{t} r_{t}\Big]
$$</p>
<p>$$
Q^{\pi}(s,a) = \mathbb{E}_{s_{0} = s, a_{0} = a, \tau \sim \pi} \Big[\sum_{t=0}^{T} \gamma^{t} r_{t}\Big]
$$</p>
<p align="justify">
$V^{\pi}$ evaluates how good or bad a state is, assuming we continue with the current policy. $Q^{\pi}$ instead evaluates how good an action-state pair is.
</p>
<p align="justify">
The last of the three functions is the <b>environment model</b> or the <b>transition function</b> $P(s' \mid s,a)$ which provides information about the environment itself. If an agent learns this function, it is able to predict the next state $s'$ that the environment will transition into after taking action $a$ in state $s$. This gives the agent some kind of "imagination" about the consequences of its actions without interacting with the environment (planning).
</p>
<p align="justify">
All of the three functions discussed above can be learned so we can use deep neural networks as the function approximation method. Based on this you are also able to categorize deep RL algorithms.
</p>
<h2 id="algorithms-overview">Algorithms Overview</h2>
<p align="justify">
We can group RL algorithms based on the functions they learn into four different categories:
</p>
<ul>
<li><em>Policy-based algorithms</em></li>
<li><em>Value-based algorithms</em></li>
<li><em>Model-based algorithms</em></li>
<li><em>Combined Methods</em></li>
</ul>
<p><strong>Policy-based</strong> or <strong>policy optimization</strong> algorithms are a very general class of optimization methods which can be applied to problems with any type of actions, discrete, continuous or a mixture (multiaction). Moreover they are guaranteed to converge. The main drawbacks of these algorithms are that they have a high variance and are sample inefficient.</p>
<p align="justify">
Most of the <b>value-based</b> algorithms learn $Q^{\pi}$ instead of $V^{\pi}$ because it is easier to convert into a policy. Moreover they are typically more sample efficient than policy-based algorithms because they have lower variance and make better use of data gathered from the environment. But they don't have a convergence guarantee and are only applicable to discrete action spaces (*QT-OPT* can also be applied to continuous action spaces).
</p>
<p align="justify">
As mentioned before <b>model-based</b> algorithms learn a model of the environments transition dynamics or make use of a known dynamics model. The agent can use this model to "imagine" what will happen in the future by predicting the trajectory for a few time steps. Purely model-based approaches are commonly applied to games with a target state or navigation tasks with a goal state. These kinds of algorithms are very appealing because they equip an agent with foresight and need a lot fewer samples of data to learn good policies. But for most problems, models are hard to come by because many environments are stochastic, their transition dynamics are not known and the model therefore must be learned (which is pretty hard in large state spaces).
</p>
<p align="justify">
These days many <b>combined methods</b> try to get the best of each, e.g. <i>Actor-Critic</i> algorithms learn a policy and a value function where the policy acts and the value function critiques those actions. Another popular example would be <i>AlphaZero</i> which combined <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a> with learning $V^{\pi}$ and a policy $\pi$ to master the game of Go.
</p>
<p align="justify">
In Figure 2 you can see a slightly different way of categorizing these algorithms, by first differentiating between model-based and model-free methods.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/rl_algorithms_taxonomy.svg#center"
         alt="Reinforcement Learning Algorithms Taxonomy" width="90%"/> <figcaption>
            <p>Figure 2. Non-exhaustive RL algorithms taxonomy [2]</p>
        </figcaption>
</figure>

<p>Another possibility to distinguish between these algorithms would be:</p>
<ul>
<li><strong>on-policy</strong>: Training can only utilize data generated from the current policy $\pi$ which tends to be sample-inefficient and needs more training data.</li>
<li><strong>off-policy</strong>: Any data collected can be reused in training which is more sample-efficient but requires much more memory.</li>
</ul>
<p align="justify">
In the following we are going to describe three different algorithms. One of them is a policy-based algorithm called <b>REINFORCE</b> (Policy Gradient). The other two are value-based algorithms called <b>SARSA</b> and <b>Deep Q-Networks</b>. In the second part of this post I will also go into some more advanced combined methods.
</p>
<h2 id="policy-gradient---reinforce">Policy Gradient - REINFORCE</h2>
<p align="justify">
The REINFORCE algorithm was invented in 1992 by Ronald J. Williams. It learns a parameterized policy which produces action probabilities from states and an agent can use this policy directly to act in an environment. The action probabilities are changed by following the <i>policy gradient</i>. The algorithm has three core components:
</p>
<ul>
<li>parameterized policy</li>
<li>objective to be maximized</li>
<li>method for updating the policy parameters</li>
</ul>
<p align="justify">
A neural network is used to learn a good policy by function approximation. This is often called a <i>policy network</i> $\pi_{\theta}$ (parameterized by $\theta$). The objective function to maximize is the expected return over all complete trajectories generated by an agent:
</p>
<p>$$
J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}} \big[ R(\tau) \big] = \mathbb{E}_{\tau \sim \pi_{\theta}} \Big[ \sum_{t=0}^{T} \gamma^{t} r_{t} \Big]
$$</p>
<p>To maximize this objective, gradient ascent is performed on the policy parameters $\theta$. The parameters are then updated based on:</p>
<p>$$
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\pi_{\theta})
$$</p>
<p>The term $\nabla_{\theta} J(\pi_{\theta})$ is known as the <strong>policy gradient</strong> and is defined as:</p>
<p>$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}} \Big[ \sum_{t=0}^{T} R_{t}(\tau) \nabla_{\theta} \log \pi_{\theta} (a_{t} | s_{t})\Big]
$$</p>
<p align="justify">
The policy gradient is numerically estimated using <i>Monte Carlo Sampling</i> which refers to any method that uses random sampling to generate data used to approximate a function. Now lets take a look at the pseudocode for the algorithm:
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/reinforce.png#center"
         alt="pseudocode REINFORCE algorithm with baseline" width="80%"/> <figcaption>
            <p>Figure 3. Pseudocode REINFORCE with baseline [1]</p>
        </figcaption>
</figure>

<p align="justify">
One problem of REINFORCE is the policy gradient estimate can have high variance. One way to reduce this is by introducing a baseline (see Figure 3). Next we are going to learn about two popular value-based algorithms, SARSA and Q-Networks. If you are interested in how the actual code would look like, check out my <a href="https://github.com/johanngerberding/reinforcement-learning-pytorch">RL repository</a>.
</p>
<h2 id="sarsa">SARSA</h2>
<p align="justify">
SARSA (State-action-reward-state-action) is a value-based on-policy method which aims to learn the Q-function $Q^{\pi}(s,a)$. It is based on two core ideas:
</p>
<ol>
<li><strong>Temporal Difference Learning</strong> for learning the Q-function</li>
<li>Generate actions using the Q-function</li>
</ol>
<p align="justify">
In Temporal Difference Learning (TD-Learning) a neural network is used to produce Q-value estimates given $(s,a)$ pairs as input. This is called <b>value network</b>. The general learning workflow is pretty similar to a classical supervised learning workflow:
</p>
<ol>
<li>Generate trajectories $\tau$s and predict a $\hat{Q}$-value for each $(s,a)$-pair</li>
<li>Generate target Q-values $Q_{tar}$ based on the trajectories.</li>
<li>Minimize the distance between $\hat{Q}$ and $Q_{tar}$ using a standard regression loss (like MSE)</li>
<li>Repeat 1-3</li>
</ol>
<p align="justify">
If we would want to use Monto Carlo Sampling here, an agent would have to wait for episodes to end before any data from that episode can be used to learn from, which delays training. We can use TD-Learning to circumvent this problem. The key insight here is that we can define Q-values for the current time step in terms of Q-values of the next time step. This recursive definition is known as the <b>Bellman Equation</b>:
</p>
<p>$$
Q^{\pi}(s,a) \approx r + \gamma Q^{\pi}(s&rsquo;,a&rsquo;) = Q^{\pi}_{tar}(s,a)
$$</p>
<p align="justify">
But if we use the same policy to generate $\hat{Q}^{\pi}(s,a)$ and $Q_{tar}^{\pi}(s,a)$ but how does this work or learn at all? This is possible because $Q_{tar}^{\pi}(s,a)$ uses information one time step into the future when compared with $\hat{Q}^{\pi}(s,a)$. Thus it has access to the reward $r$ from the next state $s'$ ($Q_{tar}^{\pi}(s,a)$ is slightly more informative about how the trajectory will turn out). TD Learning gives us a method for learning how to evaluate state action pairs, but what about choosing the actions?
</p>
<p align="justify">
If we already learned the optimal Q-function, the value of each state-action pair will represent the best possible expected value from taking that action, so we can act greedily with respect to those Q-values. The problem is that this optimal Q-function isn't typically known in advance. But we can use an iterative approach to improve the Q-value by improving the Q-function:
</p>
<ol>
<li>Initialize a neural network randomly with the parameters $\theta$ to represent the Q-function $Q^{\pi}(s,a;\theta)$</li>
<li>Repeat the following until the agent stops improving:
<ol>
<li>Use $Q^{\pi}(s,a;\theta)$ to act in the environment, by action greedily (or $\varepsilon$-greedy) with respect to the Q-values and store all of the experiences $(s,a,r,s&rsquo;)$.</li>
<li>Use the stored experiences to update $Q^{\pi}(s,a;\theta)$ using the Bellman equation to improve the Q-function estimate, which, in turn, improves the policy.</li>
</ol>
</li>
</ol>
<p align="justify">
A greedy action selection policy is deterministic and might lead to an agent not exploring the whole state-action space. To mitigate this issue a so called $\varepsilon$-greedy policy is often used in practice, where you act greedy with probability 1-$\varepsilon$ and random with probability $\varepsilon$. A common strategy here is to start training with a high $\varepsilon$ (e.g. 1.0) so that the agent almost acts randomly and rapidly explores the state-action space. Decay $\varepsilon$ gradually over time so that after many steps the policy, hopefully, approaches the optimal policy. The figure down below shows the pseudocode for the whole SARSA algorithm.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/sarsa.png#center"
         alt="pseudocode SARSA algorithm" width="80%"/> <figcaption>
            <p>Figure 4. Pseudocode SARSA from [1]</p>
        </figcaption>
</figure>

<p align="justify">
In the next section we are going to learn more about another popular value-based method called <b>Deep Q-Networks</b>.
</p>
<h2 id="deep-q-networks">Deep Q-Networks</h2>
<h3 id="general-concept">General Concept</h3>
<p align="justify">
Deep Q-Networks (DQN) were proposed by Mnih et al. in 2013 and are like SARSA a value-based temporal difference learning algorithm that approximates the Q-function. It is also only applicable to environments with discrete action spaces. Instead of learning the Q-function for the current policy DQN learns the optimal Q-function which improves its stability and learning speed over SARSA. This makes it an off-policy algorithm because the optimal Q-function does not depend on the data gathering policy. This also makes it more sample efficient than SARSA. The main difference between the two is the $Q^{\pi}_{tar}(s,a)$ construction:
</p>
<p>$$
Q^{\pi}_{tar}(s,a) = r + \gamma \max_{a&rsquo;} Q^{\pi}(s&rsquo;,a&rsquo;)
$$</p>
<p align="justify">
Instead of using the action $a'$ actually taken in the next state $s'$ to estimate $Q^{\pi}_{tar}(s,a)$, DQN uses the maximum Q-value over all of the potential actions available in that state, which makes it independent from the policy. For action selection you can use e.g. $\epsilon$-greedy or <b>Boltzmann policy</b>. The $\epsilon$-greedy exploration strategy is somewhat naive because the exploration is random and do not use any previously learned knowledge about the environment. In contrast the Boltzmann policy tries to improve on this by selecting actions based on their relative Q-values which has the effect of focusing exploration on more promising actions. It is a parameterized softmax function, where a temperature parameter $\tau \in (0, \infty)$ controls how uniform or concentrated the resulting probability distribution is:
</p>
<p>$$
p(a|s) = \frac{e^{Q^{\pi}(s,a)/\tau}}{\sum_{a&rsquo;}e^{Q^{\pi}(s,a&rsquo;)/\tau}}
$$</p>
<p align="justify">
The role of the temperature parameter $\tau$ in the Boltzmann policy is analoguous to that of $\epsilon$ in the $\epsilon$-greedy policy. It encourages exploration of the state-action space, a high value for $\tau$ means more exploration. To balance exploration and exploitation during training, $\tau$ is adjusted properly (decreased over time). The Boltzmann policy is often more stable than the $\epsilon$-greedy policy but it also can cause an agent to get stuck in a local minimum if the Q-function estimate is inaccurate for some parts of the state space. This can be tackled by using a very large value for $\tau$ at the beginning of training. As mentioned at the beginning of this section, DQN is an off-policy algorithm that doesn't have to discard experiences once they have been used, so we need a so called <b>experience-replay memory</b> to store these experiences for the training. It stores the $k$ most recent experiences an agent has gathered and if the memory is full, older experiences will be discarded. The size of the memory should be large enough to contain many episodes of experiences, so that each batch will contain experiences from different episodes and policies. This will decorrelate the experiences used for training and reduce the variance of the parameter updates, helping to stabilize training. Down below you can the full algorithm from the paper.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/reinforcement_learning/deep_q_learning_with_experience.png#center"
         alt="Deep Q-Learning Algorithm with Experience Replay" width="90%"/> <figcaption>
            <p>Figure 5. Deep Q-Learning Algorithm with Experience Replay [3]</p>
        </figcaption>
</figure>

<h3 id="dqn-improvements">DQN Improvements</h3>
<p align="justify">
Over time people have explored multiple ways to improve the DQN algorithm which we will talk about in the last part of this post. The three modifications are the following:
</p>
<ol>
<li><em>Target networks</em></li>
<li><em>Double DQN</em></li>
<li><em>Prioritized Experience Replay</em></li>
</ol>
<h4 id="target-networks">Target Networks</h4>
<p align="justify">
In the original DQN algorithm $Q_{tar}^{\pi}$ is constantly changing because it depends on $\hat{Q}^{\pi}(s,a)$. This makes it kind of a "moving target" which can destabilize training because it makes it unclear what the network should learn. To reduce the changes in $Q_{tar}^{\pi}(s,a)$ between training steps, you can use a target network. Second network with parameters $\varphi$ which is a lagged copy of the Q-network $Q^{\pi_{\theta}}(s,a)$. It gets periodically updated to the current values for $\theta$, which is called a replacement update. The update frequency is problem dependent (1000 - 10000, for complex environments and 100 - 1000 for simpler ones). Down below you can see the modified Bellman equation:
</p>
<p>$$
Q_{tar}^{\pi_{\varphi}}(s,a) = r + \gamma \max_{a&rsquo;}Q^{\pi_{\varphi}}(s&rsquo;,a&rsquo;)
$$</p>
<p align="justify">
Introducing this network stops the target from moving and transforms the problem into a standard supervised regression. An alternative to the periodic replacement is the so called <b>Polyak update</b>. At each time step, set $\varphi$ to be a weighted average of $\varphi$ and $\theta$, which makes $\varphi$ change more slowly than $\theta$. The hyperparameter $\beta$ controls the speed at which $\varphi$ changes:
</p>
<p>$$
\varphi \leftarrow \beta \varphi + (1 - \beta) \theta
$$</p>
<p align="justify">
It is important to note that each approach has its advantages and disadvantages and no one is clearly better than the other.
</p>
<h4 id="double-dqn">Double DQN</h4>
<p align="justify">
The Double DQN addresses the problem of overestimating Q-values. If you want to know in detail about why this actually happens, take a look at the following <a href="https://arxiv.org/pdf/1509.06461.pdf">paper</a>. The Q-value overestimation can hurt exploration and the error it causes will be backpropagated in time to earlier (s,a)-pairs which adds error to those as well. Double DQN reduces this by learning two Q-function estimates using different experiences. The Q-maximizing action $a'$ is selected using the first estimate and the Q-value that is used to calculate $Q_{tar}^{\pi}(s,a)$ is generated by the second estimate using the before selected action. This removes the bias and leads to the following:
</p>
<p>$$
Q_{tar: DDQN}^{\pi}(s,a) = r + \gamma Q^{\pi_{\varphi}} \big(s&rsquo;, \max_{a&rsquo;}Q^{\pi_{\theta}}(s&rsquo;,a&rsquo;) \big)
$$</p>
<p align="justify">
If the number of time steps between the target network and the training network is large enough, we could use this one for the Double DQN.
</p>
<h4 id="prioritized-experience-replay">Prioritized Experience Replay</h4>
<p align="justify">
The main idea behind this is that some experiences in the replay memory are more informative than others. So if we can train an agent by sampling informative experiences more often then the agent may learn faster. To achieve this, we have to answer the following two questions:
</p>
<ol>
<li><em>How can we automatically assign a priority to each experience?</em></li>
<li><em>How to sample efficiently from the replay memory using these priorities?</em></li>
</ol>
<p align="justify">
As the priority we can simply use the TD error without much computational overhead. At the start of training the priorities of all values are set to a large constant value to encourage each experience to be sampled at least once. The sampling could be done rank-based or based on proportional prioritization. For details on the rank based prioritization, take a look at this <a href="https://arxiv.org/pdf/1511.05952.pdf">paper</a>. The priority for the proportional method is calculated as follows:
</p>
<p>$$
P(i) = \frac{(|\omega_{i}| + \epsilon)^{\eta}}{\sum_{j}(|\omega_{i}| + \epsilon)^{\eta}}
$$</p>
<p align="justify">
where $\omega_{i}$ is the TD error of experience $i$, $\epsilon$ is a small positive number and $\eta$. $\eta$ determines how much to prioritize, so that the larger the $\eta$ the greater the prioritization. Prioritizing certain examples changes the expectation of the entire data distribution, which introduces bias into the training process. This can be corrected by multiplying the TD error for each example by a set of weights, which is called <b>importance sampling</b>.
</p>
<h2 id="summary">Summary</h2>
<p align="justify">
In this post I have tried to give a very short intro to the basic terminology of Reinforcement Learning. If you are interested in this field I encourage you to take a look a Barto and Suttons <a href="http://incompleteideas.net/book/the-book.html">Reinforcement Learning: An Introduction</a>) and the great resource <a href="https://spinningup.openai.com/en/latest/index.html">OpenAI Spinning Up</a> created by Josh Achiam.
</p>
<p align="justify">
Moreover we've seen a way to categorize the different algorithm families of RL. I summarized one policy gradient algorithm called REINFORCE (there exist way more out there) for you to give you a better understanding of the concept of policy learning. After that we explored two value-based algorithms in SARSA and DQN, and we looked at a few tricks to further improve the performce of DQN. In the next part of this series of posts, I'm going to dive deeper in a few more modern deep RL algorithms and combined methods we talked about.
</p>
<h2 id="references">References</h2>
<p><a href="http://incompleteideas.net/book/the-book.html">[1]</a> Barto and Sutton, Reinforcement Learning: An Introduction (2018).</p>
<p><a href="https://spinningup.openai.com/en/latest/index.html">[2]</a> Josh Achiam, OpenAI Spinning Up (2018).</p>
<p><a href="https://arxiv.org/pdf/1312.5602.pdf">[3]</a> Mnih et al., Playing Atari with Deep Reinforcement Learning (2013).</p>
<p><a href="https://arxiv.org/pdf/1509.06461.pdf">[4]</a> van Hasselt et al., Deep Reinforcement Learning with Double Q-Learning (2015).</p>
<p><a href="https://www.amazon.de/Deep-Reinforcement-Learning-Python-Hands/dp/0135172381">[5]</a> Graesser and Keng, Foundations of Deep Reinforcement Learning (2019).</p>
<p><a href="https://arxiv.org/pdf/1511.05952.pdf">[6]</a> Schaul et al., Prioritized Experience Replay (2016).</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
