<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Object Detection - From R-CNN to Mask R-CNN | Johanns Blog</title><meta name=keywords content="deep-learning,computer-vision,convolutional-neural-networks"><meta name=description content="Two-Stage Object Detection Models."><meta name=author content="Johann Gerberding"><link rel=canonical href=https://johanngerberding.github.io/posts/2021-04-03-object-detection-from-r-cnn-to-mask-rcnn/><link crossorigin=anonymous href=/assets/css/stylesheet.min.11908027a6b807331e0f24a59c74f5eb13c74727fbcfbad5684e068f178d4013.css integrity="sha256-EZCAJ6a4BzMeDySlnHT16xPHRyf7z7rVaE4GjxeNQBM=" rel="preload stylesheet" as=style><link rel=icon href=https://johanngerberding.github.io/favicon.ico><link rel=apple-touch-icon href=https://johanngerberding.github.io/apple-touch-icon.png><link rel=alternate hreflang=en href=https://johanngerberding.github.io/posts/2021-04-03-object-detection-from-r-cnn-to-mask-rcnn/><meta name=twitter:title content="Object Detection - From R-CNN to Mask R-CNN | Johanns Blog"><meta name=twitter:description content="Two-Stage Object Detection Models."><meta property="og:title" content="Object Detection - From R-CNN to Mask R-CNN | Johanns Blog"><meta property="og:description" content="Two-Stage Object Detection Models."><meta property="og:type" content="article"><meta property="og:url" content="https://johanngerberding.github.io/posts/2021-04-03-object-detection-from-r-cnn-to-mask-rcnn/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-04-03T10:23:16+02:00"><meta property="article:modified_time" content="2021-04-03T10:23:16+02:00"><meta property="og:site_name" content="Johann's Blog"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://johanngerberding.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Object Detection - From R-CNN to Mask R-CNN","item":"https://johanngerberding.github.io/posts/2021-04-03-object-detection-from-r-cnn-to-mask-rcnn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Object Detection - From R-CNN to Mask R-CNN | Johanns Blog","name":"Object Detection - From R-CNN to Mask R-CNN","description":"Two-Stage Object Detection Models.","keywords":["deep-learning","computer-vision","convolutional-neural-networks"],"wordCount":"2474","inLanguage":"en","datePublished":"2021-04-03T10:23:16+02:00","dateModified":"2021-04-03T10:23:16+02:00","author":{"@type":"Person","name":"Johann Gerberding"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://johanngerberding.github.io/posts/2021-04-03-object-detection-from-r-cnn-to-mask-rcnn/"},"publisher":{"@type":"Organization","name":"Johanns Blog","logo":{"@type":"ImageObject","url":"https://johanngerberding.github.io/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript></head><body class="type-posts kind-page layout-" id=top><script data-no-instant>function switchTheme(e){switch(e){case"light":document.body.classList.remove("dark");break;case"dark":document.body.classList.add("dark");break;default:window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")}}function isDarkTheme(){return document.body.className.includes("dark")}function getPrefTheme(){return localStorage.getItem("pref-theme")}function setPrefTheme(e){switchTheme(e),localStorage.setItem("pref-theme",e)}const toggleThemeCallbacks={};toggleThemeCallbacks.main=e=>{setPrefTheme(e?"light":"dark")},window.addEventListener("toggle-theme",function(){const e=isDarkTheme();for(const t in toggleThemeCallbacks)toggleThemeCallbacks[t](e)});function toggleThemeListener(){window.dispatchEvent(new CustomEvent("toggle-theme"))}</script><script>(function(){const t="light",e=getPrefTheme(),n=e||t;switchTheme(n)})()</script><header class=header><nav class=nav><div class=logo><a href=https://johanngerberding.github.io/ accesskey=h title="Johanns Blog (Alt + H)">Johanns Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://johanngerberding.github.io/posts/ title=posts class=active><i class='fa fa-heart'></i>posts</a></li><li><a href=https://johanngerberding.github.io/tags/ title=tags><i class='fa fa-heart'></i>tags</a></li><li><a href=https://johanngerberding.github.io/reading/ title=reading><i class='fa fa-heart'></i>reading</a></li><li><a href=https://johanngerberding.github.io/about-me/ title=about><i class='fa fa-heart'></i>about</a></li></ul></nav></header><main class="main post"><article class=post-single><header class=post-header><h1 class=post-title>Object Detection - From R-CNN to Mask R-CNN</h1><div class=post-meta><span class=meta-item><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg>
<span>April 3, 2021</span></span><span class=meta-item>
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select:text"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z" style="user-select:text"/><line x1="7" y1="7" x2="7" y2="7" style="user-select:text"/></svg>
<span class=post-tags><a href=https://johanngerberding.github.io/tags/deep-learning/>Deep-Learning</a><a href=https://johanngerberding.github.io/tags/computer-vision/>Computer-Vision</a><a href=https://johanngerberding.github.io/tags/convolutional-neural-networks/>Convolutional-Neural-Networks</a></span></span><span class=meta-item>
<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg>
<span>2474 words</span></span><span class=meta-item>
<svg width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>12 min</span></span></div></header><div class="toc side right"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#r-cnn aria-label=R-CNN>R-CNN</a></li><li><a href=#fast-r-cnn aria-label="Fast R-CNN">Fast R-CNN</a></li><li><a href=#faster-r-cnn aria-label="Faster R-CNN">Faster R-CNN</a></li><li><a href=#mask-r-cnn aria-label="Mask R-CNN">Mask R-CNN</a></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>¶</a></h2><p align=justify>In the following weeks (or months) I am going to take a deep dive into Deep Learning based Object Detection models. My goal is to create a series of posts regarding different approaches and popular architectures for this task. In this post I'll start with the description of the R-CNN ("Region-based Convolutional Neural Networks") model family, their emergence and central ideas. Nowadays there exist much more accurate and efficient architectures but I think it's a good starting point for such a series. Since the R-CNN based models are so called two-stage approaches, I'll go over a few popular one-stage architectures like YOLO in the next post as well. I'll describe in a minute what exactly this means and what the key differences are. Furthermore, following these two posts, I would like to discuss more current architectures that represent the state-of-the-art in the most recognized benchmarks like <i>EfficientDet</i> or new Transformer-based approaches like <i>DETR</i>.</p><p align=justify>As mentioned before, we can broadly distinguish between one and two-stage detection frameworks. A typical two-stage pipeline consists of an initial category-independent region-proposal stage followed by the feature extraction and classification. This allows for a high localization and recognition accuracy. In contrast single-stage object detectors do not require prior proposals which makes them faster but less accurate. But more on that in the next post. Now let's start with the R-CNN models.</p><h2 id=r-cnn>R-CNN<a hidden class=anchor aria-hidden=true href=#r-cnn>¶</a></h2><p align=justify>Ross Girshick et al. presented their approach called "Region-based Convolutional Neural Networks" (R-CNN) in 2014. It was one of the first methods based on deep convolutional networks (CNNs) besides e.g. <i>Overfeat</i>. The main idea is to tackle the problem of Object Detection in several successive steps, as shown in Figure 1.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_1/RCNN.png#center alt="R-CNN model workflow" width=100%><figcaption><p>Figure 1. R-CNN model workflow [1]</p></figcaption></figure><p align=justify>It was an important contribution to the Computer Vision community because of the significant improvement of the state-of-the-art at that time (mAP improvement of more than 30% on PASCAL VOC). Moreover the structure as well as the workflow of the framework are pretty straightforward:</p><ol><li>Extract around 2000 of category-independent bounding box object region candidates per image using <a href="https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&amp;bib=all.bib">selective search</a></li><li>Each region candidate gets warped to have a fixed size of 227x227 pixels which is required by the CNN (you can detailed information on how this is done in the Appendix of the paper)</li><li>Extract the features of each candidate with a CNN (AlexNet)</li><li>Classification of each region with class-specific Support Vector Machines (SVM)</li><li>Bounding box regression based on the predicted class, predict bounding box offsets (boosts up mAP by 3-4 points)</li></ol><p align=justify>The overall training procedure is a stepwise process and requires a lot of work. First you have to pretrain your CNN classification on ImageNet. In the next step you have to get rid of the last classification layer and insert a new one with $K+1$ classes ($K$ = number of classes ;+1 for background). Start finetuning this network using warped proposal windows. It is very important for training that you reduce your learning rate when finetuning (0.01 for ImageNet and 0.001 for finetuning). In the training process all proposals with an Intersection over Union (IoU) >= 0.5 are considered positive samples. The mini-batch size here was 128, consisting of 32 positive and 96 negative boxes, so its biased towards the positives (selective search produces much more negatives than positives). After finetuning your CNN we start building the class-specific binary SVMs. Here a the authors used grid search to choose the IoU threshold of 0.3. In addition to speed up the process they used hard negativ mining. If you want the details on why it is done this way you can look it up in the Appendix of the paper. The last step of the training procedure is the creation of class-specific bounding box regressors, which output bbox offsets. To train these only proposals with an IoU >= 0.6 are used.</p><p align=justify>Now let's dive a bit deeper into <b>Bounding Box Regression</b>. The offsets get calculated based on the features after $pool_{5}$ layer of each proposal $\mathbf{p}$. The regressor is build to learn scale-invariant transformations between the centers and a log-scale transformation between widths and heights. This is illustrated down below.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_1/RCNN-bbox-regression.png#center alt="Bounding Box Regression" width=80%><figcaption><p>Figure 2. R-CNN - Bounding Box Regression</p></figcaption></figure><p>$$
\hat{g}_{x} = p_{w}d_{x}(\mathbf{p}) + p_{x} \
\hat{g}_{y} = p_{h}d_{y}(\mathbf{p}) + p_{y} \
\hat{g}_{w} = p_{w}e^{d_{w}(\mathbf{p})} \
\hat{g}_{h} = p_{h}e^{d_{h}(\mathbf{p})}
$$</p><p>$\mathbf{p}$ represents the predicted bbox information $(p_{x}, p_{y}, p_{w}, p_{h})$ whereas $\mathbf{g}$ contains the ground truth values $(g_{x}, g_{y}, g_{w}, g_{h})$. The targets to learn are the following:</p><p>$$
t_{x} = (g_{x} - p_{x}) / p_{w} \
t_{y} = (g_{y} - p_{y}) / p_{h} \
t_{w} = \log (g_{w} / p_{w}) \
t_{h} = \log (g_{h} / p_{h})
$$</p><p align=justify>A regression model can solve the problem by minimizing the Sum of Squared Error Loss with regularization (ridge regression):</p><p>$$
L_{reg} = \sum_{i \in { x, y, w, h }} (t_{i} - d_{i}(\mathbf{p}))^{2} + \lambda | \mathbf{w} |^{2}
$$</p><p align=justify>The regularization term ($\lambda$ = 1000) is critical and the authors picked it by using cross validation. One benefit of the application of these transformations is that alle the box correction functions $d_{i}(p)$ where $i \in \{ x, y, w, h \}$ can take any value $[- \infty, + \infty]$.</p><p><strong>Shortcomings:</strong></p><ul><li>training is a multi-stage pipeline</li><li>training is expensive in space and time</li><li>speed bottleneck due to selective search and feature vector generation for every region proposal ($N$ images $*$ 2000)</li></ul><h2 id=fast-r-cnn>Fast R-CNN<a hidden class=anchor aria-hidden=true href=#fast-r-cnn>¶</a></h2><p align=justify>To overcome these shortcomings described above, Girshick improved the R-CNN training procedure by unifying the three models into one jointly trainable framework called <b>Fast R-CNN</b>. Instead of extracting feature vectors for every object proposal separately, here the entire image gets forward passed through a deep CNN (<a href=https://arxiv.org/pdf/1409.1556.pdf>VGG16</a>)) to produce a convolutional feature map. For each object proposal a <b>Region of Interest (RoI) pooling layer</b> extracts a fixed length feature vector from this feature map. Those feature vectors are then fed into multiple fully-connected layers which finally branch into the object classifier and a bounding box regressor. This intergration leads to a lot of shared computation which speeds up the whole prediction process.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_1/fast-RCNN.png#center alt="Fast R-CNN model workflow" width=100%><figcaption><p>Figure 3. Fast R-CNN architecture</p></figcaption></figure><p align=justify>One new key component of the proposed framework is the RoI pooling layer which is a type of max pooling that allows us to convert the features inside any valid region proposals into a feature map with a fixed window of size H $\times$ W. This is done by dividing the input region into H $\times$ W small grids where every subwindow size is approximately of size [h/H $\times$ w/W]. In each of those grid cells apply regular max pooling.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_1/roi_pooling.png#center alt="Region of Interest Pooling" width=100%><figcaption><p>Figure 4. Region of Interest Pooling</p></figcaption></figure><p align=justify>The training procedure of this framework is somewhat similar to the one of R-CNN. Here they also pre-train a deep CNN on ImageNet and use selective search for proposal generation. After pre-training the last max pooling layer is replaced by a RoI pooling layer which is set to be compatible with the nets first fully connected layer, for VGG16 H = W = 7. The last fully connected layer and softmax are replaced by two sibling layers, one for classification consisting of one fully connected layer followed by a softmax over $K+1$ categories and one for category-specific bounding box regressors. Another major advantage of this approach is the possibility of end-to-end training with a multi-task loss. This loss function sums up the cost of classification and bbox prediction:</p><p>$$
L(p,u,t^{u},v) = L_{cls}(p,u) + \lambda [u \geq 1] L_{loc}(t^{u}, v)
$$</p><table><thead><tr><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td>$p$</td><td>discrete probability distribution (per RoI) over K+1 classes, $p=(p_{0}, &mldr;, p_{K})$ computed by a softmax</td></tr><tr><td>$u$</td><td>ground-truth class label, $u = 1, &mldr;, K$ (background $u = 0$)</td></tr><tr><td>$t^{u}$</td><td>predicted bounding box offsets, $t^{u} = (t_{x}^{u},t_{y}^{u},t_{w}^{u},t_{h}^{u})$</td></tr><tr><td>$v$</td><td>true bouding box regression targets $v = (v_{x},v_{y},v_{w},v_{h})$</td></tr><tr><td>$\lambda$</td><td>hyperparameter to control the balance between the two losses</td></tr></tbody></table><p>The indicator function $[u \geq 1]$ is defined as</p><p>$$
[u \geq 1] =
\begin{cases}
1 & \quad \text{if } u \geq 1\
0 & \quad \text{otherwise}
\end{cases}
$$</p><p align=justify>to ignore background classifications. $L_{cls}(p,u) = - \log p_{u}$ is a log loss for the true class $u$. The bounding box loss is defined as followed:</p><p>$$
L_{box}(t^{u}, v) = \sum_{i \in {x,y,w,h}} smooth_{L_{1}} (t_{i}^{u} - v_{i})
$$</p><p align=justify>It measures the difference between $t_{i}^{u}$ and $v_{i}$ using a robust smooth $L_{1}$ loss function which is claimed to be less sensitive to outliers than the $L_{2}$ loss.</p><p>$$
L_{1}^{smooth}(x) =
\begin{cases}
0.5x^2 & \quad \text{if } |x| &lt; 1\
|x| - 0.5 & \quad \text{otherwise}
\end{cases}
$$</p><p align=justify>As mentioned before Fast R-CNN is much faster in training and testing but one speed bottleneck in form of selective search still remains which leads us to the next evolutionary stage of the architecture.</p><h2 id=faster-r-cnn>Faster R-CNN<a hidden class=anchor aria-hidden=true href=#faster-r-cnn>¶</a></h2><p align=justify>Here the region proposal step gets integrated into the CNN in form of a so called <b>Region Proposal Network</b> (RPN) that shares conv features with the detection network ("it tells the classification network where to look").</p><p align=justify>The RPN takes an image of arbitrary size as input and outputs a set of rectangular object proposals and objectness scores (= object vs. background). It is important to keep in mind that the goal here is to share computation, so a set of conv layers is chosen to be also part of the object detection pipeline to extract features.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_1/faster-RCNN.png#center alt="Faster R-CNN model" width=100%><figcaption><p>Figure 5. Faster R-CNN architecture (left) and RPN workflow (right)</p></figcaption></figure><p align=justify>To generate region proposals, a small $n \times n$ spatial window gets slided over the last shareable conv feature map to create lower dimensional feature vectors (256 dims). These reduced feature vectors get fed into two sibling fully connected layers, a bbox regression layer and a box classification layer. It is implemented using one $n \times n$ convolution and two $1 \times 1$ conv layers. The classification part is implemented as a two class softmax layer. At each sliding window location $k$ regions of various scales and ratios get proposed simultaneously (k = 9). Those proposals are parameterized relative to $k$ reference boxes called <b>anchors</b>. Each anchor is centered at the sliding window position and is associated with a combination of different aspect ratios (1:1, 1:2, 2:1) and scales (128x128, 256x256, 512x512). This multi-scale anchor-based design is a key component for sharing features without extra cost for addressing scales and aspect ratios.</p><p align=justify>The loss function used to train the RPN is also a multi-task loss which is similar to the one of Fast R-CNN we discussed before:</p><p>$$
L({p_{i}},{t_{i}}) = \frac{1}{N_{cls}} \sum L_{cls} (p_{i}, p_{i}^{\ast}) + \lambda \frac{1}{N_{reg}} \sum p_{i}^{\ast} L_{reg} (t_{i}, t_{i}^{\ast})
$$</p><table><thead><tr><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td>$p_{i}$</td><td>predicted probability that anchor $i$ is an object</td></tr><tr><td>$p_{i}^{\ast}$</td><td>ground-truth label (1 or 0)</td></tr><tr><td>$t_{i}$</td><td>predicted parameterized coordinates $(x,y,w,h)$</td></tr><tr><td>$t_{i}^{\ast}$</td><td>ground-truth parameterized coordinates $(x,y,w,h)$</td></tr><tr><td>$\lambda$</td><td>balancing parameter, set to be 10, to balance out $L_{reg}$ and $L_{cls}$</td></tr><tr><td>$N_{cls}$</td><td>normalization term, set to be the same as the mini batch size (256)</td></tr><tr><td>$N_{reg}$</td><td>normalization term, set to be approx. the number of anchor boxes ($\sim$ 2400)</td></tr></tbody></table><p>The regression loss is also smooth $L_{1}$ like in Fast R-CNN. The classification loss can be calculated as followed:</p><p>$$
L_{cls} (p_{i}, p_{i}^{\ast}) = -p_{i}^{\ast} \log p_{i} - (1 - p_{i}^{\ast}) \log (1 - p_{i})
$$</p><p align=justify>For the calculations of $t\_{i}$ and $t\_{i}^{\ast}$ take a look at section 3.1.2 of the <a href=https://arxiv.org/abs/1506.01497>paper</a>. To train the network the authors use a mini batch size of 256 randomly sampled anchors consisting of 128 positives and negatives (padded with negative ones if necessary). All anchors which are beyond the image boundaries are ignored in the training process. For the Fast R-CNN framework integration the authors experimented with different training strategies and chose an alternating one. First they train the RPN and then the Fast R-CNN using the RPN proposals and the trainede backbone. Thereafter they initialize the RPN with the tuned Fast R-CNN network and iterate a couple of times. For detailed evaluation and benchmarks take a look at the paper.</p><h2 id=mask-r-cnn>Mask R-CNN<a hidden class=anchor aria-hidden=true href=#mask-r-cnn>¶</a></h2><p align=justify>In the last step of this "evolution", Faster R-CNN was further extended to the task of pixel level instance segmentation, called <b>Mask R-CNN</b>. A third branch is added in parallel to the existing classification and localization branches for predicting binary object masks for every proposed RoI of size $m \times m$.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_1/mask-rcnn.png#center alt="Mask R-CNN model" width=100%><figcaption><p>Figure 6. Mask R-CNN architecture</p></figcaption></figure><p align=justify>The mask branch consists of a fully convolutional network (FCN) which allows each layer to maintain the explicit $m \times m$ object spatial layout without collapsing it into a vector representation that lacks those spatial dimensions. To ensure high quality and precision mask predictions the RoI features have to be well aligned to preserve the explicit per-pixel spatial correspondence. The RoI pooling layer lacks the required precision because of quantization which motivated the authors to develop the so called <b>RoI Align</b> layer. RoI Align avoids quantization and instead uses <a href=https://en.wikipedia.org/wiki/Bilinear_interpolation>Bilinear Interpolation</a> to compute the exact values of the input features at four regularly sampled locations in each RoI bin and aggregate the result (max or avg pooling). This leads to large improvements in mask prediction quality. To get a good understanding of how RoI Align works, take a look at the following <a href=https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193>blogpost</a>.</p><p align=justify>The authors also demonstrate the generality of the proposed framework by experimenting with different backbones (ResNet, ResNext, FPN, ...) and heads for prediction of bboxes, classification and mask prediction. Moreover, they show that the framework is able to predict keypoints with minimal adjustments. For details on this, take a look at the <a href=https://arxiv.org/abs/1703.06870>paper</a>. For training the multi-task loss of Faster R-CNN for each RoI gets extended by adding a mask loss:</p><p>$$
L_{mask} = -\frac{1}{m^{2}} \sum_{1 \leq i, j \leq m} [ y_{ij} \log \hat{y}_{ij}^{k} + (1-y_{ij}) \log (1 - \hat{y}_{ij}^{k}) ]
$$</p><p align=justify>$y_{ij}$ describes the label of a cell (i,j) in the true mask whereas $\hat{y}_{ij}^{k}$ represents the predicted value of cell (i,j). The loss is defined as the average binary cross entropy loss, only including the k-th mask if the region is associated with the ground truth class $k$. The mask branch outputs $K * m^{2}$ binary masks but only the k-th mask contributes to the loss (rest gets ignored). This provides a decoupling between class and mask predictions.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>¶</a></h2><p align=justify>To sum everything up and give a broad overview of all architectures covered in this post take a look at the following overview created by Lilian Weng.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_1/rcnn-family-summary.png#center alt="R-CNN model family" width=100%><figcaption><p>Figure 7. Overview R-CNN model family</p></figcaption></figure><p align=justify>For a general overview of the field of Deep Learning based Object Detection I highly recommend <a href=https://arxiv.org/pdf/1809.02165v1.pdf>this survey</a> ("Deep Learning for Generic Object Detection: A Survey") from 2018 which in my opinion gives a great overview for beginners.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>¶</a></h2><p><a href=https://arxiv.org/pdf/1311.2524.pdf>[1]</a> R. Girshick, J. Donahue, T. Darrell, J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation, 2014.</p><p><a href=https://arxiv.org/pdf/1504.08083.pdf>[2]</a> Ross Girshick. Fast R-CNN, 2015.</p><p><a href=https://arxiv.org/pdf/1506.01497.pdf>[3]</a> S. Ren, K. He, R. Girshick, J. Sun. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, 2016.</p><p><a href=https://arxiv.org/pdf/1703.06870.pdf>[4]</a> k. He, G. Gkioxari, P. Dollár, R. Girshick. Mask R-CNN, 2017.</p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://johanngerberding.github.io/posts/2021-05-02-object-detection-faster-models/><span class=title><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></svg>&nbsp;Prev Page</span><br><span>Object Detection - Faster Models</span>
</a><a class=next href=https://johanngerberding.github.io/posts/2020-12-18-overview-of-human-pose-estimation/><span class=title>Next Page&nbsp;<svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span><br><span>Overview - Human Pose Estimation</span></a></nav></footer><div class=comments-separator></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://johanngerberding.github.io/>Johanns Blog</a></span><span style=display:inline-block;margin-left:1em>
<a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA</a>
</span><span style=display:inline-block;margin-left:1em>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/reorx/hugo-PaperModX/ rel=noopener target=_blank>PaperModX</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>(function(){const t=""=="1";if(t)return;let e=document.getElementById("theme-toggle");e.removeEventListener("click",toggleThemeListener),e.addEventListener("click",toggleThemeListener)})()</script><script>(function(){let e=document.getElementById("menu");e&&(e.scrollLeft=localStorage.getItem("menu-scroll-position"),e.onscroll=function(){localStorage.setItem("menu-scroll-position",e.scrollLeft)});const t=""=="1",n=""=="1";if(window.matchMedia("(prefers-reduced-motion: reduce)").matches||t||n)return;document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})})()</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>if(window.scrollListeners)for(const e of scrollListeners)window.removeEventListener("scroll",e);window.scrollListeners=[]</script><script src=/js/medium-zoom.min.js data-no-instant></script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>(function(){const a="1"=="1";if(!a)return;if(!document.querySelector(".toc")){console.log("no toc found, ignore toc scroll");return}const r=window.scrollListeners,t=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id]"),n="active";let e=t[0];o(e).classList.add(n);const c=()=>{const s=[];for(const e of t)if(l(e)<5)s.push(e);else break;s.length>0?newActiveHeading=s[s.length-1]:newActiveHeading=t[0],e!=newActiveHeading&&(o(e).classList.remove(n),e=newActiveHeading,o(e).classList.add(n))};let s=null;const i=()=>{s!==null&&clearTimeout(s),s=setTimeout(c,50)};window.addEventListener("scroll",i,!1),r.push(i);function o(e){const t=encodeURI(e.getAttribute("id")).toLowerCase();return document.querySelector(`.toc ul li a[href="#${t}"]`)}function l(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect();return t.top}})()</script><script>mediumZoom(".entry-cover img"),mediumZoom(".post-content img:not([no-zoom])")</script></body></html>