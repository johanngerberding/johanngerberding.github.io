<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Parameter-Efficient LLM Finetuning | Johanns Blog</title>
<meta name="keywords" content="llm, finetuning, transformer">
<meta name="description" content="Finetuning of Large Language Models in a parameter efficient way.">
<meta name="author" content="Johann Gerberding">
<link rel="canonical" href="http://localhost:1313/posts/2023-09-08-parameter-efficient-llm-finetuning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.a0264e6a2df146fcf73b7783950d40c4be7e8efa44c2889964f6ec37304880d9.css" integrity="sha256-oCZOai3xRvz3O3eDlQ1AxL5&#43;jvpEwoiZZPbsNzBIgNk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2023-09-08-parameter-efficient-llm-finetuning/">

<meta name="twitter:title" content="Parameter-Efficient LLM Finetuning | Johanns Blog" />
<meta name="twitter:description" content="Finetuning of Large Language Models in a parameter efficient way." />
<meta property="og:title" content="Parameter-Efficient LLM Finetuning | Johanns Blog" />
<meta property="og:description" content="Finetuning of Large Language Models in a parameter efficient way." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/2023-09-08-parameter-efficient-llm-finetuning/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2023-09-08T10:00:00&#43;02:00" />
  <meta property="article:modified_time" content="2023-09-08T10:00:00&#43;02:00" /><meta property="og:site_name" content="Johann&#39;s Blog" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Parameter-Efficient LLM Finetuning",
      "item": "http://localhost:1313/posts/2023-09-08-parameter-efficient-llm-finetuning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Parameter-Efficient LLM Finetuning | Johanns Blog",
  "name": "Parameter-Efficient LLM Finetuning",
  "description": "Finetuning of Large Language Models in a parameter efficient way.",
  "keywords": [
    "llm", "finetuning", "transformer"
  ],
  "wordCount" : "1286",
  "inLanguage": "en",
  "datePublished": "2023-09-08T10:00:00+02:00",
  "dateModified": "2023-09-08T10:00:00+02:00",
  "author":{
    "@type": "Person",
    "name": "Johann Gerberding"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/2023-09-08-parameter-efficient-llm-finetuning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Johanns Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'light';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Johanns Blog (Alt + H)">Johanns Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="posts" class="active"
                ><i class='fa fa-heart'></i>posts
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags"
                ><i class='fa fa-heart'></i>tags
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/reading/" title="reading"
                ><i class='fa fa-heart'></i>reading
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about-me/" title="about"
                ><i class='fa fa-heart'></i>about
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Parameter-Efficient LLM Finetuning<sup><span class="entry-isdraft">&nbsp;&nbsp;[draft]</span></sup></h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>September 8, 2023</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select: text;"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z" style="user-select: text;"></path><line x1="7" y1="7" x2="7" y2="7" style="user-select: text;"></line></svg>
  <span class="post-tags"><a href="http://localhost:1313/tags/llm/">Llm</a><a href="http://localhost:1313/tags/finetuning/">Finetuning</a><a href="http://localhost:1313/tags/transformer/">Transformer</a></span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select: text;"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" style="user-select: text;"></path><polyline points="14 2 14 8 20 8" style="user-select: text;"></polyline><line x1="16" y1="13" x2="8" y2="13" style="user-select: text;"></line><line x1="16" y1="17" x2="8" y2="17" style="user-select: text;"></line><polyline points="10 9 9 9 8 9" style="user-select: text;"></polyline></svg>
  <span>1286 words</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>7 min</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#prefix-tuning" aria-label="Prefix Tuning">Prefix Tuning</a></li>
                <li>
                    <a href="#adapters" aria-label="Adapters">Adapters</a></li>
                <li>
                    <a href="#low-rank-adaptation-lora" aria-label="Low Rank Adaptation (LoRA)">Low Rank Adaptation (LoRA)</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#notes" aria-label="Notes">Notes</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">¶</a></h2>
<p align="justify">
I am already quite late to the Large Language Models (LLM) party but better starting late than never. In this post I am going over a couple of popular techniques for fine-tuning these models without the need of training the whole thing which would be quite expensive. But before I dive into the techniques we should talk about what LLMs and fine-tuning are, why we need it and what the current problems are.
</p>
<p align="justify">
LLMs are based on the <a href="http://jalammar.github.io/illustrated-transformer/">Transformer</a> architecture, like the GPTs or BERT, which have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks, like Translation, Question Answering, Summarization etc. The paradigm these days is to train such a model on generic web-scale data (basically the whole internet) and fine-tune it on a downstream task. Fine-tuning in this case just means that you train the model further with a small dataset you collected for a specific task. This fine-tuning results in most cases in huge performance gains when compared to using just the LLM as is (e.g. zero-shot inference).  
</p>
<p align="justify">
However, based on the current model sizes a full fine-tuning becomes infeasible to train on consumer hardware (which makes me a bit sad). In addition when you want to store and deploy multiple fine-tuning model instances for different tasks, this becomes very expensive because they are the same size as the base LLM. Because of these two main problems, people came up with more efficient methods for doing this which are referred to as Parameter-efficient fine-tuning (PEFT) procedures. These approaches basically enable you to get performance comparable to full fine-tuning while only having a small number of trainable parameters.  
</p>
<p align="justify">
PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. They have also been shown to be better than fine-tuning in cases when you don't have that much task data and in out-of-domain scenarios. By saving just the extra model parameters you also solve the portability and cost problem because the PEFT checkpoints are much smaller, just a few MB, in contrast to the LLM checkpoints that need multiple GB. The small trained weights from PEFT approaches are added on top of the pretrained LLM. So the same LLM can be used for multiple tasks by adding small weights without having to replace the entire model. huggingface provides a nice <a href="https://github.com/huggingface/peft">library</a> for a lot of PEFT methods. 
</p>
<p align="justify">
In the following I will going to dive a bit deeper in how some of these methods work. We will cover Prefix Tuning, Adapters and Low Rank Adaptation (LoRA).
</p>
<h2 id="prefix-tuning">Prefix Tuning<a hidden class="anchor" aria-hidden="true" href="#prefix-tuning">¶</a></h2>
<p align="justify">
There exists different forms of prompt tuning, one of them is hard prompt tuning, also called prompt engineering or in context learning, where you change the input to the model, e.g. provide examples for how the model should answer. This form of tuning is non-differentiable, so the weights don't change and you don't add any weights. Prefix tuning is a form of soft prompt tuning where you concatenate a trainable tensor to the embeddings of the input tokens. This tensor is trainable, so you add a small amount of weights. The model parameters are kept frozen and you just optimize a small continuous task-specific vector which is called prefix.  
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/parameter_efficient_llm_finetuning/finetuning_vs_prefix_tuning.png#center"
         alt="Finetuning vs. Prefix Tuning" width="70%"/> <figcaption>
            <p>Figure 1. Finetuning vs. Prefix Tuning</p>
        </figcaption>
</figure>

<p align="justify">
Prefix tuning shines especially in low-data settings and the extrapolation to new tasks, e.g. summarization of texts with different topics, is better. It has up to 1000x fewer parameters to learn than in a fine-tuning setting. Another very cool thing is, that it enables personalization by having a different prefix per user trained only on the user data (no cross-contamination) and you could do batch processing of multiple users/tasks and one LLM. 
</p>
<h2 id="adapters">Adapters<a hidden class="anchor" aria-hidden="true" href="#adapters">¶</a></h2>
<p align="justify">
Adapter methods are somewhat related to prefix tuning as they also add additional trainable parameters to the original LLM. Instead of prepending prefixes to the input embeddings, you add adapter layers into the transformer blocks. As you can see in the figure down below, the fully connected network in the adapter module has a bottleneck structure similar to an autoencoder which keeps the number of added parameters low and makes the method quite efficient.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/parameter_efficient_llm_finetuning/adapter_module_layout.png#center"
         alt="Adapter Module Layout" width="70%"/> <figcaption>
            <p>Figure 2. Adapter Module Layout</p>
        </figcaption>
</figure>

<p align="justify">
In the orginal adapter paper, the authors trained a BERT model with this method and reached a modeling performance comparable to a fully finetuned BERT model while only requiring the training of 3.6% of the parameters. Based on the original prefix tuning paper, the performance of adapters matches the performance of prefix tuning of 0.1% of the parameters at 3%, which makes it less efficient.
</p>
<p align="justify">
Llama-Adapter combines the two ideas of prefix tuning and adapters for LLaMA models from Meta. Each transformer block in the model has its own distinct learned prefix, allowing for more tailored adaptation across different model layers. On top of that, LLaMA-Adapter introduces a zero-initialized attention mechanism coupled with gating.
</p>
<h2 id="low-rank-adaptation-lora">Low Rank Adaptation (LoRA)<a hidden class="anchor" aria-hidden="true" href="#low-rank-adaptation-lora">¶</a></h2>
<p align="justify">
LoRA and QLoRA 
</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">¶</a></h2>
<p align="justify">
</p>
<h2 id="notes">Notes<a hidden class="anchor" aria-hidden="true" href="#notes">¶</a></h2>
<p align="justify">
- in-context learning -> models are able to perform tasks by providing them the right context (examples of your task) -> this is cool when you only have access to the model via an API or UI (this only works with generative models like the GPTs) 
- this works fine in some cases but for more specific things you still have to gather a dataset for the task and the specific domain and then finetune that model, this would result in superior results than just in-context learning (I don't know why they call this learning)
- what does finetuning even mean here? -> it basically means that you want to train the model on your task specific dataset 
- there are two ways to do this: 
1. the feature based approach, where you keep the transformer weights frozen (no training here) and just train a classifier which you feed the output embeddings of the frozen LLM (e.g. logistic regression model, random forest or XGBoost)
2. finetuning, where you train the whole model or a bigger portion of it (e.g. your just train the output layers after the Transformer blocks) 
- in general training the whole model will give you the best results but it is also the most expensive one    
- the problem is, that the currently best performing models are so big that there is no way that you can finetune this thing on your personal computer anymore
- so the question of how to utilize them more efficiently and effectively has become a very active area 
- people want to run those models on their laptops or desktop pcs without buying multiple graphics cards 
- so what do you do when in-context learning doesnt cut it for your use case and you don't have the compute to finetune those bigger models?
- researchers developed several techniques to make the finetuning of LLMs much more efficient by training just a small number of parameters
- I try to explain some of the most popular parameter-efficient finetuning techniques (PEFT) in this blogpost: Prefix Tuning, Adapters, Low Rank Adaptation (LoRA)
<p>and Baogao et. al wrote a paper comparing a lot of these methods and introduced HiWi</p>
</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">¶</a></h2>
<p>Sebastian Raschka - Understanding Parameter Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters (2023)
<a href="https://lightning.ai/pages/community/article/understanding-llama-adapters/">https://lightning.ai/pages/community/article/understanding-llama-adapters/</a></p>
<p>Sebastian Raschka - Parameter-Efficient LLM Finetuning With Low Rank Adaptation (2023)
<a href="https://lightning.ai/pages/community/tutorial/lora-llm/">https://lightning.ai/pages/community/tutorial/lora-llm/</a></p>
<p>Edward Hu et. al - LoRA: Low-Rank Adaptation of Large Language Models (2021)
<a href="https://arxiv.org/pdf/2106.09685.pdf">https://arxiv.org/pdf/2106.09685.pdf</a></p>
<p>Renrui Zhang et. al - LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (2023)
<a href="https://arxiv.org/pdf/2303.16199.pdf">https://arxiv.org/pdf/2303.16199.pdf</a></p>
<p>Xiang Lisa Li - Prefix-Tuning: Optimizing Continuous Prompts for Generation (2021)
<a href="https://arxiv.org/pdf/2101.00190.pdf">https://arxiv.org/pdf/2101.00190.pdf</a></p>
<p>Tim Dettmers et. al - QLORA: Efficient Finetuning of Quantized LLMs (2023)
<a href="https://arxiv.org/pdf/2305.14314.pdf">https://arxiv.org/pdf/2305.14314.pdf</a></p>
<p>Parameter-Efficient Fine-Tuning without Introducing New Latency (2023)
<a href="https://arxiv.org/pdf/2305.16742.pdf">https://arxiv.org/pdf/2305.16742.pdf</a></p>


  </div>

  <footer class="post-footer">
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/2024-04-23-retrieval-augmented-text-generation/">
    <span class="title">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select: text;"><line x1="19" y1="12" x2="5" y2="12" style="user-select: text;"></line><polyline points="12 19 5 12 12 5" style="user-select: text;"></polyline></svg>&nbsp;Prev Page</span>
    <br>
    <span>Retrieval-Augmented Text Generation (RAG)</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-2/">
    <span class="title">Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12" style="user-select: text;"></line><polyline points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg>
    </span>
    <br>
    <span>A Peek into Deep Reinforcement Learning - Part II</span>
  </a>
</nav>

  </footer>
    <div class="comments-separator"></div>
</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2024 <a href="http://localhost:1313/">Johanns Blog</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied!';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>
<script>
  mediumZoom('.entry-cover img');
  mediumZoom('.post-content img:not([no-zoom])');
</script>

</body>

</html>
