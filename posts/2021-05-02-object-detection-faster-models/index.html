<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Object Detection - Faster Models | Johanns Blog</title><meta name=keywords content><meta name=description content="One-Stage Object Detection Models."><meta name=author content="Johann Gerberding"><link rel=canonical href=https://johanngerberding.github.io/posts/2021-05-02-object-detection-faster-models/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://johanngerberding.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://johanngerberding.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://johanngerberding.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://johanngerberding.github.io/apple-touch-icon.png><link rel=mask-icon href=https://johanngerberding.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:title" content="Object Detection - Faster Models"><meta property="og:description" content="One-Stage Object Detection Models."><meta property="og:type" content="article"><meta property="og:url" content="https://johanngerberding.github.io/posts/2021-05-02-object-detection-faster-models/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-02T10:45:16+02:00"><meta property="article:modified_time" content="2021-05-02T10:45:16+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Object Detection - Faster Models"><meta name=twitter:description content="One-Stage Object Detection Models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://johanngerberding.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Object Detection - Faster Models","item":"https://johanngerberding.github.io/posts/2021-05-02-object-detection-faster-models/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Object Detection - Faster Models","name":"Object Detection - Faster Models","description":"One-Stage Object Detection Models.","keywords":[],"articleBody":"Introduction In the previous post we have reviewed region-based object detection algorithms (R-CNN models). In the following post I will dive a bit deeper into fast one-stage detection models like YOLO and RetinaNet which are more suited for certain applications with real-time requirements. The models I'm going to talk about here are a bit outdated and don't necessarily correspond to the state-of-the-art in this area anymore. Nevertheless, I find the general development in this area very interesting and the algorithms presented here form the basis for the current state-of-the-art. At that time, two-stage detectors were usually ahead of single-stage detectors in terms of accuracy, which is no longer the case today. In my next post I will go into more detail about state-of-the-art models such as EfficientDet and YOLOv4. YOLO As mentioned before, two stage detection models like Faster R-CNN are region based and considered to slow for certain applications that require real-time capabilities, e.g. in the robotics area or autonomous driving. So let's start with YOLO (\"You Only Look Once\") which was one of the first approaches to building a fast real-time object detector. Instead of relying on region proposals the authors reframed the object detection as a single regression problem, predicting bounding boxes and class probabilities directly from the images (therefore the name).  Figure 1. YOLO network architecture\n  This makes the whole system (Figure 1) fairly simple (single ConvNet) and very fast (45 fps). Since the model uses features from the entire image to predict the boxes it reasons globally. How it works The input image gets divided into an $S \\times S$ grid, where each grid cell predicts $B$ bounding boxes and confidence scores ($S=7$, $B=2$). If the center of an object falls into a grid cell than this grid cell is \"responsible\" for the detection. Each bounding box consists of 5 predictions: x_center, y_center, width, height and confidence. The x and y coordinates are relative to the bounds of a grid cell. The width and height are relative to the image. So all predicted values are between 0 and 1. In addition each grid cell also predicts $C$ class probabilities which are conditional on the grid cell containing an object (for PascalVOC: $C=20$). These values encode the probabilities of that class appearing in the box and how well the predicted box fits the object. One of the main limitations of this approach is the fact that each grid cell can only contain one object (max: 49 objects per image). Training Now let's talk about training the YOLO model. First the convolutional layers get pretrained on ImageNet for a week with an image input size of 224x224. Thereafter to finetune the network on the detection task, four convolutional layers and two fully connected layers get added and the image size is increased to 448x448. As activation LeakyReLU is applied. The loss function is Sum-Squared Error (SSE) consisting of two parts: localization and classification loss: $$ L_{loc} = \\lambda_{coord} \\sum_{i=0}^{S^{2}} \\sum_{j=0}^{B} \\mathbb{1}_{ij}^{obj} [(x_{i} - \\hat{x}_{i})^{2} + (y_{i} - \\hat{y}_{i})^{2} + (\\sqrt{w_{i}} - \\sqrt{\\hat{w}_{i}})^{2} + (\\sqrt{h_{i}} - \\sqrt{\\hat{h}_{i}})^{2}] $$\n$$ L_{cls} = \\sum_{i=0}^{S^{2}} \\sum_{j=0}^{B} (\\mathbb{1}_{ij}^{obj} \\lambda_{noobj}(1 - \\mathbb{1}_{ij}^{obj})) (C_{i} - \\hat{C}_{i})^{2} + \\sum_{i=0}^{S^{2}} \\sum_{C=C} \\mathbb{1}_{i}^{obj} (p_{i}(c) - \\hat{p}_{i}(c))^{2} $$\n$$ L = L_{cls} + L_{loc} $$\n   Symbol Description     $\\mathbb{1}_{i}^{obj}$ indicator of whether the cell $i$ contains an object   $\\mathbb{1}_{ij}^{obj}$ ground-truth class label, $u = 1, …, K$ (background $u = 0$)   $C_{i}$ confidence score of cell $i$, $Pr(contains object) * IoU (pred, truth)$   $\\hat{C}_{i}$ predicted confidence score (box with higher IoU of the two predicted boxes)   $\\mathcal{C}$ set of all classes (Pascal VOC: 20)   $p_{i}$ conditional probability of whether cell $i$ contains an object of class $c \\in \\mathcal{C}$   $\\hat{p}_{i}$ predicted conditional class probabilities   $S^{2}$ grid size, here $S=7$   $B$ number of predicted bounding boxes per grid cell    Because of model instability due to the inbalance between cells containing or not containing objects the authors use two scale parameters to increase the loss from bounding box predictions ($\\lambda_{coord} = 0.5$) and decrease the loss from confidence predictions for boxes that don't contain objects ($\\lambda_{noobj} = 0.5$). The loss function only penalizes classification error if an object is present in that grid cell and it only penalizes bbox error if the cell is \"responsible\" for the ground truth box. Some more training details: The authors trained YOLO on VOC 2007 and VOC 2012 with a batch size of 64, momentum of 0.9 and a weight decay of 0.0005. For regularization they rely on dropout and data augmentation. Shortcomings  strong spatial constraints since we have only one prediction per grid cell (7x7 - max. 49 object predictions); this is one of the reasons why the model struggles with crowds of small objects struggles to generalize to objects in new or unusual aspect ratios or configurations (maybe this could be reduced with clever data augmentation or training on different image scales) many incorrect localizations due to an inappropriate loss function and coarse features for bounding box prediction (multiple downsampling layers)  YOLOv2 YOLOv2 is basically an improved version of YOLO, adding some tricks to overcome its shortcomings described before. Moreover the paper covers YOLO9000 which is built on top of YOLOv2 and trained with a joint dataset combining COCO and the top 9000 classes of ImageNet (combination of detection and classification). I will only cover YOLOv2 here, for those of you who are interested in YOLO9000 and the joint training procedure, should take a look a the paper. As mentioned before, the central goal of YOLOv2 was to fix the problems of YOLO, primarily recall and localization shortcomings. The authors did this based on a variety of new ideas in the field (at that time) and they try to avoid increasing the model size at the same time to preserve the high speed: Batch Normalization: This leads to significant improvements in convergence while eliminating the need for other forms of regularization like dropout (+2% mAP).\nHigh Resolution Classifier: Finetune the classification network at higher resolution (448x448) for 10 epochs on ImageNet before detection finetuning.\nConvolutional Anchor Box Detection: The fully connected layers from YOLO are removed and instead YOLOv2 incorporates anchor boxes (like Faster R-CNN) to predict the bounding boxes; this also decouples the class prediction from the spatial location by predicting class and objectness for every anchor box which leads to a slight decrease in accuracy (-0.3% mAP) but increases recall significantly (+7%) which gives the model more room to improve.\nBox Dimension Clustering: Instead of using hand picked anchor box sizes, YOLOv2 runs k-means clustering on the training data to determine good priors for anchor box dimensions; to maximize IoU scores, it relies on the following distance metric:\n$$ d(x, c_{i}) = 1 - IoU(x, c_{i}), \\quad i=i,…k $$\nwhere $x$ is a ground truth box candidate and $c_{i}$ is one of the centroids / the closest centroid.\nDirect Location Prediction: In Region Proposal Networks the box location prediction is unconstrained which means any anchor box can end up at any point in the image which can lead to an unstable training. YOLOv2 follows the approach of the original YOLO model by predicting location coordinates relative to the location of the grid cell (using a logistic activation). Given the anchor box width $p_{w}$ and height $p_{h}$ in the grid cell with the top left corner ($c_{x}, c_{y}$) the model predicts 5 values ($t_{x}, t_{y}, t_{w}, t_{h}, t_{o}$) which correspond to the following box values:\n$$ b_{x} = \\sigma (t_{x}) + c_{x} \\ b_{y} = \\sigma (t_{y}) + c_{y} \\ b_{w} = p_{w}e^{t_{w}} \\ b_{h} = p_{h}e^{t_{h}} \\ Pr(obj) =IoU(b, obj) = \\sigma (t_{o}) $$\nThis in combination with clustering priors improves mAP by up to 5%.\n Figure 2. YOLOv2 bounding box prediction\n  Fine-grained Features: The grid size of the final feature map of YOLOv2 is increased from 7x7 in YOLO to 13x13. Moreover YOLOv2 incorporates a so called passthrough layer that brings features from an earlier layer at 26x26 resolution to the output layer. This process can be compared with identity mappings from ResNets to incorporate higher dimensional features (+1% mAP).\nMulti-scale Training: To increase the robustness of the model the authors trained it on images of different sizes. Every 10 batches the input size gets randomly sampled (between 320x320 and 608x608).\nTo maintain the high inference speed, YOLOv2 is based on the Darknet-19 model, consisting of 19 convolutional and 5 max-pooling layers. For detailed information on the architecture check out Table 6 in the paper. RetinaNet Next up in our list of fast detection models is RetinaNet. The creators had the goal of closing the accuracy gap between one and two-stage detection approaches. To achieve this, RetinaNet relies on two crucial building blocks, Feature Pyramid Networks (FPN) as a backbone and a new loss function called Focal Loss. Focal Loss The central cause for the accuracy gap between the two approaches lies in the extreme foreground-background class imbalance during training. In two-stage detectors this problem is addressed by narrowing down the number of candidate object locations (filtering out many background samples) and by using sampling heuristics like a fixed foreground-to-background ratio or online hard example mining. The proposed Focal Loss is designed to address this issue for one-stage detectors by focusing on hard negatives and down-weighting the easier predictions (obvious empty background). It is based on the normal cross entropy loss (for simplicity we use binary loss down here) $$CE(p,y) = -y \\log p - (1-y) \\log (1 - p) $$\nwhere $y={0,1}$ is a ground truth binary label, indicating whether a bounding box contains an object and $p \\in [0,1]$ is the predicted probability that there is an object (also called objectness or confidence score). For notational convenience, let $$ p_{t} = \\begin{cases} p \u0026 \\quad \\text{if } y=1\\ 1-p \u0026 \\quad \\text{otherwise} \\end{cases} $$\nwhich leads to\n$$ CE(p_{t}) = - \\log p_{t} $$\nEasily classified negatives ($p\\_{t} \\gg 0.5 ,y=0$) comprise the majority of the loss. You can balance the importance of the positive/negative examples by adding a balancing factor $\\alpha$ $$ CE(p_{t}) = - \\alpha_{t} \\log p_{t} $$\nbut this does not differentiate between easy or hard examples. To overcome this the Focal Loss adds a modulating factor $(1-p\\_{t})^{\\gamma}$ with a tunable focusing parameter $\\gamma \\geq 0$: $$ FL(p_{t}) = - (1 - p_{t})^{\\gamma} \\log (p_{t}) $$\n Figure 3. Focal Loss with different gamma values\n  For better control of the shape of the weighting function the authors used an $\\alpha$-balanced version in practice, where $\\alpha = 0.25$ and $\\gamma = 2$ worked best in their experiments: $$ FL(p_{t}) = - \\alpha (1 - p_{t})^{\\gamma} \\log (p_{t}) $$\nFeature Pyramid Network The FPN backbone for RetinaNet was constructed on top of ResNet. To really understand what that means you should take a look at the paper. Figure 4 down below shows the fundamental idea of FPN which is to leverage a ConvNets pyramidal feature hierarchy to build a feature pyramid with high level semantics throughout. It is general purpose and can be applied to many convolutional backbone architectures.  Figure 4. Featurized Pyramid Network architecture\n  The basic structure contains a sequence of pyramid levels each corresponding to one network stage. Often these stages contain multiple conv layers of the same size and stage sizes are scaled down by a factor of two. $C\\_{i}$ represents the different layers of those stages (for ResNet e.g. {$C\\_{2}, C\\_{3}, C\\_{4}, C\\_{5}$}). As you can see there are two different pathways which connect the conv layers:  Bottom-up pathway: regular feedback path Top-down pathway: goes in the opposite direction, adding coarse but semantically stronger feature maps back into the previous levels of layer size by lateral connections (1x1 conv to match dimensions) and nearest neighbor upsampling; the combination of the two maps is done by element-wise addition  The final predictions ({$P\\_{i}$} where $i$ indicates the pyramid level and has resolution $2^{i}$ lower than the input) are generated out of every merged map by a 3x3 conv layer. RetinaNet utilizes feature pyramid levels $P\\_{3}$ to $P\\_{7}$ computed from the corresponding ResNet residual stage from $C\\_{3}$ to $C\\_{5}$. All pyramid levels have 256 channels (most of RetinaNet is similar to FPN with a few minor differences). The authors used translation-invariant anchor boxes as priors, similar to those used in RPN variant of FPN. To improve Average Precision the number of anchors was increased to $A=9$ (three aspect ratios {1:2, 1:1, 2:1} with three different sizes {$2^{0}, 2^{1/3}, 2^{2/3}$}). As seen before, for each anchor box the model predicts a class probability for each of $K$ classes with a classification subnet trained with Focal Loss. A box regression subnet outputs the offsets for the boxes to the nearest ground truth object. Both networks are independent Fully Convolutional Networks that don't share any parameters.  Figure 5. RetinaNet architecture\n  YOLOv3 YOLOv3 was created by applying changes to YOLOv2 inspired by, at that time, recent advances in the object detection world. It's a pretty short and rather unscientifically (I like it :D ) written. The following list summarizes the most important improvements:  Logistic Regression for objectness scores instead of sum of squared errors Independent Logistic Classifiers for class prediction instead of softmax which increases the performance on non mutually exclusive multilabel datasets like Open Images Multi-scale predictions inspired by FPN (3 scales per stage) Darknet-53 as Feature Extractor which performs similar to ResNet-152 but is 2x faster  Overall YOLOv3 performs better and faster than SSD, worse then RetinaNet but is 3.8x faster and comparable to state-of-the-art methods on the $AP\\_{50}$ metric at that time. In the appendix Joseph (the author) adds a cool comment on his opinion about the COCO evaluation metrics. It's refreshing to see someone questioning stuff like this.  Figure 6. YOLOv3 performance\n  Summary In this blog post, we went over four popular but now somewhat aging fast object recognition systems and you got a first introduction to the world of real-time object recognition. In the next post I would like to talk about some more recent models like EfficientDet and YOLOv4. References [1] Joseph Redmon, et al. “You only look once: Unified, real-time object detection.” CVPR 2016.\n[2] Joseph Redmon and Ali Farhadi. “YOLO9000: Better, Faster, Stronger.” CVPR 2017.\n[3] Joseph Redmon, Ali Farhadi. “YOLOv3: An incremental improvement.”.\n[4] Tsung-Yi Lin, et al. “Feature Pyramid Networks for Object Detection.” CVPR 2017.\n[5] Tsung-Yi Lin, et al. “Focal Loss for Dense Object Detection.” IEEE transactions on pattern analysis and machine intelligence, 2018.\n","wordCount":"2394","inLanguage":"en","datePublished":"2021-05-02T10:45:16+02:00","dateModified":"2021-05-02T10:45:16+02:00","author":{"@type":"Person","name":"Johann Gerberding"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://johanngerberding.github.io/posts/2021-05-02-object-detection-faster-models/"},"publisher":{"@type":"Organization","name":"Johanns Blog","logo":{"@type":"ImageObject","url":"https://johanngerberding.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://johanngerberding.github.io/ accesskey=h title="Johanns Blog (Alt + H)">Johanns Blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://johanngerberding.github.io/about-me/ title=about><span><i class="fa fa-heart"></i>about</span></a></li><li><a href=https://johanngerberding.github.io/posts/ title=posts><span><i class="fa fa-heart"></i>posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Object Detection - Faster Models</h1><div class=post-meta><span title="2021-05-02 10:45:16 +0200 +0200">May 2, 2021</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Johann Gerberding</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#yolo aria-label=YOLO>YOLO</a><ul><li><a href=#how-it-works aria-label="How it works">How it works</a></li><li><a href=#training aria-label=Training>Training</a></li><li><a href=#shortcomings aria-label=Shortcomings>Shortcomings</a></li></ul></li><li><a href=#yolov2 aria-label=YOLOv2>YOLOv2</a></li><li><a href=#retinanet aria-label=RetinaNet>RetinaNet</a><ul><li><a href=#focal-loss aria-label="Focal Loss">Focal Loss</a></li><li><a href=#feature-pyramid-network aria-label="Feature Pyramid Network">Feature Pyramid Network</a></li></ul></li><li><a href=#yolov3 aria-label=YOLOv3>YOLOv3</a></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p align=justify>In the previous <a href=https://johanngerberding.github.io/johannsblog/Object-Detection-From-R-CNN-to-Mask-RCNN>post</a> we have reviewed region-based object detection algorithms (R-CNN models). In the following post I will dive a bit deeper into fast one-stage detection models like YOLO and RetinaNet which are more suited for certain applications with real-time requirements. The models I'm going to talk about here are a bit outdated and don't necessarily correspond to the state-of-the-art in this area anymore. Nevertheless, I find the general development in this area very interesting and the algorithms presented here form the basis for the current state-of-the-art. At that time, two-stage detectors were usually ahead of single-stage detectors in terms of accuracy, which is no longer the case today. In my next post I will go into more detail about state-of-the-art models such as <a href=https://arxiv.org/pdf/1911.09070.pdf>EfficientDet</a> and <a href=https://arxiv.org/pdf/2004.10934.pdf>YOLOv4</a>.</p><h2 id=yolo>YOLO<a hidden class=anchor aria-hidden=true href=#yolo>#</a></h2><p align=justify>As mentioned before, two stage detection models like Faster R-CNN are region based and considered to slow for certain applications that require real-time capabilities, e.g. in the robotics area or autonomous driving. So let's start with YOLO ("You Only Look Once") which was one of the first approaches to building a fast real-time object detector. Instead of relying on region proposals the authors reframed the object detection as a single regression problem, predicting bounding boxes and class probabilities directly from the images (therefore the name).</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_2/yolo-network-architecture.png#center alt="YOLO model architecture" width=100%><figcaption><p>Figure 1. YOLO network architecture</p></figcaption></figure><p align=justify>This makes the whole system (Figure 1) fairly simple (single ConvNet) and very fast (45 fps). Since the model uses features from the entire image to predict the boxes it reasons globally.</p><h3 id=how-it-works>How it works<a hidden class=anchor aria-hidden=true href=#how-it-works>#</a></h3><p align=justify>The input image gets divided into an $S \times S$ grid, where each grid cell predicts $B$ bounding boxes and confidence scores ($S=7$, $B=2$). If the center of an object falls into a grid cell than this grid cell is "responsible" for the detection. Each bounding box consists of 5 predictions: <i>x_center</i>, <i>y_center</i>, <i>width</i>, <i>height</i> and <i>confidence</i>. The x and y coordinates are relative to the bounds of a grid cell. The width and height are relative to the image. So all predicted values are between 0 and 1. In addition each grid cell also predicts $C$ class probabilities which are conditional on the grid cell containing an object (for PascalVOC: $C=20$). These values encode the probabilities of that class appearing in the box and how well the predicted box fits the object. One of the main limitations of this approach is the fact that each grid cell can only contain one object (max: 49 objects per image).</p><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p align=justify>Now let's talk about training the YOLO model. First the convolutional layers get pretrained on ImageNet for a week with an image input size of 224x224. Thereafter to finetune the network on the detection task, four convolutional layers and two fully connected layers get added and the image size is increased to 448x448. As activation LeakyReLU is applied. The loss function is Sum-Squared Error (SSE) consisting of two parts: localization and classification loss:</p><p>$$
L_{loc} = \lambda_{coord} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_{ij}^{obj} [(x_{i} - \hat{x}_{i})^{2} + (y_{i} - \hat{y}_{i})^{2} + (\sqrt{w_{i}} - \sqrt{\hat{w}_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}]
$$</p><p>$$
L_{cls} = \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} (\mathbb{1}_{ij}^{obj} \lambda_{noobj}(1 - \mathbb{1}_{ij}^{obj})) (C_{i} - \hat{C}_{i})^{2} + \sum_{i=0}^{S^{2}} \sum_{C=C} \mathbb{1}_{i}^{obj} (p_{i}(c) - \hat{p}_{i}(c))^{2}
$$</p><p>$$
L = L_{cls} + L_{loc}
$$</p><table><thead><tr><th>Symbol</th><th>Description</th></tr></thead><tbody><tr><td>$\mathbb{1}_{i}^{obj}$</td><td>indicator of whether the cell $i$ contains an object</td></tr><tr><td>$\mathbb{1}_{ij}^{obj}$</td><td>ground-truth class label, $u = 1, &mldr;, K$ (background $u = 0$)</td></tr><tr><td>$C_{i}$</td><td>confidence score of cell $i$, $Pr(contains object) * IoU (pred, truth)$</td></tr><tr><td>$\hat{C}_{i}$</td><td>predicted confidence score (box with higher IoU of the two predicted boxes)</td></tr><tr><td>$\mathcal{C}$</td><td>set of all classes (Pascal VOC: 20)</td></tr><tr><td>$p_{i}$</td><td>conditional probability of whether cell $i$ contains an object of class $c \in \mathcal{C}$</td></tr><tr><td>$\hat{p}_{i}$</td><td>predicted conditional class probabilities</td></tr><tr><td>$S^{2}$</td><td>grid size, here $S=7$</td></tr><tr><td>$B$</td><td>number of predicted bounding boxes per grid cell</td></tr></tbody></table><p align=justify>Because of model instability due to the inbalance between cells containing or not containing objects the authors use two scale parameters to increase the loss from bounding box predictions ($\lambda_{coord} = 0.5$) and decrease the loss from confidence predictions for boxes that don't contain objects ($\lambda_{noobj} = 0.5$). The loss function only penalizes classification error if an object is present in that grid cell and it only penalizes bbox error if the cell is "responsible" for the ground truth box.</p><p align=justify>Some more training details: The authors trained YOLO on VOC 2007 and VOC 2012 with a batch size of 64, momentum of 0.9 and a weight decay of 0.0005. For regularization they rely on dropout and data augmentation.</p><h3 id=shortcomings>Shortcomings<a hidden class=anchor aria-hidden=true href=#shortcomings>#</a></h3><ul><li>strong spatial constraints since we have only one prediction per grid cell (7x7 -> max. 49 object predictions); this is one of the reasons why the model struggles with crowds of small objects</li><li>struggles to generalize to objects in new or unusual aspect ratios or configurations (maybe this could be reduced with clever data augmentation or training on different image scales)</li><li>many incorrect localizations due to an inappropriate loss function and coarse features for bounding box prediction (multiple downsampling layers)</li></ul><h2 id=yolov2>YOLOv2<a hidden class=anchor aria-hidden=true href=#yolov2>#</a></h2><p align=justify>YOLOv2 is basically an improved version of YOLO, adding some tricks to overcome its shortcomings described before. Moreover the paper covers YOLO9000 which is built on top of YOLOv2 and trained with a joint dataset combining COCO and the top 9000 classes of ImageNet (combination of detection and classification). I will only cover YOLOv2 here, for those of you who are interested in YOLO9000 and the joint training procedure, should take a look a the paper.</p><p align=justify>As mentioned before, the central goal of YOLOv2 was to fix the problems of YOLO, primarily recall and localization shortcomings. The authors did this based on a variety of new ideas in the field (at that time) and they try to avoid increasing the model size at the same time to preserve the high speed:</p><p><strong>Batch Normalization:</strong> This leads to significant improvements in convergence while eliminating the need for other forms of regularization like dropout (+2% mAP).</p><p><strong>High Resolution Classifier:</strong> Finetune the classification network at higher resolution (448x448) for 10 epochs on ImageNet before detection finetuning.</p><p><strong>Convolutional Anchor Box Detection:</strong> The fully connected layers from YOLO are removed and instead YOLOv2 incorporates anchor boxes (like Faster R-CNN) to predict the bounding boxes; this also decouples the class prediction from the spatial location by predicting class and objectness for every anchor box which leads to a slight decrease in accuracy (-0.3% mAP) but increases recall significantly (+7%) which gives the model more room to improve.</p><p><strong>Box Dimension Clustering:</strong> Instead of using hand picked anchor box sizes, YOLOv2 runs k-means clustering on the training data to determine good priors for anchor box dimensions; to maximize IoU scores, it relies on the following distance metric:</p><p>$$
d(x, c_{i}) = 1 - IoU(x, c_{i}), \quad i=i,&mldr;k
$$</p><p>where $x$ is a ground truth box candidate and $c_{i}$ is one of the centroids / the closest centroid.</p><p><strong>Direct Location Prediction:</strong> In Region Proposal Networks the box location prediction is unconstrained which means any anchor box can end up at any point in the image which can lead to an unstable training. YOLOv2 follows the approach of the original YOLO model by predicting location coordinates relative to the location of the grid cell (using a logistic activation). Given the anchor box width $p_{w}$ and height $p_{h}$ in the grid cell with the top left corner ($c_{x}, c_{y}$) the model predicts 5 values ($t_{x}, t_{y}, t_{w}, t_{h}, t_{o}$) which correspond to the following box values:</p><p>$$
b_{x} = \sigma (t_{x}) + c_{x} \
b_{y} = \sigma (t_{y}) + c_{y} \
b_{w} = p_{w}e^{t_{w}} \
b_{h} = p_{h}e^{t_{h}} \
Pr(obj) =IoU(b, obj) = \sigma (t_{o})
$$</p><p>This in combination with clustering priors improves mAP by up to 5%.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_2/yolov2-loc.png#center alt="YOLOv2 bounding box prediction format" width=60%><figcaption><p>Figure 2. YOLOv2 bounding box prediction</p></figcaption></figure><p><strong>Fine-grained Features:</strong> The grid size of the final feature map of YOLOv2 is increased from 7x7 in YOLO to 13x13. Moreover YOLOv2 incorporates a so called passthrough layer that brings features from an earlier layer at 26x26 resolution to the output layer. This process can be compared with identity mappings from ResNets to incorporate higher dimensional features (+1% mAP).</p><p><strong>Multi-scale Training:</strong> To increase the robustness of the model the authors trained it on images of different sizes. Every 10 batches the input size gets randomly sampled (between 320x320 and 608x608).</p><p align=justify>To maintain the high inference speed, YOLOv2 is based on the <b>Darknet-19</b> model, consisting of 19 convolutional and 5 max-pooling layers. For detailed information on the architecture check out Table 6 in the <a href=https://arxiv.org/pdf/1612.08242.pdf>paper</a>.</p><h2 id=retinanet>RetinaNet<a hidden class=anchor aria-hidden=true href=#retinanet>#</a></h2><p align=justify>Next up in our list of fast detection models is RetinaNet. The creators had the goal of closing the accuracy gap between one and two-stage detection approaches. To achieve this, RetinaNet relies on two crucial building blocks, <b>Feature Pyramid Networks</b> (FPN) as a backbone and a new loss function called <b>Focal Loss</b>.</p><h3 id=focal-loss>Focal Loss<a hidden class=anchor aria-hidden=true href=#focal-loss>#</a></h3><p align=justify>The central cause for the accuracy gap between the two approaches lies in the extreme foreground-background class imbalance during training. In two-stage detectors this problem is addressed by narrowing down the number of candidate object locations (filtering out many background samples) and by using sampling heuristics like a fixed foreground-to-background ratio or online hard example mining. The proposed Focal Loss is designed to address this issue for one-stage detectors by focusing on hard negatives and down-weighting the easier predictions (obvious empty background). It is based on the normal cross entropy loss (for simplicity we use binary loss down here)</p><p>$$CE(p,y) = -y \log p - (1-y) \log (1 - p) $$</p><p align=justify>where $y={0,1}$ is a ground truth binary label, indicating whether a bounding box contains an object and $p \in [0,1]$ is the predicted probability that there is an object (also called objectness or confidence score). For notational convenience, let</p><p>$$
p_{t} =
\begin{cases}
p & \quad \text{if } y=1\
1-p & \quad \text{otherwise}
\end{cases}
$$</p><p>which leads to</p><p>$$
CE(p_{t}) = - \log p_{t}
$$</p><p align=justify>Easily classified negatives ($p\_{t} \gg 0.5 ,y=0$) comprise the majority of the loss. You can balance the importance of the positive/negative examples by adding a balancing factor $\alpha$</p><p>$$
CE(p_{t}) = - \alpha_{t} \log p_{t}
$$</p><p align=justify>but this does not differentiate between easy or hard examples. To overcome this the Focal Loss adds a modulating factor $(1-p\_{t})^{\gamma}$ with a tunable focusing parameter $\gamma \geq 0$:</p><p>$$
FL(p_{t}) = - (1 - p_{t})^{\gamma} \log (p_{t})
$$</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_2/focal-loss.png#center alt="Focal Loss with different gamma values" width=70%><figcaption><p>Figure 3. Focal Loss with different gamma values</p></figcaption></figure><p align=justify>For better control of the shape of the weighting function the authors used an $\alpha$-balanced version in practice, where $\alpha = 0.25$ and $\gamma = 2$ worked best in their experiments:</p><p>$$
FL(p_{t}) = - \alpha (1 - p_{t})^{\gamma} \log (p_{t})
$$</p><h3 id=feature-pyramid-network>Feature Pyramid Network<a hidden class=anchor aria-hidden=true href=#feature-pyramid-network>#</a></h3><p align=justify>The FPN backbone for RetinaNet was constructed on top of ResNet. To really understand what that means you should take a look at the <a href=https://arxiv.org/pdf/1612.03144.pdf>paper</a>. Figure 4 down below shows the fundamental idea of FPN which is to leverage a ConvNets pyramidal feature hierarchy to build a feature pyramid with high level semantics throughout. It is general purpose and can be applied to many convolutional backbone architectures.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_2/featurized-image-pyramid.png#center alt="Featurized Pyramid Network architecture" width=100%><figcaption><p>Figure 4. Featurized Pyramid Network architecture</p></figcaption></figure><p align=justify>The basic structure contains a sequence of pyramid levels each corresponding to one network stage. Often these stages contain multiple conv layers of the same size and stage sizes are scaled down by a factor of two. $C\_{i}$ represents the different layers of those stages (for ResNet e.g. {$C\_{2}, C\_{3}, C\_{4}, C\_{5}$}). As you can see there are two different pathways which connect the conv layers:</p><ol><li><strong>Bottom-up</strong> pathway: regular feedback path</li><li><strong>Top-down</strong> pathway: goes in the opposite direction, adding coarse but semantically stronger feature maps back into the previous levels of layer size by lateral connections (1x1 conv to match dimensions) and nearest neighbor upsampling; the combination of the two maps is done by element-wise addition</li></ol><p align=justify>The final predictions ({$P\_{i}$} where $i$ indicates the pyramid level and has resolution $2^{i}$ lower than the input) are generated out of every merged map by a 3x3 conv layer. RetinaNet utilizes feature pyramid levels $P\_{3}$ to $P\_{7}$ computed from the corresponding ResNet residual stage from $C\_{3}$ to $C\_{5}$. All pyramid levels have 256 channels (most of RetinaNet is similar to FPN with a few minor differences). The authors used translation-invariant anchor boxes as priors, similar to those used in RPN variant of FPN. To improve Average Precision the number of anchors was increased to $A=9$ (three aspect ratios {1:2, 1:1, 2:1} with three different sizes {$2^{0}, 2^{1/3}, 2^{2/3}$}). As seen before, for each anchor box the model predicts a class probability for each of $K$ classes with a classification subnet trained with Focal Loss. A box regression subnet outputs the offsets for the boxes to the nearest ground truth object. Both networks are independent Fully Convolutional Networks that don't share any parameters.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_2/retina-net.png#center alt="RetinaNet architecture" width=100%><figcaption><p>Figure 5. RetinaNet architecture</p></figcaption></figure><h2 id=yolov3>YOLOv3<a hidden class=anchor aria-hidden=true href=#yolov3>#</a></h2><p align=justify>YOLOv3 was created by applying changes to YOLOv2 inspired by, at that time, recent advances in the object detection world. It's a pretty short and rather unscientifically (I like it :D ) written. The following list summarizes the most important improvements:</p><ul><li><strong>Logistic Regression for objectness scores</strong> instead of sum of squared errors</li><li><strong>Independent Logistic Classifiers</strong> for class prediction instead of softmax which increases the performance on non mutually exclusive multilabel datasets like Open Images</li><li><strong>Multi-scale predictions</strong> inspired by FPN (3 scales per stage)</li><li><strong>Darknet-53 as Feature Extractor</strong> which performs similar to ResNet-152 but is 2x faster</li></ul><p align=justify>Overall YOLOv3 performs better and faster than SSD, worse then RetinaNet but is 3.8x faster and comparable to state-of-the-art methods on the $AP\_{50}$ metric at that time. In the appendix Joseph (the author) adds a cool comment on his opinion about the COCO evaluation metrics. It's refreshing to see someone questioning stuff like this.</p><figure class=align-center><img loading=lazy src=/imgs/object_detection_2/yolov3-res.png#center alt="YOLOv3 performance" width=80%><figcaption><p>Figure 6. YOLOv3 performance</p></figcaption></figure><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p align=justify>In this blog post, we went over four popular but now somewhat aging fast object recognition systems and you got a first introduction to the world of real-time object recognition. In the next post I would like to talk about some more recent models like EfficientDet and YOLOv4.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p><a href=https://arxiv.org/pdf/1506.02640.pdf>[1]</a> Joseph Redmon, et al. “You only look once: Unified, real-time object detection.” CVPR 2016.</p><p><a href=https://arxiv.org/pdf/1612.08242.pdf>[2]</a> Joseph Redmon and Ali Farhadi. “YOLO9000: Better, Faster, Stronger.” CVPR 2017.</p><p><a href=https://arxiv.org/pdf/1804.02767.pdf>[3]</a> Joseph Redmon, Ali Farhadi. “YOLOv3: An incremental improvement.”.</p><p><a href=https://arxiv.org/pdf/1612.03144.pdf>[4]</a> Tsung-Yi Lin, et al. “Feature Pyramid Networks for Object Detection.” CVPR 2017.</p><p><a href=https://arxiv.org/pdf/1708.02002.pdf>[5]</a> Tsung-Yi Lin, et al. “Focal Loss for Dense Object Detection.” IEEE transactions on pattern analysis and machine intelligence, 2018.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://johanngerberding.github.io/>Johanns Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>