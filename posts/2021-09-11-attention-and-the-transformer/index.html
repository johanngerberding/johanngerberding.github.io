<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Attention and the Transformer | Johanns Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms.">
<meta name="author" content="Johann Gerberding">
<link rel="canonical" href="http://localhost:1313/posts/2021-09-11-attention-and-the-transformer/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.a0264e6a2df146fcf73b7783950d40c4be7e8efa44c2889964f6ec37304880d9.css" integrity="sha256-oCZOai3xRvz3O3eDlQ1AxL5&#43;jvpEwoiZZPbsNzBIgNk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2021-09-11-attention-and-the-transformer/">

<meta name="twitter:title" content="Attention and the Transformer | Johanns Blog" />
<meta name="twitter:description" content="Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms." />
<meta property="og:title" content="Attention and the Transformer | Johanns Blog" />
<meta property="og:description" content="Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/2021-09-11-attention-and-the-transformer/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2021-10-15T10:45:16&#43;02:00" />
  <meta property="article:modified_time" content="2021-10-15T10:45:16&#43;02:00" /><meta property="og:site_name" content="Johann&#39;s Blog" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Attention and the Transformer",
      "item": "http://localhost:1313/posts/2021-09-11-attention-and-the-transformer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Attention and the Transformer | Johanns Blog",
  "name": "Attention and the Transformer",
  "description": "Introduction to the world of Reinforcement Learning, where I cover the basics and some algorithms.",
  "keywords": [
    
  ],
  "wordCount" : "1021",
  "inLanguage": "en",
  "datePublished": "2021-10-15T10:45:16+02:00",
  "dateModified": "2021-10-15T10:45:16+02:00",
  "author":{
    "@type": "Person",
    "name": "Johann Gerberding"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/2021-09-11-attention-and-the-transformer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Johanns Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script><script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  
    window.addEventListener('load', (event) => {
        document.querySelectorAll("mjx-container").forEach(function(x){
          x.parentElement.classList += 'has-jax'})
      });
  
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-post" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'light';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Johanns Blog (Alt + H)">Johanns Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="posts" class="active"
                ><i class='fa fa-heart'></i>posts
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags"
                ><i class='fa fa-heart'></i>tags
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/reading/" title="reading"
                ><i class='fa fa-heart'></i>reading
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about-me/" title="about"
                ><i class='fa fa-heart'></i>about
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">Attention and the Transformer<sup><span class="entry-isdraft">&nbsp;&nbsp;[draft]</span></sup></h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>October 15, 2021</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select: text;"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z" style="user-select: text;"></path><polyline points="14 2 14 8 20 8" style="user-select: text;"></polyline><line x1="16" y1="13" x2="8" y2="13" style="user-select: text;"></line><line x1="16" y1="17" x2="8" y2="17" style="user-select: text;"></line><polyline points="10 9 9 9 8 9" style="user-select: text;"></polyline></svg>
  <span>1021 words</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>5 min</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#whats-wrong-with-rnns" aria-label="Whats wrong with RNN&rsquo;s?">Whats wrong with RNN&rsquo;s?</a></li>
                <li>
                    <a href="#self-attention" aria-label="(Self)-Attention">(Self)-Attention</a></li>
                <li>
                    <a href="#transformer" aria-label="Transformer">Transformer</a><ul>
                        
                <li>
                    <a href="#architecture" aria-label="Architecture">Architecture</a></li>
                <li>
                    <a href="#positional-encoding" aria-label="Positional Encoding">Positional Encoding</a></li>
                <li>
                    <a href="#multi-head-attention" aria-label="Multi-Head Attention">Multi-Head Attention</a></li></ul>
                </li>
                <li>
                    <a href="#applications" aria-label="Applications">Applications</a><ul>
                        
                <li>
                    <a href="#nlp" aria-label="NLP">NLP</a></li>
                <li>
                    <a href="#computer--vision" aria-label="Computer  Vision">Computer  Vision</a></li></ul>
                </li>
                <li>
                    <a href="#summary" aria-label="Summary">Summary</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="whats-wrong-with-rnns">Whats wrong with RNN&rsquo;s?<a hidden class="anchor" aria-hidden="true" href="#whats-wrong-with-rnns">¶</a></h2>
<p align="justify">
Before we dive into the details of the (Vanilla) Transformer model architecture I want to give you a short intro about how the self-attention mechanism, which is one of the key elements of a transformer block, evolved and why it is part of so many state-of-the art approaches, especially in Natural Language Processing (In the meantime, it can be said that they are also gradually taking over the computer vision field; the state of the art in the field of image classification is a combination of a convolutional neural net and a transformer, called CoAtNet[9]). Much of this information comes from the 2019 lecture by [Justin Johnson](https://www.youtube.com/watch?v=YAgjfMR9R_M&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=14&ab_channel=MichiganOnline) (Michigan State University) and the blogpost [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) from Lilian Weng, which I think are two of the best resources for getting started on the topic.
</p>
<p align="justify">
Let's start by looking at pre-Transformer sequence-to-sequence architectures. Classic seq2seq models have an encoder-decoder architecture and aim to transform an input sequence (source, e.g. german sentence) into a different output sequence (target, e.g. english translation). Both sequences can be of arbitrary length and the Encoder as well as the Decoder are different Recurrent Neural Network architectures (e.g. LSTM, GRU).
</p>
<p>![Encoder Decoder Architecture]({{ &lsquo;/assets/imgs/transformer/encoder_decoder.png&rsquo; | relative_url}}){: style=&ldquo;width: 100%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 1.</strong> Encoder Decoder Architecture [5]</p>
<p align="justify">
Typical transformation tasks of those kinds of models are f.e. machine translation, question-answer dialog generation, image/video captioning [7], speech recognition [6] or parsing sentences into grammar trees [8]. The encoder and the decoder network are typically connected with a fixed length context vector which transfers information between the encoded and the decoded sequence but becomes a bottleneck for longer sequences because of its fixed size. Often the model has "forgotten" the first part of a long sequence once it completes processing the whole input.
</p>
<p align="justify">
To solve this problem, the **Attention mechanism** was born. Instead of only relying on the last hidden state, the idea was to create shortcut connections between the context vector and the entire source input which should negate the "forgetting". The alignment between the source and the target sequence is learned and controlled by the context vector. The illustration down below shows this mechanism.
</p>
<p>![Additive Attention Mechanism]({{ &lsquo;/assets/imgs/transformer/additive_attention.png&rsquo; | relative_url}}){: style=&ldquo;width: 100%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 2.</strong> Additive Attention Mechanism used in [5] from [10]</p>
<p align="justify">
To better understand how this works I have re-implemented this [here](). Feel free to clone the repo and train the model yourself. The Attention mechanism used is called **Additive Attention** (there exist different forms of Attention mechanisms). The encoder consists of a bidirectional RNN with a forward and a backward hidden state, $\overrightarrow{\boldsymbol{h_{i}}}$ and $\overleftarrow{\boldsymbol{h_{i}}}$, which are concatenated to form the encoder state $\boldsymbol{h_{i}}$. The context vector $\boldsymbol{c_{t}}$ for the output $y_{t}$ is a sum of hidden states of the input sequence weighted by alignment scores where $n$ is the length of the input sequence:
</p>
<p align="center">
$$
\boldsymbol{c}_t = \sum_{i=1}^{n}\alpha_{t,i} \boldsymbol{h}_{i} \\
\alpha_{t,i} = softmax(score(\boldsymbol{s}_{t}, \boldsymbol{h}_{i})) \\
score(\boldsymbol{s}_{t}, \boldsymbol{h}_{i}) = \boldsymbol{v}_{a}^{T} tanh(\boldsymbol{W}_{a}[\boldsymbol{s}_{t}; \boldsymbol{h}_{i}])
$$
</p>
<p align="justify">
The alignment model in [5] is a feed-forward network with a single hidden layer. It assigns a score to the pair of input at position $i$ and output at position $t$ based on how well the two words match. Both $\boldsymbol{v_{a}}$ and $\boldsymbol{W_{a}}$ are learned by the alignment model. Based on this, the decoder network calculates the hidden state:
</p>
<p align="center">
$$
\boldsymbol{s}_{t} = f(\boldsymbol{s}_{t-1}, y_{t-1}, \boldsymbol{c}_{t}) \\
$$
</p>
<p align="justify">
With the alignment scores you can create pretty cool matrices which show the correlation between the source and the target words. Down below I have created such a plot with a model I trained for a few epochs on the Multi30k torchtext dataset.
</p>
<p>![Matrix of alignment scores]({{ &lsquo;/assets/imgs/transformer/attention_matrix.png&rsquo; | relative_url}}){: style=&ldquo;width: 80%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 3.</strong> Attention score matrix</p>
<p align="justify">
If you want to learn more about different forms of Attention and popular alignment score functions you can take a look at [10] which provides a table summarizing this.
</p>
<h2 id="self-attention">(Self)-Attention<a hidden class="anchor" aria-hidden="true" href="#self-attention">¶</a></h2>
<ul>
<li>at each timestep of decoder, the context vector &ldquo;looks at&rdquo; different parts of the input sequence</li>
<li>how: calculate attention scalars based on the hidden state and the decoder state for every hidden state, then multiply the hidden states by the attention scalars and sum them up to create a context vector</li>
</ul>
<p>this mechanism doesn&rsquo;t use the fact that hidden state i forms an ordered sequence, it just treats the hidden states as an unordered set ${h_{i}}$</p>
<p>this means that you can use a similar architecture given any set of input hidden vectors (e.g in image captioning)</p>
<p>in CS, if we discover something useful which is generally applicable we try to abstract</p>
<p>![Self-Attention Layer]({{ &lsquo;/assets/imgs/transformer/self_attention_layer.png&rsquo; | relative_url}}){: style=&ldquo;width: 100%; margin: 0 auto; display: block;&rdquo;}<strong>Figure 2.</strong> Self-Attention Layer (<a href="https://www.youtube.com/watch?v=YAgjfMR9R_M&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&amp;index=14&amp;ab_channel=MichiganOnline">Michigan Online - Justin Johnson</a>)</p>
<p>&ldquo;Memory is attention through time&rdquo; - Alex Graves (2020)</p>
<p>Attention is about ignoring things to focus on specific parts of the data.</p>
<h2 id="transformer">Transformer<a hidden class="anchor" aria-hidden="true" href="#transformer">¶</a></h2>
<p align="justify">
I think it is fair to say that the **Transformer** is by far the most popular model architecture choice in the research community at the moment. Vaswani et al. presented the architecture in their paper titeled "Attention is All you Need" which already gives an idea of what it is all about. The Transformer is entirely built on the self-attention mechanism presented before without using any sequence aligned recurrent architecture.
</p>
<h3 id="architecture">Architecture<a hidden class="anchor" aria-hidden="true" href="#architecture">¶</a></h3>
<p><strong>Encoder</strong></p>
<p><strong>Decoder</strong></p>
<h3 id="positional-encoding">Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#positional-encoding">¶</a></h3>
<h3 id="multi-head-attention">Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-attention">¶</a></h3>
<p><strong>Scaled Dot-Product Attention</strong></p>
<p>$$
Attention(Q,K,V) = softmax(QK^{T}/)V
$$</p>
<p><strong>Multi-Head Attention</strong></p>
<h2 id="applications">Applications<a hidden class="anchor" aria-hidden="true" href="#applications">¶</a></h2>
<h3 id="nlp">NLP<a hidden class="anchor" aria-hidden="true" href="#nlp">¶</a></h3>
<p>a few popular NLP architectures (GPT, Bert and stuff)</p>
<h3 id="computer--vision">Computer  Vision<a hidden class="anchor" aria-hidden="true" href="#computer--vision">¶</a></h3>
<p>Vision Transformer</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">¶</a></h2>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">¶</a></h2>
<p><a href="https://arxiv.org/pdf/1706.03762.pdf">[1]</a> Vaswani et al. &ldquo;Attention is all you need&rdquo;, 2017.</p>
<p><a href="http://peterbloem.nl/blog/transformers">[2]</a> Transformers from Scratch.</p>
<p><a href="https://theaisummer.com/attention/">[3]</a> How Attention works in Deep Learning: Understanding the attention mechanism in sequence models.</p>
<p><a href="https://www.youtube.com/watch?v=YAgjfMR9R_M&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&amp;index=14&amp;ab_channel=MichiganOnline">[4]</a> Lecture 13: Attention, Justin Johnson.</p>
<p><a href="https://arxiv.org/pdf/1409.0473.pdf">[5]</a> Bahdanau et al. &ldquo;Neural Machine Translation by jointly learning to align and translate&rdquo;, 2016.</p>
<p><a href="https://arxiv.org/pdf/1610.03022.pdf">[6]</a> Zhang et al. &ldquo;Very Deep Convolutional Networks for End-to-End Speech Recognition&rdquo;, 2016.</p>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7505636">[7]</a> Vinyals et al. &ldquo;Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge&rdquo;, 2017.</p>
<p><a href="https://proceedings.neurips.cc/paper/2015/file/277281aada22045c03945dcb2ca6f2ec-Paper.pdf">[8]</a> Vinyals et al. &ldquo;Grammar as a Foreign Language&rdquo;, 2015.</p>
<p><a href="https://arxiv.org/pdf/2106.04803v2.pdf">[9]</a> Dai et al. &ldquo;CoAtNet: Marrying Convolution and Attention for All Data Sizes&rdquo;, 2021.</p>
<p><a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">[10]</a> Lilian Weng &ldquo;Attention? Attention!&rdquo;, 2018.</p>


  </div>

  <footer class="post-footer">
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/2022-01-15-a-peek-into-deep-reinforcement-learning-part-1/">
    <span class="title">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select: text;"><line x1="19" y1="12" x2="5" y2="12" style="user-select: text;"></line><polyline points="12 19 5 12 12 5" style="user-select: text;"></polyline></svg>&nbsp;Prev Page</span>
    <br>
    <span>A Peek into Deep Reinforcement Learning - Part I</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/2021-05-02-object-detection-faster-models/">
    <span class="title">Next Page&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12" style="user-select: text;"></line><polyline points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg>
    </span>
    <br>
    <span>Object Detection - Faster Models</span>
  </a>
</nav>

  </footer>
    <div class="comments-separator"></div>
</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2024 <a href="http://localhost:1313/">Johanns Blog</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied!';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>
<script>
  mediumZoom('.entry-cover img');
  mediumZoom('.post-content img:not([no-zoom])');
</script>

</body>

</html>
