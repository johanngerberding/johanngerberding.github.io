---
layout: post
title: A Peek into Deep Reinforcement Learning - Part II
date: 2022-02-04 10:00:00 +0200
author: Johann Gerberding
summary: Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space.
include_toc: true
showToc: true
math: true
draft: true
---

<p align="justify">
In this second part on Deep Reinforcement Learning we are going to explore some of the combined methods (look into <a href="https://johanngerberding.github.io/posts/A-Peek-into-Deep-Reinforcement-Learning-Part-1">Part I</a> if you don't know what I mean with combined methods). We will start with the so called <b>Advantage Actor-Critic</b> algorithm which combines some of the concepts we have learned about in the first part. Then we dive into the very popular <b>Proximal Policy Optimization</b> algorithm. Moreover I will give you an overview of the <b>Alpha(Go)Zero</b> algorithm from <a href="https://deepmind.com/blog/article/alphago-zero-starting-scratch">DeepMind</a> and the <b>Soft Actor-Critic</b> method. In the third part of this series we are going to talk more about <b>Imitation Learning</b> and <b>Model-based RL</b>.
</p>

## Advantage Actor-Critic

<p align="justify">
As mentioned before, Advantage Actor-Critic (A2C) algorithms combine the ideas from policy gradient methods (e.g. <i>REINFORCE</i>) and a learned value function (e.g. <i>DQN</i>). Here we reinforce a policy with a learned reinforcing signal generated by a learned value function. A2C algorithms therefore consist of two jointly learned components:
</p>

* an <b>actor</b> which learns a parameterized policy and
* a <b>critic</b> which learns a value function to evaluate state-action pairs (it provides a reinforcing signal to the actor)

<p align="justify">
The motivation for this is that a learned reinforcing signal can be much more informative for a policy than the rewards available from an environment. Instead of learning $Q^{\pi}(s,a)$ or $V^{\pi}$, it is common to learn the so called <b>advantage function</b> $A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$ as the reinforcing signal. The key idea behind this is that it is better to select an action based on how it performs relative to the other actions available in a particular state, instead of using the absolute value (hence the name <i>advantage</i> actor-critic). The actors learn a parameterized policy $\pi_{\theta}$ using the policy gradient. This is similar to the REINFORCE algorithm, but instead of the Monte-Carlo estimate $R_{t}(\tau)$, the advantage is used:
</p>

$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}\_{t} \Big[ A\_{t}^{\pi} \nabla_{\theta} \log \pi_{\theta} (a\_{t} | s\_{t})\Big]
$$

<p align="justify">
The critic is responsible for learning how to evaluate state-action-pairs and using this to generate $A^{\pi}$. To estimate the advante function, we will go over two possible methods: <b>n-step returns</b> and <b>Generalized Advantage Estimation</b> (GAE). In general the advantage function measures the extent to which an action is better or worse than the policy's average action in a particular state:
</p>

$$
A^{\pi}(s\_{t},a\_{t}) = Q^{\pi}(s\_{t},a\_{t}) - V^{\pi}(s\_{t})
$$

<p align="justify">
One benefit of using the advantage instead of $Q^{\pi}$ or $V^{\pi}$ is that it avoids penalizing an action for the policy currently being in a particularly bad state, like in the following example:
</p>

$$
Q^{\pi}(s,a) = 110, \quad V^{\pi}(s) = 100, \quad A^{\pi}(s,a) = 10
$$

$$
Q^{\pi}(s,a) = -90, \quad V^{\pi}(s) = -100, \quad A^{\pi}(s,a) = 10
$$

<p align="justify">
The advantage function is better able to capture the long-term effects of an action because it considers all future time steps while ignoring the effects of all the actions to date.
</p>

<br>

### n-step Returns

<p align="justify">
As seen before, to calculate $A^{\pi}$ we need estimates for both $Q^{\pi}(s,a)$ and $V^{\pi}(s)$. In the n-step Returns method we achieve this by learning $V^{\pi}(s)$ and estimating $Q^{\pi}(s,a)$ from it:
</p>

$$
Q^{\pi}(s,a) = \mathbb{E}\_{\tau \sim \pi} \Big[ r\_{t} + \gamma r\_{t+1} + \gamma^{2} r\_{t+2} ... + \gamma^{n} r\_{t+n} \Big] + \gamma^{n+1} \hat{V}^{\pi}(s\_{t+n+1})
$$

<p align="justify">
The expectation part of the Q-value estimate show above is calculated based on a 3-step return, which means that we use our collected trajectory data to look three steps in the future and sum up the rewards multiplied by a discounting factor $\gamma^{t}$. This part of the equation is unbiased but has a high variance because it comes from only one trajectory. $n$ is a hyperparameter that needs to be tuned. The bigger the value of $n$, the higher the variance of the estimate. The return after the n-th step is calculated by the critic network. It has lower variance since it reflects an expectation over all of the trajectories seen so far, but it is biased because it is calculated using a function approximator. From this we now get a formula for estimating the advantage:
</p>

$$
A\_{NSTEP}^{\pi}(s\_{t}, a\_{t}) = Q^{\pi}(s\_{t}, a\_{t}) - V^{\pi}(s\_{t}) 
$$

$$
A\_{NSTEP}^{\pi}(s\_{t}, a\_{t}) \approx r\_{t} + \gamma r\_{t+1} + \gamma^{2} r\_{t+2} + ... + \gamma^{n} r\_{t+n} + \gamma^{n+1} \hat{V}^{\pi}(s\_{t+n+1}) - \hat{V}^{\pi}(s\_{t})
$$

<br>

### Generalized Advantage Estimation (GAE)

<p align="justify">
Generalized Advantage Estimation was proposed as an improvement over the n-step returns estimate for the advantage function. It addresses the problem of having to explicitly choose the number of steps of returns $n$. The main idea is, instead of picking one value of $n$, we mix multiple values by calculating the advantage using a weighted average of individual advantages calculated with $n = 1, 2, 3, ..., k$. This significantly reduces the variance of the estimator while keeping the bias introduced as low as possible.
</p>

$$
A\_{GAE}^{\pi}(s\_{t}, a\_{t}) = \sum^{\infty}\_{l=0}(\gamma \lambda) \delta\_{t+l}
$$

$$
\text{where }  \delta\_{t} = r\_{t} + \gamma V^{\pi}(s\_{t+1}) - V^{\pi}(s\_{t})
$$

<p align="justify">
GAE is taking a weighted average over a number of advantage estimators with different bias and variance. It weights the high-bias, low-variance 1-step advantage the most but also includes contributions from lower-bias, higher-variance estimators using $2,3,...,n$ steps. The contributions decay at an exponential rate as the number of steps increases and the decay rate gets controlled by the coefficient $\lambda$ (the larger $\lambda$, the higher the variance). In contrast to $n$, $\lambda$ represents a softer choice than $n$, which means smaller values of $\lambda$ will more heavily weight the V-function estimate, whilst larger values will weight the actual rewards more.
</p>

<br>

### Algorithm & Network Architecture

<p align="justify">
But how do we learn the value function $V^{\pi}$? We have different possibilities here, one of them is <i>Temporal Difference Learning</i>, similar to DQN. First we parameterize $V^{\pi}$ with $\theta$, then we generate $V_{tar}^{\pi}$ for each of the experiences an agent gathers. Finally we minimize the distance between $\hat{V}^{\pi}(s; \theta)$ and $V^{\pi}_{tar}$ using a simple regression loss such as MSE. You can use different methods to generate $V_{tar}^{\pi}$:
</p>

* n-step estimate:

$$
V\_{tar}^{\pi}(s) = r + \hat{V}^{\pi}(s'; \theta)
$$

$$
V\_{tar}^{\pi}(s\_{t}) = r\_{t} + \gamma r\_{t+1} + \gamma^{2} r\_{t+2} + ... + \gamma^{n} r\_{t+n} + \gamma^{n+1} \hat{V}^{\pi}(s\_{t+n+1})
$$

* Monte-Carlo estimate:

$$
V\_{tar}^{\pi}(s\_{t}) = \sum\_{t'=t}^{T}\gamma^{t'-t} r\_{t'}
$$

* GAE:

$$
V\_{tar}^{\pi}(s\_{t}) = A\_{GAE}^{\pi}(s\_{t}, a\_{t}) + \hat{V}^{\pi}(s\_{t})
$$

<p align="justify">
The choice is often related to the method used to estimate the advantage. It is also possible to use a different, more complicated optimization procedure (trust region method) when learning the value function $\hat{V}^{\pi}$ which you can find in the original <a href="https://arxiv.org/abs/1506.02438">GAE paper</a>.
</p>

<p align="justify">

</p>



## Proximal Policy Optimization



## Alpha(Go)Zero


### Monte Carlo Tree Search



## Soft Actor-Critic



## Summary

## References

[[1]](https://arxiv.org/abs/1602.01783) Mnih et al. "Asynchronous Methods for Deep Reinforcement Learning" (2016).

[[2]](https://arxiv.org/abs/1707.06347) Schulman et al. "Proximal Policy Optimization Algorithms" (2017).

[[3]](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) Kullback-Leiber Divergence Explained (2017).

[[4]](https://arxiv.org/abs/1502.05477) Schulman et al. "Trust Region Policy Optimization" (2015).

[[5]](https://arxiv.org/abs/1712.01815) Silver et al. "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" (2017).

[[6]](https://arxiv.org/abs/1506.02438) Schulman et al. "High-Dimensional Continuous Control Using Generalized Advantage Estimation" (2015).