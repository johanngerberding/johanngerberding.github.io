---
layout: post
title: A Peek into Deep Reinforcement Learning - Part II
date: 2022-02-04 10:00:00 +0200
author: Johann Gerberding
summary: Second part of the Introduction to the world of Reinforcement Learning, where I cover some more advanced deep RL algorithms and ideas in the space.
include_toc: true
showToc: true
math: true
draft: true
---

<p align="justify">
In this second part on Deep Reinforcement Learning we are going to explore some of the combined methods (look into [Part I](https://johanngerberding.github.io/posts/A-Peek-into-Deep-Reinforcement-Learning-Part-1) if you don't know what I mean with combined methods). We will start with the so called **Advantage Actor-Critic** algorithm which combines some of the concepts we have learned about in the first part. Then we dive into the very popular **Proximal Policy Optimization** algorithm.
</p>

## Advantage Actor-Critic

<p align="justify">
As mentioned before, Advantage Actor-Critic (A2C) algorithms combine the ideas from policy gradient methods (e.g. *REINFORCE*) and a learned value function (e.g. *DQN*). Here we reinforce a policy with a learned reinforcing signal generated by a learned value function. A2C algorithms therefore consist of two jointly learned components:
</p>

* an **actor** which learns a parameterized policy and
* a **critic** which learns a value function to evaluate state-action pairs (it provides a reinforcing signal to the actor)

<p align="justify">
The motivation for this is that a learned reinforcing signal can be much more informative for a policy than the rewards available from an environment. Instead of learning $Q^{\pi}(s,a)$ or $V^{\pi}$, it is common to learn the so called **advantage function** $A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$ as the reinforcing signal. The key idea behind this is that it is better to select an action based on how it performs relative to the other actions available in a particular state, instead of using the absolute value (hence the name *advantage* actor-critic). The actors learn a parameterized policy $\pi_{\theta}$ using the policy gradient. This is similar to the REINFORCE algorithm, but instead of the Monte-Carlo estimate $R_{t}(\tau)$, the advantage is used:
</p>

$$
\nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{t} \Big[ A_{t}^{\pi} \nabla_{\theta} \log \pi_{\theta} (a_{t} | s_{t})\Big]
$$

<p align="justify">
The critic is responsible for learning how to evaluate state-action-pairs and using this to generate $A^{\pi}$. To estimate the advante function, we will go over two possible methods: **n-step returns** and **Generalized Advantage Estimation** (GAE). In general the advantage function measures the extent to which an action is better or worse than the policy's average action in a particular state:
</p>

$$
A^{\pi}(s_{t},a_{t}) = Q^{\pi}(s_{t},a_{t}) - V^{\pi}(s_{t})
$$

<p align="justify">
One benefit of using the advantage instead of $Q^{\pi}$ or $V^{\pi}$ is that it avoids penalizing an action for the policy currently being in a particularly bad state, like in the following example:
</p>

$$
Q^{\pi}(s,a) = 110, \quad V^{\pi}(s) = 100, \quad A^{\pi}(s,a) = 10 \\
Q^{\pi}(s,a) = -90, \quad V^{\pi}(s) = -100, \quad A^{\pi}(s,a) = 10
$$

<p align="justify">
The advantage function is better able to capture the long-term effects of an action because it considers all future time steps while ignoring the effects of all the actions to date.
</p>

### n-step Returns

<p align="justify">
As seen before, to calculate $A^{\pi}$ we need estimates for both $Q^{\pi}(s,a)$ and $V^{\pi}(s)$. In the n-step Returns method we achieve this by learning $V^{\pi}(s)$ and estimating $Q^{\pi}(s,a)$ from it:
</p>

$$
Q^{\pi}(s,a) = \mathbb{E}_{\tau \sim \pi} \Big[ r_{t} + \gamma r_{t+1} + \gamma^{2} r_{t+2} ... + \gamma^{n} r_{t+n} \Big] + \gamma^{n+1} \hat{V}^{\pi}(s_{t+n+1})
$$

<p align="justify">

</p>

### Generalized Advantage Estimation (GAE)


### Algorithm


### Network Architecture


## Proximal Policy Optimization


## AlphaZero


## Summary

## References

[[1]](https://arxiv.org/pdf/1706.03762.pdf) Vaswani et al. Attention is all you need.

[[2]](https://spinningup.openai.com/en/latest/index.html) Josh Achiam, OpenAI Spinning Up (2018).