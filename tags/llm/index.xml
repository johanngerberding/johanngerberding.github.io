<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Llm on Johanns Blog</title><link>https://johanngerberding.github.io/tags/llm/</link><description>Recent content in Llm on Johanns Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 23 Aug 2024 10:00:00 +0200</lastBuildDate><atom:link href="https://johanngerberding.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Vision Language Models</title><link>https://johanngerberding.github.io/posts/2024-08-23-vision-language-models/</link><pubDate>Fri, 23 Aug 2024 10:00:00 +0200</pubDate><guid>https://johanngerberding.github.io/posts/2024-08-23-vision-language-models/</guid><description>LLMs are boring but VLMs are awesome, let&amp;rsquo;s see why.</description><content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
Vision Language Models (VLM) are multimodal models that can learn from text and images and generate text as an output (some are able to generate images too). Typical use cases for this range from image chatting, image recognition, visual question answering to image captioning, document understanding and more. In addition to that some models are also able to perform object detection, segmentation or reasoning about relative positions of objects (which is kinda fire).
</p>
<p align="justify">
The focus of this post lies on open source models just because we have information about their architecture, datasets and training processes. VLMs currently are far from being perfect, there exist a lot of open questions, challenges when building them and problems that have to be addressed e.g.:
</p>
<ul>
<li>bad understanding of spatial relationships</li>
<li>bad at counting (without complicated engineering overhead that relies on additional data annotation or other hacks)</li>
<li>lack understanding of attributes and ordering</li>
<li>ignorance of parts of the input prompt (need for a lot of prompt engineering to produce the results you want)</li>
<li>hallucinations (like in LLMs)</li>
</ul> 
<p align="justify">
In the following I will categorize them by their training paradigm like in [2] and will go over some prominent examples. There exist way to many models to cover them all so if this overview here isn't enough and you want more information check out <a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&sort=trending">huggingface models</a>, this cool <a href="https://huggingface.co/collections/merve/vision-language-models-papers-66264531f7152ac0ec80ceca">paper collection</a> or <a href="https://paperswithcode.com/">paperswithcode</a>. 
</p>
<h2 id="families-of-vlms">Families of VLMs</h2>
<p align="justify">
One way to categorize VLMs is based on the training paradigm like in [2]:
</p>
<ul>
<li><b>contrastive</b>: Leverage pairs of positive and negative examples.</li>
<li><b>masking</b>: Leverage reconstruction of masked image patches given some unmasked text.</li>
<li><b>pretrained backbones</b>: Combine a pretrained LLM with a pretrained image encoder and then learn a mapping between those two.</li>
<li><b>generative</b>: Generate captions and images.</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/families_of_vlms.png#center"
         alt="Families of Vision Language Models" width="90%"/> <figcaption>
            <p>Figure 1. Families of VLMs [2]</p>
        </figcaption>
</figure>

<p align="justify">
The paradigms are not mutually exclusive and many approaches we explore in this post rely on a mix of those training strategies. In the following we will describe some approaches for each paradigm.
</p>
<h3 id="contrastive-based-methods">Contrastive-based Methods</h3>
<p align="justify">
In this section I am presenting two contrastive-based VLMs, the very popular CLIP model from OpenAI and one of the successor models from Meta called Llip.
</p>
<h4 id="clip">CLIP</h4>
<p align="justify">
CLIP (<b>C</b>ontrastive <b>L</b>anguage <b>I</b>mage <b>P</b>re-training) was one of those models created by OpenAI which were really open (you know back in the days OpenAI was really open and cool). The pre-training task here was not to predict the exact caption for each image but to predict which whole caption to pair with a certain image. This switch from a predictive objective (so the classic ML approach with labels) to a contrastive one lead to a 4x efficiency improvement. To train this model the authors leveraged captions from the internet and collected a huge dataset of 400 million image-text-pairs which they called WIT for WebImageText. Another advantage of prediction captions instead of e.g. classes was a better and more flexible zero-shot transfer capability of the model. The figure down below gives a good overview of the approach.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/clip.png#center"
         alt="CLIP approach overview" width="100%"/> <figcaption>
            <p>Figure 2. CLIP approach overview [3]</p>
        </figcaption>
</figure>

<p align="justify">
The CLIP model is trained using a batch of $N$ image-text pairs. The training objective is to predict which of the $NÃ—N$ possible image-text pairings within the batch actually occurred. To achieve this, CLIP learns a multimodal embedding space by jointly training an image encoder and a text encoder. The goal is to maximize the cosine similarity between the image and text embeddings of the $N$ correct (real) pairs in the batch while minimizing the cosine similarity for the incorrect pairings. For optimization, a symmetric cross-entropy loss, also known as InfoNCE loss, is applied to the similarity scores. The following pseudocode outlines this procedure.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/clip_code.png#center"
         alt="CLIP training pseudocode" width="60%"/> <figcaption>
            <p>Figure 3. CLIP training pseudocode [3]</p>
        </figcaption>
</figure>

<p align="justify">
As the image encoder, the authors trained 5 ResNet and 3 ViT versions and found the ViT-L/14@336px version to perform the best. The text encoder is a 63M parameter Transformer (12 layers) with a vocab size of ~49K. The model showed strong zero-shot performance on ImageNet classification task (same as the original ResNet50). Some of the limitations of CLIP were e.g. that the zero-shot performance on more finegrained vision tasks was quite bad (like differenciating between different car models), on some tasks it was random (like counting objects) and it was not as good as you would expect on simple out of distribution tasks like MNIST (just 88% accuracy).  
</p>
<h4 id="llip">Llip</h4>
<p align="justify">
One of the problems with CLIP was, that there are a thousand ways to caption an image, based on the fact that the caption could describe only specific regions of an image or specific objects. To better model the visual richness of an image, a training objective of a vision language model should aim to capture all the possible text descriptions. This is what the authors of Llip, <b>L</b>atent <b>L</b>anguage <b>I</b>mage <b>P</b>retraining, try to do. To enable the prediction of different representations from a fixed image, they implemented the image-to-text representation function as a one-to-many mapping. This is achieved by augmenting the visual encoder with a latent variable that captures context information. The contextual latent is inferred from the caption and used to modulate the representation. The visual encoder is implemented as a Vision Transformer that outputs $K$ learnable mixture tokens in addition to the visual tokens. These mixture tokens should capture different visual aspects of an image. Figure 4 down below shows this simple modification of CLIP.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/clip_vs_llip.png#center"
         alt="CLIP vs. Llip" width="80%"/> <figcaption>
            <p>Figure 4. CLIP vs. Llip [4]</p>
        </figcaption>
</figure>

<p align="justify">
The authors added a cross-attention mechanism to infer the mixture token weights as a function of the text caption. The weighted mixture defines the contextual representation that is contrasted with text representations. This leads to significant improvement of the visual representation quality as well as a more rich visual representation.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/llip_cross_attention.png#center"
         alt="Llip Cross Attention mechanism" width="90%"/> <figcaption>
            <p>Figure 5. Llip cross-attention mechanism [4]</p>
        </figcaption>
</figure>

<p align="justify">
On zero-shot transfer classification, Llip consistently outperforms CLIP pretraining for architecture of similar size on a large set of benchmarks. Especially on zero-shot image-text and text-image retrieval, Llip consistently outperforms CLIP pretraining on COCO by 6.0% image-to-text retrieval.
</p>
<h3 id="vlms-with-masking-objectives">VLMs with Masking Objectives</h3>
<p align="justify">
Masking is a commonly used technique in deep learning research. It can be viewed as a specific form of denoising autoencoder in which the noise has a spatial structure. In 2019 the authors of the BERT paper used Masked-Language-Modeling (MLM) to predict missing text tokens in a sentence. More recently the same concept (Masked-Image-Modeling) was used in the vision space to learn strong visual representations like in I-JEPA. In the following we are going through two approaches that combined those techniques to train a VLM, FLAVA [5] and MaskVLM [6].
</p>
<h4 id="flava">FLAVA</h4>
<p align="justify">
Contrastive methods like CLIP aren't easy usable on multimodal problems that require dealing with both modalities at the same time. Many of the more recent models that rely on early fusion and shared self-attention across modalities often perform very bad on vision-only or language-only tasks. The goal of the authors was to create a single "foundation" model that is good at vision tasks, language tasks and cross- and multi-modal tasks. FLAVA consists of three models, an image encoder, a text encoder and a multimodal encoder that takes as input the encoded image and text and integrates their represenations for multimodal reasoning.
</p>
<p align="justify">
The image encoder is a ViT-B/16 model with a fixed image size. It outputs is a list of hidden state vectors $\{h_{I}\}$, each corresponding to an image patch, and a classification token $h_{CLS,I}$. The text encoder has the same architecture as the vision part and outputs a hidden state vector $\{h_{T}\}$ and a text classification token $h_{CLS,T}$. The multimodal encoder also is a transformer model that fuses image and text hidden states. Over each of the hidden state vectors generated by the image and text encoder two learned linear projections are applied and an additional $[CLS_M]$ token is added before feeding this into the multimodal encoder. Like the text and image encoders, the multimodal encoder also outputs a list of hidden state vectors $\{h_{M}\}$ and a vector $h_{CLS,M}$ for the classification token. 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/flava_overview.png#center"
         alt="Overview of the FLAVA model architecture" width="100%"/> <figcaption>
            <p>Figure 6. Overview of the FLAVA model architecture [5]</p>
        </figcaption>
</figure>

<p align="justify">
The training process consists of a joint pretraining on both unimodal and multimodal data. First the authors pretrained the text encoder, then the image encoder and then they used both pretrained encoders to train the multimodal encoder. 
For the unimodal pretraining they used established loss components. For the image encoder they used a masked image modeling (MIM) objective like in BEiT. First they tokenize the image patches with a dVAE tokenizer, then they used a classifier on top of the image encoder outputs to predict the missing tokens. Masked language modeling (MLM) was used for the text encoder as an objective with 15% of text tokens masked. For the multimodal pretraining three loss components were used:
</p>
<ul>
<li><b>Global contrastive</b> (GC) loss, like in CLIP</li>
<li><b>Masked Multimodal Modeling</b> (MMM) loss, masking of both the image patches and text tokens</li>
<li><b>Image Text Matching</b> (ITM) loss, by applying a classifier on top of the multimodal encoder to decide if the input image and text match to each other</li>
</ul>
<p align="justify">
One of the cool things of this paper is, that the authors only used public unimodal and multimodal datasets for training, 70 million image-text pairs in total (but the avg. text length was just 12 words). The validated FLAVA on 35 tasks across vision, NLP and multimodal domains and performed better or competitive on all of those tasks with the state of the art models at that time which were mostly trained on much larger and probably cleaner datasets. 
</p>
<h4 id="maskvlm">MaskVLM</h4>
<p align="justify">
Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, the authors propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. This works especially well in scenarios with limited data. Figure 7 illustrates the difference between this new paradigm and the classic MIM and MLM based approaches.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/maskvlm_idea.png#center"
         alt="Left MIM and MLM and right the MaskVLM idea" width="85%"/> <figcaption>
            <p>Figure 7. Left: MIM &amp; MLM; Right: Masked Vision Language Modeling [6]</p>
        </figcaption>
</figure>

<p align="justify">
There are two main types of pre-training objectives in this model. The first is masked vision and language modeling. Here, transformer-based models are used as image and text encoders to extract features from both modalities. These features are then processed by image and text cross-modality encoders, which consist of three cross-attention blocks. These blocks allow the text and image features to interact, enhancing the representation of each by leveraging information from the other. The second objective is multimodal alignment, which includes image-text contrastive learning (ITC) and image-text matching (ITM). These methods align the representations of images and text to ensure they correspond accurately. For more detailed information, you can refer to the original paper.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/maskvlm_architecture.png#center"
         alt="MaskVLM model architecture" width="90%"/> <figcaption>
            <p>Figure 8. Overview of the MaskVLM model architecture [6]</p>
        </figcaption>
</figure>

<p align="justify">
MaskVLM is very data efficient, it especially shines in limited data scenarios where only âˆ¼40% of data used by the state-of-the-art models is sufficient to match their performance.
</p>
<h3 id="generative-based-vlms">Generative-based VLMs</h3>
<p align="justify">
In contrast to the paradigms above, that mostly operate on latent representations we will now look at generative VLM that are trained to generate text and images. We are looking at two methods in more detail, <b>CoCa</b> which learns to generate text and <b>Chameleon</b> which is a multimodal generative model that can generate text and images. Before we delve deeper I will list some of the advantages of generative classifiers and why this training paradigm can be a good idea: 
</p>
<ul>
<li>more effective robustness which means better out-of-distribution performance</li>
<li>better on compositional reasoning tasks than discriminative methods like CLIP</li> 
<li>more shape bias and better alignment with human judgement</li>
<li>can be jointly adapted with discriminative models at test time using only unlabeled test samples which improves classification, segmentation and depth prediction performance</li>
</ul>
<h4 id="coca">CoCa</h4>
<p align="justify">
The <b>Contrastive Captioner (CoCa)</b> is an advanced image-text encoder-decoder foundation model that integrates both contrastive and captioning techniques. Pretrained with a combination of contrastive loss and captioning loss, CoCa combines contrastive methods like CLIP with generative approaches. In CoCa, the decoder is split into two components: a <b>unimodal decoder</b> and a <b>multimodal decoder</b>. The unimodal decoder is responsible for encoding text-only representations and omits cross-attention, while the multimodal decoder employs cross-attention to interact with image encoder outputs, learning multimodal image-text representations. The model is trained with a contrastive loss between unimodal image and text embeddings, alongside a captioning loss applied to the multimodal decoder's outputs, which autoregressively predict text tokens. By sharing the computational graph, CoCa efficiently computes both training objectives with minimal overhead.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/coca_architecture.png#center"
         alt="CoCa model architecture, training objectives and pseudocode" width="100%"/> <figcaption>
            <p>Figure 9. Overview of the CoCa model architecture and training objectives [10]</p>
        </figcaption>
</figure>

<p align="justify">
CoCa is pretrained from scratch by treating annotated image labels as text. This pretraining process utilizes two major datasets: ALIGN, which contains approximately 1.8 billion images paired with alt-text, and JFT-3B, an internal dataset with over 29.5k classes. In the case of JFT-3B, the labels are treated as alt-text for training purposes. This pretraining setup enables CoCa to transfer quickly to downstream tasks, either through zero-shot transfer or with minimal task-specific adaptation, making it highly versatile for a range of applications.
</p>
<h4 id="cm3leon">CM3leon</h4>
<p align="justify">
CM3leon is a retrieval-augmented, token-based, decoder-only multimodal system designed for generating both text and images. It is built upon the CM3 model architecture, which has shown significant benefits from scaling and instruction tuning. In CM3, images are transformed into 1024 tokens using a VQGAN (Vector Quantized Generative Adversarial Network). The training process follows a recipe adapted from text-only language models. This includes a large-scale retrieval-augmented pretraining phase, followed by a second stage of multi-task supervised finetuning to enhance the modelâ€™s performance across different tasks.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/ra-cm3_architecture.png#center"
         alt="RA-CM3 model architecture" width="100%"/> <figcaption>
            <p>Figure 10. Overview of the RA-CM3 model architecture and training pipeline [12]</p>
        </figcaption>
</figure>

<p align="justify">
The training objective for the model is based on an infill approach, where specific spans are masked and relocated to the end of the sequence, followed by the use of standard next-token prediction loss. Despite using only a fraction of the training data and compute, the model achieves zero-shot results that are on par with state-of-the-art models, such as on the MS-COCO benchmark. The authors further fine-tuned the model for instructable image generation and conditional text generation, yielding similar results as in the zero-shot settings. Notably, the model can compete with state-of-the-art systems like Flamingo and OpenFlamingo, even though those models were exposed to significantly more tokens (3B vs. 100B or 40B).
</p>
<h3 id="vlms-from-pretrained-backbones">VLMs from Pretrained Backbones</h3>
<p align="justify">
Training models from scratch is prohibitively expensive, requiring hundreds or even thousands of GPUs and millions of image-text pairs. To mitigate these high costs, a growing area of research focuses on leveraging existing unimodal models rather than building new systems from the ground up. This approach involves learning to map between the text and image modalities, which significantly reduces the amount of compute resources required. In this section, we will explore some of the best open-source vision-language models available, including the Idefics series, InternVL1.5 and 2, Qwen2-VL and MiniCPM-2.6.
</p>
<h4 id="idefics">Idefics</h4>
<p align="justify">
The Idefics model family currently includes three versions that build on one another, each advancing the capabilities of the previous model.
</p>
<p align="justify">
<b>Idefics1</b> introduces the OBELICS dataset, a significant milestone in this model family. The dataset is composed of interleaved image-text documents, derived from 141 million web pages extracted from Common Crawl. It contains a total of 353 million associated images and 115 billion text tokens. One of the standout features of the OBELICS dataset is the extensive detail and quantity of text per image, which surpasses other image-text datasets in size and richness. Figure 9 below illustrates the complete dataset creation process.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/obelics.png#center"
         alt="Generation process of the OBELICS dataset" width="90%"/> <figcaption>
            <p>Figure 9. Overview of the OBELICS generation process [7]</p>
        </figcaption>
</figure>

<p align="justify">
Using the OBELICS dataset, the authors developed two vision-language models under the name "Idefics," featuring versions with 9 billion and 80 billion parameters. These models are based on the Flamingo architecture, which integrates two frozen unimodal backbones: Llama, serving as the language encoder, and OpenCLIP, handling the vision component. To bridge these two encoders, the model incorporates learnable cross-attention Transformer blocks and Perceiver blocks, enabling effective communication between the unimodal systems. The Idefics models demonstrate top performance when evaluated on a combination of the LAION and OBELICS datasets.
</p>
<p align="justify">
<b>Idefics2</b> focuses on exploring key factors that influence the development of VLMs, specifically investigating two critical design choices: the model architecture â€” particularly how image and text information are fused â€” and the training procedure. The main findings from this research are as follows:
</p>
<ul>
<li>Progress in vision-language models is largely driven by advances in pretrained unimodal backbones. Notably, the language model (LLM) plays a more crucial role than the vision encoder, although there is currently a lack of high-quality vision encoders. The authors suggest that models like InternVL could challenge this conclusion.</li>
<li>Fully autoregressive architectures outperform cross-attention models, but they require adjustments to the optimization process to maintain training stability.</li>
<li>Reducing the number of visual tokens through learned pooling significantly enhances computational efficiency and improves performance on downstream tasks. However, while efficiency gains are clear, performance improvements are difficult to validate without a superior vision encoder.</li>
<li>Splitting images into subimages allows for a trade-off between computational efficiency and performance during inference, particularly for tasks that involve text reading.</li>
</ul>
<p align="justify">
Based on these insights, the authors developed Idefics2, an 8-billion parameter vision-language model that achieves state-of-the-art performance within its size category.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/idefics2.png#center"
         alt="Idefics2 model architecture" width="90%"/> <figcaption>
            <p>Figure 10. Idefics2 model architecture [8]</p>
        </figcaption>
</figure>

<p align="justify">
The vision encoder used in Idefics2 is SigLIP-SO400M, while the LLM is Mistral-7B-v0.1. The model was trained using a variety of datasets, including OBELICS, LAION COCO, PMD, OCR-IDL, and PDFA.
</p>
<p align="justify">
<b>Idefics3</b> marks a significant improvement over its predecessor, Idefics2, particularly in tasks related to document understanding. This model was exclusively trained on open datasets, showcasing its accessibility and transparency. One of the key advancements in Idefics3 is the introduction of the Docmatix dataset, which includes 2.4 million images and 9.5 million question-answer pairs derived from 1.3 million PDF documents. For more details on how this dataset was created, the authors recommend consulting the original paper.
</p>
<p align="justify">
Idefics3 is built on Llama 3.1 Instruct for the language model and retains SigLIP-SO400M as the vision encoder. A notable architectural change from Idefics2 is the replacement of the perceiver resampler with a simpler pixel shuffle strategy. This technique acts as a pooling method that reduces the number of image hidden states by a factor of four, encoding images of up to 364x364 pixels into 169 visual tokens.
</p>
<p align="justify">
During both training and inference, the model utilizes an image-splitting strategy, where the original image is divided into a matrix of tiles, each measuring 364x364 pixels, and the downscaled image is appended at the end. The training process consists of three pre-training stages, followed by supervised fine-tuning.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/idefics3_training.png#center"
         alt="Idefics3 training stages overview" width="90%"/> <figcaption>
            <p>Figure 11. Overview of Idefics3 training stages [9]</p>
        </figcaption>
</figure>

<p align="justify">
In the first pretraining stage of Idefics3, the modelâ€™s backbones remain frozen to maintain their performance while the newly initialized parameters are learned. During this phase, the maximum image resolution is gradually increased from 364x364 to 1820x1820 pixels. Starting from the second stage, the backbones are trained using DoRA (a variant of LoRA, explained in the Training section) and with larger images. The final pretraining stage focuses on using large synthetic datasets for further training.
During the supervised fine-tuning stage, NEFTune noise is applied to the inputs, and the loss is calculated only on the answer tokens to refine performance.
</p>
<p align="justify">
The authors evaluated Idefics3 on several common benchmarks, such as MMMU, MathVista, MMStar, DocVQA, and TextVQA. The most notable improvements over Idefics2 were seen in document understanding tasks, where Idefics3 achieved a significant performance boost of 13.7 points on DocVQA.
</p>
<p align="justify">
While Idefics3 shows significant improvement over Idefics2, a comparison with other vision-language models of a similar size, such as InternVL2-8B (which scores 91.6 on DocVQA versus Idefics3â€™s 87.7), would have been valuable, especially on tasks like OCRBench.
</p>
<h4 id="internvl">InternVL</h4>
<p align="justify">
InternVL is a large-scale vision-language foundation model designed to address the limitations of traditional methods in aligning vision and language models. By scaling up the vision model to 6 billion parameters and progressively aligning it with a LLM, InternVL aims to create a more effective integration of these two domains. One major challenge in this integration is the disparity in parameter scales between vision models and LLMs, as well as the inconsistent representations and inefficient connections that fail to fully capture the rich cross-modal interactions. To address these shortcomings, InternVL introduces a novel architecture based on three key design principles:
</p>
<ol>
<li><b>Parameter-balanced Vision and Language Components</b>: The vision encoder in InternVL is scaled up to 6 billion parameters, complemented by an LLM middleware with 8 billion parameters. The middleware serves as a crucial connection layer, reorganizing visual features based on user commands, thus facilitating a more balanced interaction between the vision and language components.</li>
<li><b>Consistent Representations</b>: To ensure a more cohesive connection between the vision and language models, InternVL utilizes a pretrained multilingual LLaMA model to initialize the middleware. This ensures that the vision encoder is effectively aligned with the LLM, leading to consistent cross-modal representations.</li>
<li><b>Progressive Image-Text Alignment</b>: InternVL employs a two-step learning process for image-text alignment. It begins with contrastive learning on large-scale noisy image-text data and gradually transitions to generative learning on fine-grained data. This progressive alignment ensures a consistent improvement in the modelâ€™s performance and adaptability across various tasks.</li>
</ol>
<p align="justify">
Thanks to these innovations, InternVL can seamlessly integrate with other large language models, enhancing its versatility and capacity to handle complex vision-language tasks.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/internvl1_architecture.png#center"
         alt="InternVL1 architecture overview" width="70%"/> <figcaption>
            <p>Figure 12. Overview of InternVL1 model components [16]</p>
        </figcaption>
</figure>

<p align="justify">
Figure 12 illustrates the overall architecture of InternVL, highlighting its key components and design. The model's large-scale vision encoder, named InternViT-6B, is based on a vanilla vision transformer. The optimal configuration of InternViT-6B was determined through an extensive hyperparameter search, evaluating different depths, head dimensions, and MLP ratios to maximize its performance. The language middleware, called QLLaMA, is based on a multilingual LLaMA model. This middleware is enhanced with 96 additional learnable queries and cross-attention layers, adding over 1 billion parameters. These enhancements allow QLLaMA to effectively integrate visual elements into the language model, creating a smooth interaction between the vision and language components. By flexibly combining InternViT-6B and QLLaMA, InternVL is capable of supporting a wide range of tasks, including vision-specific tasks like image classification, vision-language tasks like image-text retrieval, as well as generative tasks and multimodal dialogs. This versatility makes InternVL highly adaptable for various vision and vision-language applications.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/internvl1_training.png#center"
         alt="InternVL1 training stages overview" width="100%"/> <figcaption>
            <p>Figure 13. Overview of InternVL1 training stages [16]</p>
        </figcaption>
</figure>

<p align="justify">
As illustrated in Figure 13, the training process of InternVL follows a progressive three-stage approach, consisting of vision-language contrastive training, vision-language generative training, and supervised fine-tuning. Each stage utilizes public data from diverse sources, ranging from noisy image-text pairs to high-quality datasets for captions, visual question answering (VQA), and multi-modal dialogues. A comprehensive overview of the data used is provided in the paper, offering detailed insights into the datasets employed.
</p>
<ol>
<li><b>Vision-Language Contrastive Training</b>: This stage leverages approximately 5 billion image-text pairs sourced from publicly available datasets such as LAION-en, LAION-multi, and Wukong. The objective function used here is the same as that of CLIP, enabling the model to learn strong image-text alignments from large-scale noisy data.</li>
<li><b>Vision-Language Generative Training</b>: In this phase, InternViT-6B (the vision encoder) is connected to QLLaMA (the language middleware), but both components remain frozen. Only the added learnable queries and cross-attention layers are trained using around 1 billion high-quality samples. The training employs the BLIP-2 loss function, which consists of three key components: image-text contrastive (ITC) loss, image-text matching (ITM) loss, and image-grounded text generation (ITG) loss, ensuring a robust generative capability for the model.</li>
<li><b>Supervised Fine-tuning</b>: This final stage demonstrates InternVL's effectiveness in creating multi-modal dialogue systems. It involves connecting InternVL to a larger LLM, such as Vicuna-13B, via an MLP layer. The fine-tuning process uses 4 million instruction data samples to train the model, yielding strong performance even when the LLM decoder remains frozen.</li>
</ol>
<p align="justify">
Through this progressive training strategy, InternVL achieves robust performance across various vision-language tasks, from contrastive learning to multi-modal dialogues.
</p>
<p align="justify">
<b>InternVL1.5</b> introduces three key improvements over its predecessor, InternVL1:
</p>
<ol>
<li><b>Enhanced Vision Encoder</b>: A continuous learning strategy is employed to improve the visual understanding capabilities of the InternViT-6B model. This enables stronger and more accurate visual representation.</li>
<li><b>Dynamic High-Resolution Image Processing</b>: The model now divides images into tiles sized 448x448 based on their aspect ratio and resolution, allowing for better handling of high-resolution images while maintaining efficiency.</li>
<li><b>High-Quality Bilingual Dataset</b>: A comprehensive, high-quality bilingual dataset was collected, featuring common scenes and document images. This significantly improves the model's OCR-related capabilities, making it more adept at understanding text in visual data.</li>
</ol>
<p align="justify">
Additionally, the architecture has evolved. Instead of the middleware used in InternVL1, the authors implemented a ViT-MLP-LLM structure. This is combined with Pixel Shuffle to reduce the number of visual tokens and uses an MLP Projector as a connection mechanism, optimizing performance and efficiency.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/internvl1_5.png#center"
         alt="InternVL1.5 architecture and image slicing" width="100%"/> <figcaption>
            <p>Figure 14. Overview of InternVL1.5 architecture and image slicing [17]</p>
        </figcaption>
</figure>

<p align="justify">
To enhance scalability for high-resolution images, InternVL1.5 employs a pixel shuffle operation that reduces the number of visual tokens to one-quarter of the original. As a result, a 448x448 image is represented by just 256 visual tokens, significantly improving processing efficiency. The multilingual dataset used for training was constructed from openly available datasets tailored for various tasks, which were carefully filtered to ensure quality. For more detailed information on the specific datasets used for both pretraining and fine-tuning, refer to the tables provided in the paper. InternVL1.5 particularly excels in OCR-related tasks and TextVQA benchmarks, outperforming even proprietary models such as GPT-4V and Claude-3 Opus. This demonstrates the model's superior capabilities in understanding and interpreting visual-textual information.
</p>
<p align="justify">
<b>InternVL2</b> is the most current iteration of the model family that ranges in size from 1 billion to 108 billion parameters. While there is no official paper available yet, it is described as being on par with current commercial closed-source models in terms of capabilities. The model is built on several key design principles:
</p>
<ol>
<li><b>Progressive Alignment with LLMs</b>: InternVL2 employs a progressive alignment training strategy, making it the first vision foundation model to be natively aligned with large language models (LLMs). This training strategy scales the model from small to large sizes, while the data used for training evolves from coarse to fine detail. This approach results in more cost-effective training while delivering excellent performance.</li>
<li><b>Multitask Output</b>: The model supports a variety of output formats, including images, bounding boxes, and masks. By linking the model with task-specific decoders, it can generalize across hundreds of vision-language tasks, making it highly versatile.</li>
</ol>
<p align="justify">
The pretraining process extends the stage 1 datasets used for InternVL1.5 with data from diverse sources such as Testbank, OmniCorpus, Kaptest, and more. The stage 2 training uses the same data as InternVL1.5. For more detailed information on the architectural differences, the codebase is available for review (I was to lazy to explore it myself).
</p>
<h4 id="qwen2-vl">Qwen2-VL</h4>
<p align="justify">
Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which allows the model to dynamically process images of varying resolutions by converting them into different numbers of visual tokens. The goal of this innovation is to achieve more efficient and accurate visual representations. In addition, the model incorporates Multimodal Rotary Position Embedding (M-RoPE), designed to facilitate the integration of positional information across multiple modalities such as text, images, and videos. The Qwen2-VL series is available in three model sizes: 2 billion, 7 billion, and 72 billion parameters. All these models utilize a 675 million parameter Vision Transformer (ViT) across the various-sized Qwen2 LLMs.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/qwen2vl.png#center"
         alt="Qwen2-VL architecture overview" width="100%"/> <figcaption>
            <p>Figure 15. Overview of Qwen2-VL architecture [19]</p>
        </figcaption>
</figure>

<p align="justify">
The Qwen2-VL model introduces two key architectural enhancements: Naive Dynamic Resolution and Multimodal Rotary Position Embedding (M-RoPE).
</p>
<p align="justify">
The <b>Naive Dynamic Resolution</b> mechanism enables the model to process images of varying sizes by dynamically converting them into a variable number of visual tokens. It removes the original absolute position embeddings of the Vision Transformer (ViT) and instead introduces 2D-RoPE, which is capable of capturing two-dimensional positional information from images. To further streamline image processing, an MLP layer is employed to compress groups of 2x2 tokens into a single token. For example, an image with a resolution of 224x224 is compressed into 66 tokens, making the process more efficient.
</p>
<p align="justify">
The second enhancement, <b>Multimodal Rotary Position Embedding</b> (M-RoPE), is designed to effectively model the positional information of multimodal inputs by decomposing the original rotary embeddings into three distinct components: temporal, height, and width. For text inputs, these components share identical position IDs, making the mechanism functionally equivalent to the standard 1D-RoPE. When processing images, the temporal IDs of visual tokens remain constant, while the height and width components are assigned distinct IDs based on their positions in the image. For videos, the temporal ID increases with each frame, while the width and height components follow the same assignment pattern as in the case of images. This innovative approach not only improves the modeling of positional information but also reduces the reliance on position IDs, allowing the model to generalize to longer sequences during inference. Together, these enhancements make the Qwen2-VL model more efficient and effective in handling multimodal data, with improved extrapolation capabilities for images and videos.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/mrope.png#center"
         alt="Multimodal Rotaty Position Embeddings" width="100%"/> <figcaption>
            <p>Figure 16. Overview of Multimodal Rotary Position Embeddings [19]</p>
        </figcaption>
</figure>

<h4 id="minicpm-v">MiniCPM-V</h4>
<p align="justify">
Despite the advancements in Vision-Language Models (VLMs), several challenges still prevent their widespread use in real-world applications. One of the most significant obstacles is the high cost associated with running these large models. Typically, VLMs need to be deployed on high-performance cloud servers, which significantly limits their application in mobile, offline, or privacy-sensitive environments. Addressing these challenges, the MiniCPM model family offers a promising solution. These models demonstrate strong performance on general benchmarks, particularly excelling in Optical Character Recognition (OCR) tasks. Furthermore, they provide multilingual support for over 30 languages and, importantly, can be run on mobile devices, expanding their usability to a broader range of applications.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/minicpm.png#center"
         alt="MiniCPM-V-2.5 model architecture" width="100%"/> <figcaption>
            <p>Figure 17. MiniCPM-V-2.5 model architecture [20]</p>
        </figcaption>
</figure>

<p align="justify">
The model architecture consists of three key modules: a visual encoder, a compression layer, and a large language model (LLM). The input image is encoded by a SigLIP SoViT-400m/14, using the adaptive visual encoding approach proposed by LLaVA-UHD. The compression layer has a perceiver resampler structure with a single-layer cross-attention mechanism. Once compressed, the visual tokens, together with the text input, are fed into the LLM for conditional text generation.
</p>
<p align="justify">
The process follows a series of steps: image partitioning, slice encoding (with resizing to match the ViT input size), and token compression. Specifically, each image slice is represented by 1024 tokens, which are then compressed using the cross-attention layer to 64 or 96 tokens. Training this model involves three phases:
</p>
<ol>
<li><b>Pretraining</b>
    <ul>
        <li><b>Stage 1</b>: Train only the compression layer that connects the visual encoder to the LLM, keeping all other parameters frozen.</li>
        <li><b>Stage 2</b>: Extend the input resolution of the visual encoder from 224x224 to 448x448.</li>
        <li><b>Stage 3</b>: Train the visual encoder using the adaptive visual encoding strategy.</li>
    </ul>
</li>
<li><b>Supervised Fine-Tuning</b>: In this phase, the model undergoes fine-tuning on high-quality visual question-answering datasets, labeled by humans or strong models like GPT-4. During this stage, all parameters are unlocked and trained together.</li>
<li><b>Trustworthiness and Hallucination Reduction</b>: This phase utilizes the RLAIF-V framework, which is designed to enhance the modelâ€™s trustworthiness and minimize hallucinations. The key here is gathering scalable, high-quality feedback from open-source models to conduct Direct Preference Optimization (DPO). Figure 12 outlines the three steps involved in this method: Response Generation, Feedback Collection, and DPO.</li>
</ol>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/rlaif.png#center"
         alt="RLAIF-V framework for hallucination reduction" width="100%"/> <figcaption>
            <p>Figure 18. RLAIF-V framework for hallucination reduction [20]</p>
        </figcaption>
</figure>

<p align="justify">
For more information on the datasets used, the methods for improving the quality of training data, or the advancements in on-device deployments, please refer to the detailed discussions in the paper.
The latest model in the MiniCPM-V series, MiniCPM-V 2.6, offers significant improvements over its predecessor, MiniCPM-V 2.5. However, as of now, there is no paper available that outlines the architecture, datasets, or training procedure for this latest version. Despite this, the model has achieved an impressive score of 65.2 on OpenCompass, a remarkable performance for a model of its size. It even outperforms well-known models like GPT-4o, showcasing its advanced capabilities.
</p>
<h2 id="training">Training</h2>
<p align="justify">
Scaling is often key to improving the performance of Visual Language Models (VLMs). However, the vast amount of computational resources required to train large-scale models is out of reach for many companies and academic labs. The cost and infrastructure needed to access this level of compute create barriers to participation in this rapidly advancing field.
Recently, though, researchers have discovered a way to overcome these challenges: a well-designed data curation pipeline. By carefully selecting and curating the training data, it is possible to surpass the typical scaling laws that have dominated model development. In this section, weâ€™ll explore how crucial high-quality data is in training VLMs and the recipes used to create datasets that drive superior performance.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/training.png#center"
         alt="Important training decisions to make" width="100%"/> <figcaption>
            <p>Figure 19. Important decisions to make when training VLMs [2]</p>
        </figcaption>
</figure>

<p align="justify">
Weâ€™ll explore the crucial role of data in Visual Language Model (VLM) training, focusing on the recipes used to create high-quality datasets (Figure 19). Additionally, weâ€™ll discuss grounding, ensuring the model accurately links visual content to text, along with alignment to human preferences, and provide tips for selecting the right model architecture for your specific use case. 
</p>
<h3 id="data">Data</h3>
<p align="justify">
The quality and organization of data play a critical role in the successful training of Vision-Language Models (VLMs). Due to the complex nature of these models, which need to understand both visual and textual inputs, effective data management through pruning, augmentation, and quality control is essential to achieving optimal performance. Data pruning is a crucial step in refining the datasets used to train VLMs. Pruning techniques fall into three main categories:
</p>
<ul>
<li><b>Heuristic-based pruning</b> involves the use of rules to eliminate low-quality image-text pairs. This can be done through unimodal filtering, where low-quality captions (e.g., lacking complexity or containing non-English text) or inappropriate images (based on size or aspect ratio) are removed. Multimodal filtering further refines data by using image classifiers to detect when objects in the image do not align with the associated text, or by filtering out examples where the majority of text in the caption appears directly within the image itself.</li>
<li><b>Bootstrapping methods</b> leverage pre-trained VLMs to rank image-text pairs based on their multimodal alignment, using metrics such as CLIPScore. This approach helps prioritize data that better represents meaningful relationships between the visual and textual components, further refining the dataset for training.</li>
<li><b>Diversity and balance techniques</b> aim to create datasets that not only contain high-quality pairs but also represent a wide variety of content. Ensuring diversity in objects, actions, and attributes across different modalities can prevent bias and improve the modelâ€™s generalization capability.</li>
</ul>
<p align="justify">
In addition to pruning, the generation of synthetic data has become an effective strategy for enhancing VLM performance. For example, LLMs can generate captions, which can then be used by text-to-image models to create corresponding images. This helps to supplement the training data, especially in areas where real-world examples may be limited or lacking. Data augmentation techniques, such as adding variations to existing image-text pairs (e.g., rotating or cropping images, rewording captions), further contribute to improving model robustness by exposing the VLM to a wider range of inputs.
</p>
<p align="justify">
Data quality is paramount when training VLMs or any other AI model, but assessing the quality of multimodal and interleaved data remains a challenge. The lack of a standardized method for evaluating such data complicates the task, as quality often depends on the subjective judgment of what constitutes a "good" image-text pair. This is an active area of research, as models continue to evolve and demand increasingly refined datasets for training. Despite the challenges, curated datasets such as OBELICS, which contain interleaved data from multiple modalities, have shown promising results. These datasets are carefully constructed to ensure a rich variety of content, making them valuable resources for VLM training.
</p>
<p align="justify">
In summary, data plays a foundational role in training vision-language models, and techniques such as data pruning, augmentation, and synthetic data generation are essential for improving VLM performance. However, assessing data quality remains an open challenge that continues to evolve alongside advancements in multimodal learning.
</p>
<h3 id="grounding">Grounding</h3>
<p align="justify">
Visual Language Models (VLMs) are powerful tools, but they still face significant challenges when it comes to accurately understanding and grounding text prompts. These models sometimes misunderstand or misrepresent the instructions given to them, leading to either ignoring important aspects of the prompt or hallucinating elements that arenâ€™t even part of it.
One of the main issues is their difficulty in comprehending relationships and specific attributes. For example, understanding spatial relations like whether an object is on the left or right, interpreting negations, counting objects, or recognizing attributes such as color and texture are areas where models can stumble. These problems underscore the need for more robust techniques that ensure VLMs are grounded and capable of faithfully interpreting prompts.
Currently, no single solution can resolve these challenges, and this remains an active area of research. However, some methods and tricks have been developed to enhance grounding performance in certain scenarios.
</p>
<p align="justify">
<b>Bounding box annotations</b> offer one such solution, helping VLMs connect visual elements to their corresponding textual descriptions. This method incorporates box regression and Intersection over Union (IoU) loss, which aids in locating and aligning visual concepts with their textual counterparts. Existing datasets like COCO can be employed for this purpose, or detection models can be leveraged to assign bounding boxes to caption nouns. This step significantly improves the model's ability to understand where specific objects are in the image, ensuring a tighter correspondence between what is described and what is visualized.
</p>
<p align="justify">
<b>Negative captioning</b>, often used in contrastive learning objectives, is another technique that helps address the problem of grounding. Negative samples are extensively utilized to prevent model collapse, enhance generalization, and improve the learning of discriminative features. Applying similar techniques to VLMs can mitigate issues like misunderstanding the prompt or hallucination. Results from benchmarks such as <a href="https://github.com/mertyg/vision-language-models-are-bows">ARO</a> have demonstrated the effectiveness of this method, showing its potential to correct misinterpretations and improve overall model performance.
</p>
<h3 id="alignment">Alignment</h3>
<p align="justify">
To improve multimodal chat capabilities in Vision-Language Models (VLMs), several techniques such as instruction fine-tuning, Reinforcement Learning from Human Feedback (RLHF), and in-context learning are applied. These approaches are designed to align model outputs with the desired responses and enhance the models' performance.
</p>
<p align="justify">
<b>Instruction-tuning</b> is the process of fine-tuning a VLM on supervised datasets that contain instructions, text, image inputs, and the expected responses. While these datasets are typically much smaller than the vast pretraining datasetsâ€”ranging from 100 to 100,000 samplesâ€”they are crucial for helping the model learn how to interpret and respond to multimodal inputs effectively.
</p>
<p align="justify">
<b>Reinforcement Learning from Human Feedback</b> (RLHF) is another technique aimed at aligning model outputs with human preferences. In this approach, a reward model is trained to assess and prioritize responses based on how well they match human feedback. This helps in guiding the model toward generating outputs that are more in line with user expectations.
</p>
<p align="justify">
Similar to in-context learning in text-based models, in-context learning can also be applied to VLMs. By providing a set of instruction-image-answer examples in the modelâ€™s context, the model can learn to generalize and follow instructions more effectively in multimodal scenarios. For instance, if examples include similar instructions with different images or images that follow a sequence but differ in the accompanying instructions, the model can still successfully produce appropriate responses.
</p>
<p align="justify">
A key dataset used for such training is the MIMIC-IT dataset, which contains 2.8 million in-context instruction-image-answer tuples. These tuples are relevant to the test example in one of three ways: either the instructions are similar but the images are different, the images are the same but the instructions vary, or the images follow a sequential order while the instructions differ. This setup helps the model learn to interpret both the instructions and images in context, improving its ability to respond appropriately to multimodal inputs.
</p>
<h3 id="parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning</h3>
<p align="justify">
As vision-language models (VLMs) continue to grow in size, fine-tuning all their parameters for each specific downstream task has become increasingly impractical due to computational constraints. To overcome these challenges, researchers have developed Parameter-Efficient Finetuning (PEFT) methods. Instead of adjusting the entire set of parameters in a model, PEFT methods focus on fine-tuning only a subset of parameters, making the process more efficient while still allowing the model to adapt to new tasks.
These PEFT methods can be categorized into four main groups: Low-Rank Adapters (LoRA)-based methods, Prompt-based methods, Adapter-based methods, Mapping-based methods.
</p>
<p align="justify">
<b>LoRA-based methods</b>: very popular method which can be applied to both pure language and vision-language models. Instead of fine-tuning all parameters, LoRA focuses on adjusting only a smaller part of them, by inserting a small matrix inside the model, which is then trained to learn new information while keeping the main model mostly the same. This is not only much cheaper but also faster than a full fine-tuning. In addition you can train different LoRA adapters for different tasks and switch them in and out without changing the base model. Several variants of LoRA have been developed to enhance its functionality and efficiency, like QLoRA, VeRA or DoRA.
</p>
<p align="justify">
<b>Prompt-based methods</b>: Context Optimization (CoOp), which is a technique designed to adapt pre-trained VLMs for downstream image recognition tasks, eliminating the need for manual prompt engineering. It does that by optimizing the context of the prompt using learnable vectors during the training process. Experiments from 11 datasets indicate that CoOp outperforms handcrafted prompts and linear probe models in few shot learning. Another interesting method is called Visual Prompt Tuning (VPT) that adapts Transformers models in vision by introducing a small amount of trainable parameters in the input space (task-specific learnable prompts). 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/coop_vpt.png#center"
         alt="Overview of prompt-based methods to improve vision language models" width="100%"/> <figcaption>
            <p>Figure 20. Overview of prompt-based methods for finetuning VLMs [14] &amp; [15]</p>
        </figcaption>
</figure>

<p align="justify">
<b>Adapter-based methods</b>: Adapters are new modules that are inserted between the layers of a pre-trained network to enhance its capabilities. Examples include CLIP-Adapter, VL-Adapter, and Llama-Adapter V2. The CLIP-Adapter method involves appending a small number of additional learnable bottleneck linear layers to the language and image branches of CLIP. During few-shot fine-tuning, the original CLIP backbone remains frozen. To address overfitting and improve the robustness of the CLIP-Adapter, residual connections are introduced. These connections dynamically blend the fine-tuned knowledge with the original knowledge from CLIPâ€™s backbone, enhancing performance without compromising the integrity of the pre-trained model.
</p>
<p align="justify">
<b>Mapping-based methods</b>: A simpler approach than LoRA is available, one that does not require detailed knowledge of the network's architecture. This method involves training a mapping between pre-trained unimodal modules while keeping these modules completely frozen, with no adapter layers added. This approach is more efficient, as it requires fewer trainable parameters and results in increased data efficiency. Notable examples include LiMBeR, which uses a linear layer for projection, and MAPL, which designs a more complex mapping network to achieve similar goals.
</p>
<h2 id="evaluation--benchmarks">Evaluation &amp; Benchmarks</h2>
<p align="justify">
It is essential to measure visio-linguistic abilities to ensure that words accurately correspond to visual cues. In this section, we review various evaluation methods for VLMs, including Visual Question Answering (VQA), zero-shot predictions on tasks such as ImageNet classification, and visio-linguistic compositional reasoning, such as the Winoground benchmark.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/vlms/evaluation.png#center"
         alt="Overview of evaluation methods for vision language models" width="90%"/> <figcaption>
            <p>Figure 21. Overview of evaluation methods for VLMs [2]</p>
        </figcaption>
</figure>

<p align="justify">
In the following, we will delve into visio-linguistic benchmarking methods, as they are key to assessing how effectively a VLM has learned the mapping between visual and linguistic information.
</p>
<p align="justify">
<b>Image Captioning</b>: The COCO captioning dataset and challenge are widely used to evaluate the quality of captions generated by VLMs. These evaluations are based on metrics such as BLEU and ROUGE, which are heuristic measures designed to approximate the similarity between the generated captions and reference captions. Another method, CLIPScore, leverages CLIP to predict how closely a caption matches an image. However, while CLIPScore provides an alternative perspective on caption quality, it is not without its limitations, as CLIP itself has certain shortcomings.
</p>
<p align="justify">
<b>Text-to-Image Consistency</b>: 
Refers to the evaluation of a model's ability to generate an image that accurately reflects a given caption. Several approaches are used to assess this consistency, including formatting the text prompt as a question and using a Visual Question Answering (VQA) model to determine the probability of answering "yes".
For instance, <b>TIFA</b> and <b>Davidsonian Scene Graph</b> (DSG) utilize a language model (LM) to generate natural language binary and multiple-choice questions, which are then evaluated by a VQA model. Another method, <b>QAScore</b>, also relies on VQA but differs in its approach. Instead of generating questions via an LM, it passes the original text prompt directly to a VQA model to assess the consistency between the text and the generated image.
</p>
<p align="justify">
<b>Visual Question Answering</b>: Is the task of answering natural questions about images, making it a core evaluation method for Vision-Language Models (VLMs). In fact, many VLM tasks can be reformulated as VQA tasks. For example, image captioning can be expressed as "What is in the image?", while grounding can be reframed as "Where is it?" There are several popular VQA benchmarks, including VQAv2, TextVQA, GQA, VizWiz-QA, and MMMU, among others. The primary metric used for evaluation is VQA accuracy, which relies on exact string matching between the model's candidate answer and a set of reference answers provided by human annotators. However, in newer generative settings, this exact matching metric can be too strict, often underestimating the true performance of modern VLM systems. To address this, some methods involve artificially constraining or rephrasing the model's output to match the format of the ground truth answers. Another approach is to use a large language model (LLM) to evaluate the quality of the answers instead of relying solely on strict string matching.
In addition to evaluating the correctness of answers, another important dimension for VQA is selective prediction. This measures how effectively a VLM can abstain from answering questions it is likely to get wrong, while maintaining high accuracy on the questions it chooses to answer. This capability is crucial for building more robust and reliable models. An extended form of VQA evaluation is provided by the <b>VisDial</b> dataset and benchmark. VisDial presents a series of questions about a single image, aiming to assess the model's ability to engage in a coherent and meaningful discussion about that image. This goes beyond simple question-answer pairs, measuring how well the model can maintain context and continuity throughout an interactive dialogue.
</p>
<p align="justify">
<b>Zero-shot Image Classification</b>:
- evaluating a model on a classification task for which the model wasn't explicitly trained 
- since prompt engineering, e.g. using concept names within human-engineered prompt templates can substantially enhance zero-shot performance 
- recent methods employ LLMs to automatically generate prompts, often with rich visual descriptions 
- performance here depends mostly on the fact that the training data contains examples of the respective classes  
</p>
<p align="justify">
<b>Visio-linguistic Compositional Reasoning</b>: Introduces benchmarks that challenge models with artificially created captions designed to create ambiguity, such as reordering the words. These benchmarks evaluate the model's ability to discriminate between the correct caption and a perturbed one. One prominent task in this domain is Winoground, which assesses visio-linguistic abilities using a dataset where each example includes two images and two captions. One caption matches one image, and the task for the model is to score the correct image-caption pairs higher than the incorrect ones. The only difference between the captions is often the word order, making it a test of the model's ability to understand nuanced language composition. Another benchmark, Attribution, Relation, and Order (ARO), evaluates a model's understanding of relationships, attributes, and word order. Built using the GQA, COCO, and Flickr30k datasets, ARO generates negative captions by swapping either the relations, attributes, or order of words from the original captions. This benchmark provides a structured way to test a model's compositional understanding across these key dimensions.
</p>
<p align="justify">
<b>Dense Captioning and Crop-Caption Matching</b>: Address the limitations of VLMs when generating text descriptions, as many models are constrained by the size of their text tokenizers. For instance, the CLIP Tokenizer can only generate a maximum of 77 tokens, which is insufficient to provide a rich and detailed description of complex images. To overcome this limitation, the Densely Captioned Images (DCI) dataset was introduced. This dataset contains 7,805 images, each accompanied by captions that exceed 1,000 words, offering complete and detailed descriptions of the visual content. This allows models to learn from more comprehensive image descriptions, enabling better performance in generating and understanding detailed captions.
</p>
<p align="justify">
<b>Synthetic Data based visio-linguistic evaluations</b>: Address some of the challenges inherent in using real-world data for evaluating VLMs. Real data often comes with limitations: captions are typically simple, can contain ambiguities or biases, and may not always be suitable for evaluating VLMs effectively. Additionally, it can be difficult to find real images that naturally align with negative or incorrect captions, limiting the ability to fully assess model performance. Synthetic datasets provide a solution by allowing precise control over each scene and producing granular ground truth labels. An example is the Photorealistic Unreal Graphics (PUG) dataset, which constructs complex scenes by adding one element at a time. This granular construction allows for the assessment of whether a VLM can associate the correct caption with a given background. For instance, the authors first tested whether a VLM could match a caption to a background. Then, they added an animal to the scene to see if the model could correctly detect the animal against different backgrounds. If the animal was correctly identified, it was moved to the left or right to further test whether the VLM could still identify the proper caption indicating the animal's new position. The results showed that many current VLMs performed no better than random chance in detecting such positional changes, highlighting the limitations of existing models in spatial reasoning tasks.
</p>
<h2 id="references">References</h2>
<p><a name="references"></a></p>
<p><a href="https://huggingface.co/blog/vlms">[1]</a> M. Noyan &amp; E. Beeching &ldquo;Vision Language Models Explained&rdquo; (2024).</p>
<p><a href="https://arxiv.org/pdf/2405.17247">[2]</a> Bordes et al. &ldquo;An Introduction to Vision-Language Modeling&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2103.00020">[3]</a> Radford et al. &ldquo;Learning Transferable Visual Models from Natural Language Supervision&rdquo; (2021)</p>
<p><a href="https://arxiv.org/pdf/2405.00740">[4]</a> Lavoie et al. &ldquo;Modeling Caption Diversity in Contrastive Vision-Language Pretraining&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2112.04482">[5]</a> Singh et al. &ldquo;FLAVA: A Foundational Language And Vision Alignment Model&rdquo; (2021)</p>
<p><a href="https://arxiv.org/pdf/2208.02131">[6]</a> Kwon et al. &ldquo;Masked Vision and Language Modeling for Multi-Modal Represenation Learning&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2306.16527">[7]</a> Laurencon et al. &ldquo;OBELICS: An Open Web-Scaled Filtered Dataset of Interleaved Image-Text Documents&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2405.02246">[8]</a> Laurencon et al. &ldquo;What matters when building vision-language models?&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2408.12637">[9]</a> Laurencon et al. &ldquo;Building and better understanding vision-language models: insights and future directions&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2205.01917">[10]</a> Yu et al. &ldquo;CoCa: Contrastive Captioners are Image-Text Foundation Models&rdquo; (2022)</p>
<p><a href="https://arxiv.org/pdf/2309.02591">[11]</a> Yu et al. &ldquo;Scaling Autoregressive Multi-Modal Models: Pre-Training and Instruction Tuning&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2211.12561">[12]</a> Yasunaga et al. &ldquo;Retrieval-Augmented Multimodal Language Modeling&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2201.07520">[13]</a> Aghajanyan et al. &ldquo;CM3: A Causal Masked Multimodal Model of The Internet&rdquo; (2022)</p>
<p><a href="https://arxiv.org/pdf/2109.01134">[14]</a> Zhou et al. &ldquo;Learning to Prompt for Vision-Language Models&rdquo; (2022)</p>
<p><a href="https://arxiv.org/pdf/2203.12119">[15]</a> Jia el al. &ldquo;Visual Prompt Tuning&rdquo; (2022)</p>
<p><a href="https://arxiv.org/pdf/2312.14238">[16]</a> Chen el al. &ldquo;InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2404.16821">[17]</a> Chen el al. &ldquo;How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open Source Suites&rdquo; (2024)</p>
<p><a href="https://internvl.github.io/blog/2024-07-02-InternVL-2.0/">[18]</a> Chen el al. &ldquo;InternVL2: Better than the Best - Expanding Performance Boundaries of Open Source Multimodal Models with the Progressive Scaling Strategy&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2409.12191">[19]</a> Wang el al. &ldquo;Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2408.01800">[20]</a> Yao el al. &ldquo;MiniCPM-V: A GPT-4V Level MLLM on Your Phone&rdquo; (2024)</p>
]]></content:encoded></item><item><title>Retrieval-Augmented Text Generation (RAG)</title><link>https://johanngerberding.github.io/posts/2024-04-23-retrieval-augmented-text-generation/</link><pubDate>Tue, 23 Apr 2024 10:00:00 +0200</pubDate><guid>https://johanngerberding.github.io/posts/2024-04-23-retrieval-augmented-text-generation/</guid><description>A short intro to Retrieval-Augmented Text Generation, what is it, why is it useful and how does it work.</description><content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p align="justify">
In 2020, Lewis et al. introduced a novel approach to enhance language model outputs through the use of Retrieval-Augmented Generation (RAG). Since its inception, RAG has evolved significantly, providing a robust solution to some of the persistent challenges faced by Large Language Models (LLMs).
</p>
<p align="justify">
LLMs, while transformative, are not without their issues. They often produce "hallucinations" or convincing yet inaccurate answers, due to their sole reliance on the fixed dataset on which they were trained on. This limitation is compounded by their inability to incorporate information that emerges after their last training update, rendering them outdated as the world evolves. Additionally, their performance can be suboptimal in specialized areas because their training prioritizes breadth over depth to ensure broad accessibility and applicability.
</p>
<p align="justify">
RAG addresses these challenges by dynamically supplementing model responses with external data fetched in real time in response to queries. This approach not only keeps the information current but also enhances the accuracy and relevancy of the responses.
</p>
<p align="justify">
For a RAG system to deliver high-quality outputs, two core components are crucial. First, it must effectively retrieve the most relevant information from external sources. This involves a sophisticated understanding of the query context and the ability to sift through vast amounts of data quickly and efficiently. Second, it needs to generate accurate responses that integrate the retrieved information, ensuring that the final output is both coherent and contextually appropriate.
</p>
<p align="justify">
Building on these technologies, startups <a href="https://www.trychroma.com/">like Chroma</a>, <a href="https://weaviate.io/">Weaviate</a>, and <a href="https://www.pinecone.io/">Pinecone</a> have expanded upon RAG's foundational concepts by incorporating robust text storage solutions and advanced tooling into their platforms. These enhancements facilitate more efficient data handling and retrieval processes, which are critical for delivering precise and contextually relevant responses.
</p>
<p align="justify">
In the upcoming sections, we will dig deeper into the world of RAG by providing a comprehensive overview of the framework and highlighting the key components essential for an effective RAG system. Additionally, we will explore practical strategies that can enhance the quality and performance of your RAG system. While RAG offers significant benefits, it is not without its challenges. Therefore, we will also examine some of the common obstacles encountered in building RAG systems and discuss the current limitations that engineers need to be aware of as they build these kind of systems.
</p>
<h2 id="rag-framework">RAG Framework</h2>
<p align="justify">
The basic workflow of a RAG system consists of three fundamental steps: indexing, retrieval, and generation. This sequence begins with indexing, where data is organized to facilitate quick and efficient access. The retrieval step can be further divided into three subparts: pre-retrieval, retrieval, and post-retrieval. Pre-retrieval involves preparing and setting up the necessary parameters for the search. Retrieval is the actual process of fetching the most relevant information based on the query. Post-retrieval includes the refinement and selection of the fetched data to ensure its relevance and accuracy. Finally, in the generation phase, the system synthesizes this information into coherent and contextually appropriate responses. These foundational steps remain at the core of every advanced RAG workflow, serving as the building blocks for more complex systems.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_framework.png#center"
         alt="Abstract Concept Graphic Retrieval-Augmented Generation Elements" width="100%"/> <figcaption>
            <p>Figure 1. RAG Framework Paradigm [1]</p>
        </figcaption>
</figure>

<p align="justify">
RAG presents a cost-effective alternative to the traditional, resource-intensive training and fine-tuning processes required for LLMs. Unlike standard LLMs that need model updates to incorporate new information, RAG dynamically integrates fresh data by retrieving relevant external content and concatenating it to the search query. This ensures flexibility and scalability to make it particularly valuable across a variety of use cases, like self-service applications where solutions get retrieved from an internal knowledge base and integrated into a helpful answer. Down below you can see the basic workflow that most RAG systems follow. 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_workflow.png#center"
         alt="Basic Retrieval-Augmented Generation Workflow" width="100%"/> <figcaption>
            <p>Figure 2. Basic RAG Workflow []</p>
        </figcaption>
</figure>

<p align="justify"> 
You start by generating document vectors using an embedding model for every document or text you want to include in your knowledge database. Embedding models (e.g. BERT, OpenAI text-embedding-ada-002) turn text into a vector representation, which is essential for the retrieval process by enabling similarity search. The documents you would want to retrieve can range from specific company guidelines or documents, a website to solution descriptions to known problems. Popular choices for such vector stores are <a href="https://github.com/facebookresearch/faiss">FAISS</a>, <a href="https://www.trychroma.com/">Chroma</a> or <a href="https://lancedb.com/">Lance</a>.  
</p>
<p align="justify"> 
After you have generated the vector store you are good to go. First your query gets embedded into a vector, like we have described above, which is then used in a similarity search over your vector store to determine the most relevant documents. These documents get retrieved and concatenated to your query before its fed into the LLM. The LLM then produces based on the query and context an answer. This describes the very basic and naive workflow. In practice there are a lot of other things involved, like prompt engineering, a limited context window, limited input tokens of the embedding model, preprocessing of the query and documents, postprocessing of the answer etc. We will touch most of these things in more detail in the next sections. 
</p>
<h3 id="pre-retrieval">Pre-Retrieval</h3>
<p>The pre-retrieval phase is all about the data or the knowledge base. The goal is to ensure efficient and correct information retrieval.</p>
<h4 id="data-preparation--modification">Data Preparation &amp; Modification</h4>
<p align="justify">
If you have worked on an applied ML project with real customers than you are aware of the quality of company data. It would be pretty naive to assume all the information you want to have in your knowledge base is in an easy to read and parse format. Let's be real, most of the company knowledge is in Excel sheets and PowerPoint presentations or in pdf scans that have no pretty text layers. Most of the current RAG pipelines just work on text data, so you have to think about ways to transform everything into text. In the long run, I think you will have multimodal pipelines that can also work with images for example so you can embed e.g. a presentation as a sequence of images. Most of the data preparation phase is about OCR (which still is far from being solved), data cleaning and being creative about how to incorporate all the knowledge you have. You can use frameworks like <a href="https://github.com/Unstructured-IO/unstructured">unstructured</a> for parsing your data. This is really helpful for quickly getting a prototype up and running. But if you have ever seen business Excel sheets you might know why I think that those tools are definitely not good enough and we will need something that works on images.
</p>
<p align="justify">
Data Modification is another important component of the pre-retrieval phase for enhancing retrieval efficiency. It includes e.g. removing irrelevant or redundant information from your text to improve the quality of retrieval results or enriching the data with additional information such as metadata to boost relevance and diversity of the retrieved content. You could e.g. use visual models to describe images and diagrams in text form or use a LLM to extract metadata from documents.
</p>
<h4 id="indexing">Indexing</h4>
<p align="justify">
After our data is clean and pretty we follow up with indexing which is dependent on the task and data type. In the classical workflow, documents/texts are split into chunks and then each chunk is embedded and indexed. The embeddings are created through the use of an embedding model like the one from <a href="https://docs.mistral.ai/capabilities/embeddings/">Mistral</a>. Such an embedding model turns your text into a feature vector that captures the semantic meaning of chunks. Unfortunately most of these embedding models are closed sourced but feel free to build your own.</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/indexing.png#center"
         alt="Basic Chunking/Indexing Workflow" width="80%"/> <figcaption>
            <p>Figure 3. Basic Chunking/Indexing Workflow</p>
        </figcaption>
</figure>

<p align="justify">
As you might already know, LLMs have limited capacity to process text or more specifically tokens, which is often described as context length. E.g. GPT3.5 has a context length of 4,096 tokens which it can process at once. This roughly about 6 pages of english text after tokenization. By chunking our documents we first of all have to make sure, that the chunk size is smaller than the maximum context length of our LLM of choice. Nowadays there exist models like Llama3.1 that have extensive context lengths like 128K tokens. Some people might argue that RAG will become irrelevant in the long term because we can shove our whole knowledge base in-context but that's for the future. In most cases we just need small passages of text from a document and we want to combine it with information from other documents as well to answer our question. Determining the chunk size is a critical decision that not only influences the accuracy but also the efficiency (response generation time) of the retrieval and the overall RAG system. The goal is to choose the correct chunks of optimal lengths so that you can give the LLM as much relevant information as possible.</p>
<p align="justify">
There are a bunch of different possibilities of how to chunk you documents, e.g. page-level, sentence-level or you try to split by sections. You can imagine, based on the use case and data type, different strategies could be helpful here. There is no one size fits all approach and most of the time you have to do a bit of experimenting. E.g. Sentence-level indexing is beneficial for question-answering systems to precisely locate answers, while document-level indexing is more appropriate for summarizing documents to understand their main concepts and ideas. In some cases the retrieved data doesn't even have to be the same we used while indexing. For example you could use a LLM to generate questions that every chunk answers and then you retrieve based on similarity of questions. Or you could generate summaries for every chunk and index those which can reduce redundant information and irrelevant details (Figure 4).</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/advanced_indexing.png#center"
         alt="Advanced Chunking/Indexing Workflow" width="90%"/> <figcaption>
            <p>Figure 4. Advanced Chunking/Indexing Workflow</p>
        </figcaption>
</figure>

<p align="justify">
Both strategies shown above are just two examples of how to improve the chunk indexing and you can be creative here, based on you use case. Another interesting way of indexing is described in MemWalker paper <a href="#references">[6]</a>, where the documents are split into segments which are recursively summarized and combined to form a memory tree. To generate answers the tree is navigated down to the most important segments. The method outperforms various long context LLMs.
</p>
<h4 id="query-manipulation">Query Manipulation</h4>
<p align="justify">
After the indexing is done the last possibility to improve you retrieval pipeline is through query manipulation to adjust the user queries for a better match with the indexed data. Examples of this include:
</p>
<ul>
<li><b>Query Reformulation/Refinement</b>: Rewrite the query to align more closely with the user's 'real' intention (this is hard to do) or create multiple queries with the same intention.</li>
<li><b>Query Expansion</b>: Extend the query to capture more relevant results through e.g. including synonyms or related terms or identifying people/places/companies and add information regarding those.</li>
<li><b>Query Normalization</b>: Resolve differences in spelling or terminology for more consistent query matching.</li>
</ul>
<p align="justify">
This is another part of the pipeline where you can be creative and experiment with different approaches, combine them or modify them for your specific use case. Here is a short list of papers that implemented those strategies or combinations of them: Rewrite-Retrieve-Read <a href="#references">[11]</a>, FiD (Fusion-in-Decoder) <a href="#references">[7]</a>, CoK (Chain-of-Knowledge) <a href="#references">[8]</a>, Query2Doc <a href="#references">[10]</a>, Step-Back <a href="#references">[9]</a> or FLARE (Forward-Looking Active Retrieval) <a href="#references">[12]</a>.
</p>
<h3 id="retrieval">Retrieval</h3>
<p align="justify">
The retrieval process is a combination of search and ranking algorithms. The overall goal is to select and prioritize documents from a knowledge base to enhance the quality of the generation model outputs. The first thing you have to think about here is the type of search strategy you want to use. There are three main choices you can select from, vector search, keyword search or a hybrid approach. 
<p align="justify">
Since you have embedded all your knowledge into feature vectors it's kinda easy to just use cosine similarity and k-Nearest-Neighbors to identify the 'k' vectors that are most similar to the query. kNN is a very popular choice due to its simplicity and explainability. One shortcoming of this strategy is the computational intensity of this approach when you have a very large knowledge base. Instead of cosine similarity you can also use euclidean distance or the dot prodoct which can lead to better search results in some cases.
</p>
<p align="justify">
There is a lot of research going on here and the following is a list of a couple of interesting methods:
</p>
<ul>
<li><b>Atlas</b> <a href="#references">[24]</a>: Attention Distillation and Perplexity Distillation to steer the retriever to more relevant documents.</li>
<li><b>IRCOT</b> <a href="#references">[25]</a>: Improve retrieval effectiveness by integrating reasoning.</li>
<li><b>SURGE</b> <a href="#references">[26]</a>: Subgraph retriever to extract relevant subgraphs from a knowledge graph.</li>
<li><b>PRCA</b> <a href="#references">[27]</a>: Employ domain specific abstract summarization to extract relevant and context-rich information using a supervised learning strategy.</li>
</ul>
<h3 id="post-retrieval">Post-Retrieval</h3>
<p align="justify">
This phase serves to refine the initially retrieved documents to improve the quality of text generation. This phase consists of re-ranking and filtering, each aimed at optimizing the document selection for the final generation task.
</p>
<h4 id="re-ranking">Re-Ranking</h4>
<p align="justify">
In the re-ranking step, the documents previously retrieved are reassessed, scored, and reorganized. The objective is to more accurately highlight the documents most relevant to the query and diminish the importance of the less relevant ones. There are a bunch of different methods out there for re-ranking:
</p>
<ul>
<li><b>Re2G</b> <a href="#references">[28]</a>: Sequence pair classification for re-ranking, using BERT to analyze both query and passage.</li>
<li><b>PROMPTAGATOR</b> <a href="#references">[29]</a>: Employs a cross-attention model for re-scoring. It progressively improves content quality by a strategy called "Lift Yourself Up" that iteratively selects the best candidate from a pool for further generation rounds.</li>
<li><b>In-Context RALM</b> <a href="#references">[30]</a>: Explores two re-ranking approaches, zero-shot re-ranking using LLMs and predictive re-ranking using trained models. The goal is to refine the selection of documents based on their expected utility for improving language model performance.</li>
<li><b>ITER-RETGEN</b> <a href="#references">[31]</a>: Leverages knowledge distillation from the re-ranker to the dense retriever, finetuning retrieval efforts based on relevance signals from LLM outputs.</li>
<li><b>DKS-RAC</b> <a href="#references">[32]</a>: Presents Dense Knowledge Similarity (DKS) for aligning the knowledge between answers and retrieved passages at the sequence level.</li>
</ul>
<h4 id="filtering">Filtering</h4>
<p align="justify">
Filtering aims to remove documents that fail to meet specified quality or relevance standards. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below a certain relevance level. Furthermore, the use of feedback from users or prior relevance evaluations assists in adjusting the filtering process, guaranteeing that only the most relevant documents are retained for text generation. The following three methods try to improve the filtering process:
</p>
<ul>
<li><b>COK</b> <a href="#references">[33]</a>: Progressive Rationale Correction technique aimed at iteratively refining rationales with retrieved knowledge. It is a continuous optimization process to enhance the relevance and quality of information used in content generation.</li>
<li><b>FiD-TF</b> <a href="#references">[34]</a>: Focus on removal of irrelevant and redundant tokens by employing a dynamic mechanism to identify those tokens.</li>
<li><b>RECOMP</b> <a href="#references">[35]</a>: Focus on removal of irrelevant and redundant tokens by compressing documents into concise summaries focusing only on selecting the most pertinent content for the generation process.</li>
</ul>
<h3 id="generation">Generation</h3>
<p align="justify">
In the generation phase the overall goal is to generate text that is relevant to the query and reflective of the information found in the retrieved documents. The process is quite simple, we just concat the query with the found information and input that into a LLM. The main challenge is about ensuring the outputs alignment and accuracy with the retrieved contents which isn't straightforward. The generated output should accurately convey the information from the retrieved documents and align with the query's intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data.
</p>
<p align="justify">
There exist a lot of different approaches to improve the generation quality in a RAG pipline. The following list contains some recent papers which aimed at doing exactly that:
</p>
<ul>
<li><b>REPLUG</b> <a href="#references">[36]</a>: Prepend retrieved documents to the input context before the final prediction. This introduces an ensemble strategy to encode retrieved documents in parallel, overcoming the limitations of LM context length and enhancing accurarcy through the allocation of increased computational resources.</li>
<li><b>RECITE</b> <a href="#references">[37]</a>: Implements a self-consistency technique which involves generating multiple recitations independently and employing a majority vote system to determine the most appropriate answer.</li>
<li><b>PKG</b> <a href="#references">[38]</a>: Customize outputs by generating background knowledge internally using a pre-trained model. This method directly integrates domain or task-specific knowledge into the generation step which enhances the LMs capacity to produce responses that are specifically tailored to the given context.</li>
<li><b>SURGE</b> <a href="#references">[39]</a>: Customization through application of graph-text contrastive learning to ensure that the generated dialogue responses are in tight alignment with the knowledge contained in the retrieved subgraph.</li>
</ul>
<h2 id="advanced-rag-pipelines">Advanced RAG Pipelines</h2>
<p align="justify">
The naive RAG implementation described before is rarely enough to satisfy production grade requirements. This has multiple reasons:
</p>  
<ul>
<li><b>Question ambiguity</b>: user questions are not well defined and may lead to irrelevant retrieval results </li>
<li><b>Low retrieval accuracy</b>: retrieved documents may not be equally relevant to the question </li>
<li><b>Limited knowledge</b>: the knowledge base may not include the information the user is looking for </li>
<li><b>Context window performance limitations</b>: trying to "over-retrieve" may hit on the capacity of the context window or otherwise produce a context window that is too big to return a result in a reasonable amount of time </li>
</ul>
<p align="justify">
Many new RAG patterns have emerged to address these limitations. In the following I will go over some of those techniques, but it is important to note that there is no silver bullet. Each one of these methods may still produce poor results in certain situations or isn't well fitted for your specific use case. 
</p>
<h3 id="self-rag">Self RAG</h3>
<p align="justify">
The Self-Reflective RAG paper describes fine-tuned models that incorporate mechanisms for adaptive information retrieval and self critique. These models can dynamically determine when external information is needed, and can critically evaluate its generated responses for relevance and factual accuracy. Figure X down below shows the retrieval process using these new tokens.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/self_rag.png#center"
         alt="Flowchart of the Self RAG method" width="100%"/> <figcaption>
            <p>Figure 5. Self RAG Process [3]</p>
        </figcaption>
</figure>

<p align="justify">
First the Language Model decides about the necessity of additional information to answer the prompt. In the second step multiple segments are generated concurrently and each of them is rated in terms of relevance to the prompt and usefulness of the retrieved information. Thereafter each segments is critiqued and the best one is chosen. Then the cycle can repeat, if the model decides to retrieve more information. The table down below shows the four new tokens that make this whole process work.
</p>
<table>
    <tr>
        <th>Name</th>
        <th>Description</th>
        <th>Output</th>
</tr>
    <tr>
        <td>Retrieval Token</td>
        <td>Retrieve additional information from knowledge base</td>
        <td>yes, no</td>
    </tr>
    <tr>
        <td>Relevance Token</td>
        <td>Retrieved information is relevant to the prompt</td>
        <td>relevant, irrelevant</td>
    </tr>
    <tr>
        <td>Support Token</td>
        <td>Generated information is supported by the provided context</td>
        <td>fully supported, partially supported, no support</td>
    </tr>
    <tr>
        <td>Critique Token</td>
        <td>Useful response to the prompt</td>
        <td>5, 4, 3, 2, 1</td>
    </tr>
</table> 
<p align="justify">
For more detailed information regarding training, how they inserted these special tokens and data collection I'll recommend checking out the <a href="https://arxiv.org/pdf/2310.11511">paper</a>.
</p>
<h3 id="corrective-rag">Corrective RAG</h3>
<p align="justify">
The Corrective Retrieval Augmented Generation method to improve the accuracy and robustness of LM by re-incorporating information from retrieved documents. This is accomplished by the introduction of three additional components. First a lightweight evaluator model that assesses the quality of retrieved documents and determines which to use, ignore or request more documents based on a score. The second new component is an incorporated web search to extend the information base and get more up-to-date information. The last new addition is a decompose-then-recompose algorithm for selectively focusing on key information and filtering out irrelevant information that also makes use of the evaluator model. The Figure X down below gives a nice overview of the CRAG method at inference.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/crag.png#center"
         alt="An overview of Corrective Retrieval Augmented Generation" width="100%"/> <figcaption>
            <p>Figure 6. An overview of CRAG at inference [4]</p>
        </figcaption>
</figure>

<p align="justify">
The Knowledge Refinement process is quite straightforward and starts by segmenting each retrieved document into fine-grained knowledge strips through heuristic rules. Then the retrieval evaluator is used to calculate a relevance score of each knowledge strip. Based on these scores the strips are filtered and recomposed via concatenation in order.
</p>
<h3 id="rag-fusion">RAG Fusion</h3>
<p align="justify">
The origin of RAG-Fusion is a <a href="https://github.com/Raudaschl/RAG-Fusion">github project</a> which got noticed from an Infineon employee who deployed it internally. The motivation behind it was, the need for employees to rapidly obtain product information but regular RAG didn't work that well. RAG-Fusion is a combination of traditional RAG and reciprocal rank fusion (RRF) and the workflow is as followed:
</p>
<ol>
<li>Receive query and generate a number of new search queries through a LLM</li>
<li>Vector Search like in regular RAG pipeline</li>
<li>Reciprocal Rank Fusion: assign scores to every document and re-rank them accordingly</li>
<li>Send all the re-ranked queries and documents to the LLM to generate an answer</li>
</ol>
<p align="justify">
The figure down below shows this process described above.
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/rag_fusion.png#center"
         alt="RAG Fusion" width="80%"/> <figcaption>
            <p>Figure 7. RAG Fusion Workflow [24]</p>
        </figcaption>
</figure>

<p align="justify">
The Infineon customers and engineers found the biggest downside of the method was the slow answer time because of the generation of multiple similar queries. Moreover the response times fluctuated over time (from 10 - 32s) because of the use of an third party API. Another main critique was the inability the empirically evaluate the answers. But in general, the RAG-Fusion answers were more accurate and comprehensive than those of the vanilla RAG system.
</p>
<h3 id="self-reasoning">Self-Reasoning</h3>
<p align="justify">
The method described in this paper focuses on the problem of irrelevant document retrieval which may result in unhelpful response generation or even performance deterioration. It is similar to the Self-RAG without the need of extra models and datasets. The authors propose an end-to-end self-reasoning framework to improve on these problems and especially improve reliability and traceability. 
</p>
<figure class="align-center ">
    <img loading="lazy" src="/imgs/rag/self-reasoning.png#center"
         alt="Self-Reasoning Framework to Improve RAG" width="100%"/> <figcaption>
            <p>Figure 8. Self-Reasoning Framework to Improve RAG [5]</p>
        </figcaption>
</figure>

<p align="justify">
As shown above the framework consists of three processes, the Relevance-Aware Process, the Evidence-Aware Selective Process and the Trajectory Analysis Process.
</p>
<p align="justify">
<b>Relevance-Aware Process</b>: Instruct the model to judge the relevance between the retrieved documents $\mathcal{D}$ and the given question $\mathcal{q}$. In addition the model is requested to generate reasons explaining the relevance of each document. The self-reasoning trajectories are defined as $\mathcal{\tau_{r}}$. If none of the documents is classified as relevant, the answer should be generated by the basic LLM. 
</p>
<p align="justify">
<b>Evidence-Aware Selective Process</b>: Instruct the model to choose relevant documents and select snippets of key sentences for the selected documents. Then the model has to generate the reason why the selected snippets can answer the question, so that you end up with a list containing the cited content and the reason for the cite. The self-reasoning trajectories generated in this process are defined as $\mathcal{\tau_{e}}$.</p>
<p align="justify">
<b>Trajectory Analysis Process</b>: Here the self-reasoning trajectories ($\mathcal{\tau_{r}}$ & $\mathcal{\tau_{e}}$) are consolidated to form a chain of reasoning snippets. With these snippets as context the LLM is instructed to output content with two fields: analysis (long-form answer) and answer (short-form). 
</p>
<p align="justify">
Regarding the training process the authors propose a stage-wise training because of problems with the generation of long reasoning trajectories. In the first stage only the first two stage trajectories are masked, in the second stage only the trajectories from the third process are masked. Finally all trajectories are concatenated and trained end-to-end. For more details on this procedure, check out the paper.</p>
<h2 id="evaluation">Evaluation</h2>
<p align="justify">
Now you have built you fancy RAG engine, but how do you determine if this thing is any good? Is it even better as a vanilla LLM or is it even worse? Since RAG systems are comprised of multiple components and can get kinda complicated as we have seen above, you can evaluate single components individually or just the performance of the whole thing on the downstream task. Because of the AI hype and the push of LLMs into prodcution systems, this has become an important area of research. 
</p>
<p>Datasets for measuring performance on downstream tasks like you would assess a regular LLM using established metrics like F1 or Exact Match (EM):</p>
<ul>
<li>TriviaQA <a href="#references">[17]</a></li>
<li>HotpotQA <a href="#references">[18]</a></li>
<li>FEVER <a href="#references">[19]</a></li>
<li>Natural Questions <a href="#references">[20]</a></li>    
<li>Wizard of Wikipedia <a href="#references">[21]</a></li>
<li>T-REX <a href="#references">[22]</a></li>
</ul>
<p align="justify">
For the further development of the methods we need a more finegrained evaluation for identifying the components of the pipeline that are the weak points and we need to be able to analyze those weaknesses in more detail. Various frameworks were introduced in the recent past to evaluate RAG pipelines across multiple dimensions, including the relevance of the retrieved documents, the quality of the generated text and the resilience to misinformation. The following table from <a href="#references">[1]</a> shows more details about the different frameworks, their methods and metrics as well as the datasets they use. 
</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Framework</th>
          <th style="text-align: left">Aspects</th>
          <th style="text-align: left">Methods</th>
          <th style="text-align: left">Metrics</th>
          <th style="text-align: left">Datasets</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">RAGAS <a href="#references">[13]</a></td>
          <td style="text-align: left">Quality of RAG Systems</td>
          <td style="text-align: left">Context Relevance, Answer Relevance, Faithfulness</td>
          <td style="text-align: left">Extracted Sentences / Total Sentences, Average Cosine Similarity, Supported Statements / Total Statements</td>
          <td style="text-align: left">WikiEval</td>
      </tr>
      <tr>
          <td style="text-align: left">ARES <a href="#references">[14]</a></td>
          <td style="text-align: left">Improving RAGAS</td>
          <td style="text-align: left">Context Relevance, Answer Relevance, Answer Faithfulness</td>
          <td style="text-align: left">Confidence Intervals</td>
          <td style="text-align: left">KILT, SuperGLUE</td>
      </tr>
      <tr>
          <td style="text-align: left">RECALL <a href="#$references">[15]</a></td>
          <td style="text-align: left">Counterfactual Robustness</td>
          <td style="text-align: left">Response Quality, Robustness</td>
          <td style="text-align: left">Accuracy (QA), BLEU, ROGUE-L</td>
          <td style="text-align: left">EventKG, UJ</td>
      </tr>
      <tr>
          <td style="text-align: left">RGB <a href="#references">[16]</a></td>
          <td style="text-align: left">Impact of RAG on LLMs</td>
          <td style="text-align: left">Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness</td>
          <td style="text-align: left">Accuracy, Rejection Rate, Error Detection Rate, Error Correction Rate</td>
          <td style="text-align: left">Synthetic Dataset including English and Chinese</td>
      </tr>
  </tbody>
</table>
<p align="justify">
Let's take a closer look at the evaluation of the two components:
</p>
<p align="justify">
<b>Evaluate Retrieval</b>: The quality of the search results is most of the time evaluated based on standard metrics like Mean Average Precision (MAP), Precision, Reciprocal Rank, Recall and Normalized Discounted Cumulative Gain (NDCG) which primarily assess the relevance of retrieved documents regarding a given query. For some explanations on those take a look <a href="https://www.deepset.ai/blog/rag-evaluation-retrieval">here</a>. The Rejection Rate measures a systems ability to decline answering when no relevant information is found. In addition the Error Detection Rate assesses the models capability to identify and disregard incorrect or misleading information from retrieved documents. Another important metric is the Context Relevance that ensures the perinence of the retrieved documents so that the information used to generate responses is directly related to the querys context. Faithfulness measures the accuracy with which the generated content reflects the information in the retrieved documents.
</p>
<p align="justify">
<b>Evaluate Generation</b>: This part is more common and there are a lot of different standard metrics based on the downstream task. Those metrics assess linguistic quality and coherence (BLEU score, ROUGE-L), accuracy and the extent to which the generated text reflects the ground truth data (EM, F1 score). Beyond the standard metrics, the evaluation can incorporate task-specific criteria and novel metrics tailored to particular applications. In dialog generation perplexity and entropy are used to evaluate response diversity and naturalness. Based on your specific use case, you can get creative here.
</p>
<h2 id="references">References</h2>
<p><a name="references"></a></p>
<p><a href="https://arxiv.org/pdf/2404.10981">[1]</a> Y. Huang &amp; J.X. Huang &ldquo;A Survey on Retrieval-Augmented Text Generation for Large Language Models&rdquo; (2024).</p>
<p><a href="https://www.pinecone.io/learn/advanced-rag-techniques/">[2]</a> Roie Schwaber-Cohen &ldquo;Advanced RAG Techniques&rdquo; (2024).</p>
<p><a href="https://arxiv.org/pdf/2310.11511">[3]</a> Asai et al. &ldquo;Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2401.15884">[4]</a> Yan et al. &ldquo;Corrective Retrieval Augmented Generation&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2407.19813">[5]</a> Xia et al. &ldquo;Improving Retrieval Augmented Language Model with Self-Reasoning&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2310.05029">[6]</a> Chen et al. &ldquo;Walking Down the Memory Maze: Beyond Context Limit Through Interactive Reading&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2007.01282">[7]</a> G. Izacard &amp; E. Grave &ldquo;Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering&rdquo; (2020)</p>
<p><a href="https://arxiv.org/pdf/2305.13269">[8]</a> Xingxuan et al. &ldquo;Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2310.06117">[9]</a> Zheng et al. &ldquo;Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2303.07678">[10]</a> Wang et al. &ldquo;Query2doc: Query Expansion with Large Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.14283">[11]</a> Ma et al. &ldquo;Query Rewriting for Retrieval-Augmented Large Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.06983">[12]</a> Jiang et al. &ldquo;Active Retrieval Augmented Generation&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2309.15217">[13]</a> Shahul et al. &ldquo;RAGAS: Automated Evaluation of Retrieval Augmented Generation&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2311.09476">[14]</a> Saad-Falcon et al. &ldquo;ARES: An Automated Evaluation Framework for Retrieval Augmented Generation Systems&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2311.08147">[15]</a> Liu et al. &ldquo;RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2309.01431">[16]</a> Chen et al. &ldquo;Benchmarking Large Language Models in Retrieval-Augmented Generation&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/1705.03551">[17]</a> Joshi el al. &ldquo;TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension&rdquo; (2017)</p>
<p><a href="https://arxiv.org/pdf/1809.09600">[18]</a> Yang et al. &ldquo;HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering&rdquo; (2018)</p>
<p><a href="https://arxiv.org/pdf/1803.05355">[19]</a> Thorne et al. &ldquo;FEVER: A large-scale dataset for Fact Extraction and VERification&rdquo; (2018)</p>
<p><a href="https://aclanthology.org/Q19-1026.pdf">[20]</a> Kwiatowski el al. &ldquo;Natural Questions: A Benchmark for Question Answering Research&rdquo; (2019)</p>
<p><a href="https://arxiv.org/pdf/1811.01241">[21]</a> Dinan et al. &ldquo;Wizard of Wikipedia: Knowledge Powered Conversational Agents&rdquo; (2019)</p>
<p><a href="https://aclanthology.org/L18-1544.pdf">[22]</a> ElSahar et al. &ldquo;T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples&rdquo; (2018)</p>
<p><a href="https://arxiv.org/pdf/2402.03367">[23]</a> Z. Rackauckas &ldquo;RAG-Fusion: a New Take on Retrieval Augmented Generation&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2208.03299">[24]</a> Izacard et al. &ldquo;Atlas: Few-shot learning with retrieval augmented language models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2212.10509">[25]</a> Trivedi et al. &ldquo;Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.18846">[26]</a> Kang et al. &ldquo;Knowledge Graph Augmented Language Models for Knowledge Grounded Dialog Generation&rdquo; (2023)</p>
<p><a href="https://aclanthology.org/2023.emnlp-main.326.pdf">[27]</a> Yang et al. &ldquo;PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2207.06300">[28]</a> Glass et al. &ldquo;Re2G: Retrieve, Rerank, Generate&rdquo; (2022)</p>
<p><a href="https://arxiv.org/pdf/2209.11755">[29]</a> Dai et al. &ldquo;PROMPTAGATOR: Few-shot Dense Retrieval from 8 Examples&rdquo; (2022)</p>
<p><a href="https://arxiv.org/pdf/2302.00083">[30]</a> Ram et al. &ldquo;In-Context Retrieval Augmented Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.15294">[31]</a> Shao et al. &ldquo;Enhancing Retrieval-Augmented LLMs with Iterative Retrieval-Generation Synergy&rdquo; (2023)</p>
<p><a href="https://aclanthology.org/2023.ijcnlp-main.65.pdf">[32]</a> Huang et al. &ldquo;Retrieval Augmented Generation with Rich Answer Encoding&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.13269">[33]</a> Li et al. &ldquo;Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources&rdquo; (2024)</p>
<p><a href="https://arxiv.org/pdf/2310.13682">[34]</a> Berchansky et al. &ldquo;Optimizing Retrieval-Augmented Reader Models via Token Elimination&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2310.04408">[35]</a> Xu et al. &ldquo;RECOMP: Improving Retrieval Augmented LMs with Compression and Selective Augmentation&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2301.12652">[36]</a> Shi et al. &ldquo;REPLUG: Retrieval-Augmented Black-Box Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2210.01296">[37]</a> Sun et al. &ldquo;Recitation-Augmented Language Models&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.04757">[38]</a> Luo et al. &ldquo;Augmented Large Language Models with Parametric Knowledge Guiding&rdquo; (2023)</p>
<p><a href="https://arxiv.org/pdf/2305.18846">[39]</a> Kang et al. &ldquo;Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialog Generation&rdquo; (2023)</p>
]]></content:encoded></item></channel></rss>